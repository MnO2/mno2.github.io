<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Paul Meng's Blog</title>
    <link href="https://blog.paulme.ng/rss/feed.xml" rel="self" />
    <link href="https://blog.paulme.ng" />
    <id>https://blog.paulme.ng/rss/feed.xml</id>
    <author>
        <name>Paul Meng</name>
        <email>me@paulme.ng</email>
    </author>
    <updated>2019-10-04T00:00:00Z</updated>
    <entry>
    <title>市值權重指數基金與系統性設計問題</title>
    <link href="https://blog.paulme.ng/posts/2019-10-04-%E5%B8%82%E5%80%BC%E6%AC%8A%E9%87%8D%E6%8C%87%E6%95%B8%E5%9F%BA%E9%87%91%E8%88%87%E7%B3%BB%E7%B5%B1%E6%80%A7%E8%A8%AD%E8%A8%88%E5%95%8F%E9%A1%8C.html" />
    <id>https://blog.paulme.ng/posts/2019-10-04-%E5%B8%82%E5%80%BC%E6%AC%8A%E9%87%8D%E6%8C%87%E6%95%B8%E5%9F%BA%E9%87%91%E8%88%87%E7%B3%BB%E7%B5%B1%E6%80%A7%E8%A8%AD%E8%A8%88%E5%95%8F%E9%A1%8C.html</id>
    <published>2019-10-04T00:00:00Z</published>
    <updated>2019-10-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>前一陣子因為 The Big Short 而出名的 Michael Burry 回覆了 Bloomberg 對於市場看法的信而又引發了網路上一場論戰。因為他的回覆中關於 Index Fund 的說法提到了 “Bubble” 這個詞而許多人擺錯了重點，把重點擺在是不是有 Bubble 這件事。其實他的說法跟之前 Howard Marks 還有 Seth Klarman 提到的並沒有不同，其實關鍵的重點在於結構設計，而每個結構設計跟實體工程一樣都有他的極限在，特別是在金融市場中因為結構承重量或容錯量是不可見的，涉及抽象的概念。雖然我完全同意 Micael Burry 的說法，但我一直想不到一個好的比喻跟一般人解釋這件事。直到我前幾天看到 yinwang 關於自動駕駛責任的<a href="http://www.yinwang.org/blog-cn/2019/09/30/autopilot-responsibility">文章</a>。我覺得解釋得非常清楚，而同樣的邏輯謬誤也是可以套用在市值權重指數基金擁護者的回覆上。</p>
<p>要討論這件事涉及兩個層面</p>
<ol type="1">
<li>為什麼用【市值權重指數基金拿到的是市場報酬】不足以回應 Michael Burry 點出的風險？</li>
<li>市值權重指數基金的缺陷在哪？什麼情景下會觸發？</li>
</ol>
<p>首先用一個例子來說明第一點。在今年初發生了 Boeing 737 MAX 因為設計不良而造成的事故。一時間人心惶惶，而各國政府也相繼搬出政策禁飛 Boeing 737 MAX。為什麼大家會擔心這件事？不是說 “平均” 來講，發生空難的事件極低嗎？ 機率上來講你死在去機場的路上是遠高於你發生空難的機率。這主要的問題是因為 Boeing 737 MAX 有共同的因素驅動，也就是設計的缺陷。</p>
<p>就機率來解釋的話，就是</p>
<pre><code>P(其它 Boeing 737 MAX 墜機  | 一架 Boeing 737 MAX 墜機)</code></pre>
<p>高於</p>
<pre><code>P(其它非 Boeing 737 MAX 墜機 | 一架非 Boeing 737 MAX 墜機)</code></pre>
<p>因為他們不是類似於獨立事件。而是有共同的缺陷。假如每一家航空公司全部都是提供 Boeing 737 MAX 的話，說我身為一個個體明天搭的飛機墜機的機率趨近於平均發生空難的機率一點意義都沒有，因為平均的結果是大家一起因為起飛後因為設計的缺陷造成機頭直接往下墜毀死亡。</p>
<p>所以說【市值權重指數基金拿到的是市場報酬】並不足以回應。正確的回應要解釋為什麼市場報酬背後沒有共同驅動因素造成大家一起死的結果。這在大多數時候是對的，畢竟市場報酬是買方跟賣方資金平衡投票出來的，而且市場中每個人的想法不同，就好像是不同設計的飛機一樣。所以大多數時候市場的驅動的因子沒有共同背後的因素驅動。但接下來解釋的第二點，我會說明在某種特殊情況底下是有可能發生的，也就是市場上的參與玩家是同一個想法佔大多數的時候，而指數權重基金佔大多數也就是其中一個實現。</p>
<p>要了解第二點必須說明市值權重指數基金的運作，這邊舉一個簡單的小例子說明。台灣人很愛買房子出租，所以用房子來舉例，</p>
<p>今天市場中有三種分十年折舊的房子可以帶來租金收入，然後市場裡面有四個投資人 A, B, C, D</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">房子</th>
<th style="text-align: center;">年租金</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">i</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">ii</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td style="text-align: center;">iii</td>
<td style="text-align: center;">10</td>
</tr>
</tbody>
</table>
<p>期間是 10 年，十年後房子價值歸零。市場無風險利率是不會動的 1%，而對於房地產的投資人 A, B 風險利率要求是 2%，加上通膨 3% 折現率共 5%。那這些房子在他們兩個人間的平衡價錢應該是多少呢？用十年的 DCF 可以得到</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">房子</th>
<th style="text-align: center;">價格</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">i</td>
<td style="text-align: center;">7.72</td>
</tr>
<tr class="even">
<td style="text-align: center;">ii</td>
<td style="text-align: center;">15.44</td>
</tr>
<tr class="odd">
<td style="text-align: center;">iii</td>
<td style="text-align: center;">77.22</td>
</tr>
</tbody>
</table>
<p>而投資人 C, D 決定採取不一樣的觀點，不用類似 DCF 的方式衡量，他們決定編制 FOOBAR 市值權重指數，這個指數就是</p>
<pre><code>i 的市價 * (7.72 / 100.38) +
ii 的市價 * (15.44 / 100.38) +
iii 的市價 * (77.22 / 100.38)</code></pre>
<p>他們把他們的錢加起來成立市值權重指數基金，把他們共有的錢 100.38 照上面比例分配買。也就是 i 買 7.72 塊， ii 買 15.44 塊， iii 買 77.22 塊。所以投資人 C, D 認定的價值不是根據衡量物件本身，而是因為市場中其他人的價錢而分配他們的錢的分配。加入今天 A, B 覺得 iii 其實收不到 10 塊的年租金，其實只能收到 2 塊。那就會變成 15.44 塊。那 C, D 就會跟著把 iii 賣掉。因此你可以看到一般所謂的市值權重指數基金就是一種 Trend Following 的策略，是因為市場中其他對價值做出反應的人的定價而跟隨。在這個例子中，當 C, D 反應過來的時候，他們賣的價錢可能會低於 15.44 塊。因為他們的對手只有 A 跟 B。而 A, B 只願意用低於 15.44 塊的價格收，不賣的話就違背了他們指數的規則，所以他們一定要賣，這是機械性訂死的。 為了要賣掉他們就不得不降價，這邊的例子只有 C, D 兩個個體的資金，但你可以想像賣方是這個的 100 倍, 1000 倍然後【同時】想要賣的情況，也就是會造成 race to the bottom。最後平衡價可能遠遠低於 15.44，也許 7, 8 都有可能。</p>
<p>但市場有趣的地方就是上述情況又不是鐵定會發生。假如今天反而是 A, B 突然都變調，也投入 C, D 的指數基金。市場中就不再有人用 DCF 去定價了。所有人都認為權重永遠都是如下：</p>
<pre><code>i: (7.72 / 100.38) 
ii: (15.44 / 100.38)
iii: (77.22 / 100.38)</code></pre>
<p>整個 price discovery 的機制就不見了，不管底下的實際能收到的租金有多少，就是這樣定價，所以說股價都是所謂人類基金管理者亂炒作這說法並不公平，市值權重指數是一種跟隨策略，沒有人類基金管理者他也是不會動的。但這並不是一個穩定的平衡，遲早有人會跳出來點出國王的新衣，然後前面崩毀的情況就會發生。</p>
<p>從上面的邏輯推導與立論可以知道，市場大多數人都在使用某個系統時，而那個系統又因為一些背後共同問題造成時，平均就是大家一起平均死。沒錯，你拿到平均的結果，但你還是死了。但同時，請不要滑坡推論就說 Boeing 或是市值權重指數基金就是該死，身為渺小個體我們應該完全摒棄，因為</p>
<ol type="1">
<li>統計上 Boeing 跟 Airbus 造的飛機比起你自己亂造的飛機要強，儘管 Boeing 737 MAX 有問題。</li>
<li>統計上 Boeing 跟 Airbus 造的飛機比起你自己請的飛機廠商也要強，儘管 Boeing 737 MAX 有問題。</li>
</ol>
<p>但反面來說</p>
<ol type="1">
<li>統計上有瑕疵的立論並不能因此讓我不擔心 Boeing 737 MAX 的結構性問題，除非你能舉出有說服力的邏輯推論說明我舉出的情況不用擔心，因為儘管機率不高或是未知，但我一旦中了就死了。同樣對於市值權重指數基金也是，這關係到許多人畢生的積蓄。</li>
</ol>
<p>我想聲明，市值權重指數基金是非常偉大的發明，他讓大多數不懂金融的人都有機會分享到果實，能承受的資金量也非常巨大，至今都沒發生問題。</p>
<p>我並不是鼓勵說就不要使用指數基金，但與此同時，因為過去沒發生問題而認為以後也不會發生問題，不去用正確的邏輯回應問題。這種過度膨脹的心理而不去理解市值權重指數基金的結構所潛藏的缺陷並提醒大家，在我看來跟那些設計各種金融工程工具但不去點出工程極限的人沒有兩樣，都是不負責任的。</p>
<p>最後安插一個軼聞。Michael Burry 在 2000 年就成立 Scion Capital 來管理家族基金，但他也接受外部他喜歡的人的投資，早年完全是採取類似 Ben Graham 或是 Warren Buffett 在 Partnership 時代的策略，很多人是因為 The Big Short 才認識他，但不知道他是跟 Warren Buffett 同一風格的投資人。好巧不巧在 2001 年的時候，市值權重指數基金的教父曾經在 Forbes 專訪上嘴 Michale Burry。</p>
<pre><code>In the 2001 article, Forbes proposed the following as 
a working definition of a hedge fund: &quot;A hedge fund is 
any investment company that is unregulated, has limited
redemption privileges, and charges outrageous fees.&quot; 
The author also enlisted the help of John Bogle, the 
Vanguard index fund czar, to make the case against hedge 
funds. Bogle obliged, noting that it is not realistic 
for investors to expect the hedge fund industry of more 
than 6,000 funds and $500 billion in assets to outperform 
the rest of the market over the long term.

This is a reasonable observation, and I don&#39;t necessarily 
disagree with him. Unfortunately, he then went on to pick
a name at random from a hedge fund directory to disparage, 
saying: &quot;I don&#39;t know what to do about Scion Capital, 
started by Michael Burry M.D. after leaving his third year 
of residency in neurology.  He started it mostly with his 
own money, $1.4 million, and he&#39;s looking for more. His 
technique to manage risk is to buy on the cheap and, if 
he takes a short position -- I hope you&#39;re all sitting 
down for this -- it is because he believes the stock will 
decline.&quot;</code></pre>
<p>很可惜，他完全就挑錯了人來嘴，我們都知道 2008 發生了什麼事。</p>
]]></summary>
</entry>
<entry>
    <title>Workflow Automation</title>
    <link href="https://blog.paulme.ng/posts/2019-08-23-workflow-automation.html" />
    <id>https://blog.paulme.ng/posts/2019-08-23-workflow-automation.html</id>
    <published>2019-08-23T00:00:00Z</published>
    <updated>2019-08-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>In 2019 the information overflow has become a problem, not being able to fine tune my information channel has annoyed me for sometime, and they could be summarized as follows:</p>
<ol type="1">
<li>The service would mixed-in the Ads with the useful information you need. For example, if you install a comics app, apart from the newly released episode it also sends you the recommendation you are not interested into.</li>
<li>Even the service provides certain level of customization on what should be pushed. It’s not 100% matching what you really need or simply lacking some of the contents you care the most.</li>
<li>It’s the same for RSS, it only provides RSS for everything but you might be only interested into one specific category from this site.</li>
<li>Not to mention that the site doesn’t provide RSS at all and contains all of the Ads. It needs you to keep polling the site so that they could generate revenue from the Ads, or they could track your usage since the content is behind the paywall and requires your login.</li>
<li>Push notification could interrupt you at anytime, you could set it to be muted at certain time range but it is the OS level and not able to customise it to what you would prefer.</li>
</ol>
<p>I would like to have 100% control about my information consumption.</p>
<ol type="1">
<li>I could set the push notification to be delivered at certain time so that the context switch is reduced. The stress on information overflow could be eased without worrying that I would lose track of some of the information</li>
<li>Customise the push and pull model to my like.</li>
<li>Reduce the overhead of polling so that I could reclaim my hours to focus on the important stuffs. If I could reclaim 30 minutes every week, I could reclaim 2 hours a month. Given that a ballpark calculation that office workers has about 112 hours of disposable free time every month, reclaiming 2 hours is significant. (every work day for 2 hours on average and 9 hours each day for weekend excluding that you would like to get up late and fool around to get healthy rest, then in total is 28 hours per week and 112 hours per month).</li>
</ol>
<p>Initially I look up the site to see if there is any pre-existing service that I could leverage by simply just paying to automate the tasks, I tried out a few and decided to roll my own solution. Here are the ones I tried.</p>
<ol type="1">
<li><a href="https://ifttt.com/">IFTTT</a> has been around for a long time, but it is only for If-then-Else and the integration it provided are too simple. The only useful thing I could find is the task for putio.</li>
<li>iPhone’s Workflow is only for iPhone specific macro, it is not what I would like to have a constantly monitoring service and listening to the trigger.</li>
<li>Azure Workflow is better than IFTTT in terms of the complex tasks, and it is pricing model is more friendly if you don’t want to pay. However I felt its UI is unintuitive and buggy.</li>
<li>The best combination is <a href="https://zapier.com/">Zapier</a> and <a href="https://apify.com/">Apify</a>. Apify provides crawling site and turn it into the CSV and JSON, you could also set it to be cron jobs as well, if your crawling volume is not high then it’s free. Zapier maybe the most affordable integration service provider out there. The rest of them often charges several hundred dollars and it is business oriented. For multi-steps workflow you could chose the $25 a month plan from Zapier to integrate stuffs, and you could basically use Google Sheets as your database to wire everything. It is convenient by clicking through forms and set things up and you could test the setup by replay test. However, 25 dollars a month is simply too expensive for personal usage.</li>
</ol>
<p>I decided to roll my own solution in the end, the decision was based on</p>
<ol type="1">
<li>It is much much cheaper by paying 5 dollars a month to Virtual Hosting than 25 dollars a month to Zapier.</li>
<li>There are still automation that neither Apify nor Zapier could match where I need full programming environment.</li>
<li>It’s not significantly more complicated to setup headless-chrome and crawl by yourself to a developer. For sure it is not possible for non-tech-savvy group of people.</li>
</ol>
<p>headless-chrome and puppeteer maybe the best things in these two years to that could make your life easier. It makes the site very hard to tell the difference between bot and your normal activity. Therefore the content that used to be troublesome to crawl is so much easier now. For the content behind paywall you could just load your cookie from the local storage to pretend that you are logged-in and crawl the content. And for the content rendered by javascript you could simply just wait for certain elements to appear then dump the snapshot. As long as you rate-limited your requests, you don’t have to worry about the recaptcha since the site looks your activity pretty much like normal users.</p>
<p>Here is an example of crawling SeekingAlpha.</p>
<pre><code>import * as puppeteer from &#39;puppeteer&#39;;
import * as fs from &#39;fs&#39;;
import * as program from &#39;commander&#39;;

program
    .version(&#39;0.1.0&#39;)
    .option(&#39;-c, --cookie [file]&#39;, &#39;Use the cookie to crawl the website&#39;)
    .parse(process.argv);

if (program.args.length == 0) {
    program.outputHelp();
    process.exit(0)
}

let url_or_file = program.args[0];

function extractItems(): Array&lt;any&gt; {
    let single_articles = document.querySelectorAll(&#39;.author-single-article&#39;);

    var comments = [] as any;
    for (let single_article of single_articles) {
        let comment = single_article.querySelector(&#39;.author-comment-content&#39;);
        let article_link = single_article.querySelector(&#39;.article-link&#39;);

        if (comment != null &amp;&amp; article_link != null) {
            let o: any = {
                &quot;comment&quot;: comment.textContent,
                &quot;article_link&quot;: &quot;https://seekingalpha.com&quot; + article_link.getAttribute(&quot;href&quot;),
                &quot;article_title&quot;: article_link.textContent
            };

            comments.push(o);
        }
    }

    return comments
}

async function loadCookie(cookie_path, page): Promise&lt;void&gt; {
    var objects = JSON.parse(fs.readFileSync(program.cookie, &#39;utf8&#39;));
    if (objects.length) {
        for (let cookie of objects) {
            await page.setCookie(cookie);
        }
    }
}

(async () =&gt; {
    const browser = await puppeteer.launch();
    const page = await browser.newPage();

    if (program.cookie &amp;&amp; fs.existsSync(program.cookie)) {
        await loadCookie(program.cookie, page);
    }

    if (url_or_file.startsWith(&#39;http&#39;)) {
        await page.goto(url_or_file, { waitUntil: &#39;domcontentloaded&#39; });
        await page.waitForSelector(&#39;.author-comment-content&#39;);
    } else {
        let htmlContent = fs.readFileSync(url_or_file, &#39;utf-8&#39;);
        await page.setContent(htmlContent);
    }

    let items = await page.evaluate(extractItems);
    console.log(JSON.stringify(items));
    await browser.close();
})();</code></pre>
<p>It’s pretty intuitive and you could deploy the script to your virtual host. And deliver the content to Telegram as push notification. What you need to do is just to create a bot from Telegram and leverage the rubygem to send the text. The richness in Rubygem makes the glue programming quite easy and short. It’s not really significantly difficult to an experienced developer to do it than using the service like Zapier.</p>
<p>Since the bandwitdh on Virtual Host is also much faster than your home’s ADSL, it is also better to move the large files between services on the server. I could easily use the script to move the file from putio to google drive.</p>
<pre><code>#!/usr/bin/env bash
TARGET_DIR=$HOME/file_pipe

cd $HOME
filename=$(lftp ftp.put.io -u $PUTIO_PASSWD -e &quot;set ftp:ssl-allow no; cd Hook; ls; exit;&quot; 2&gt; /dev/null | awk &#39;{print $9}&#39; | head -1)

if [ -z &quot;$filename&quot; ]
  echo &#39;no file to pipe&#39;
  exit
fi

lftp ftp.put.io -u $PUTIO_PASSWD -e &quot;set ftp:ssl-allow no; cd Hook; get $filename; exit;&quot; 2&gt; /dev/null
mkdir -p $TARGET_DIR
mv $filename $TARGET_DIR
renamer.rb $TARGET_DIR/$filename

cd $HOME
for f in $(ls $TARGET_DIR)
do
    drive push -no-prompt --files $TARGET_DIR/$f
done

rm -rf $TARGET_DIR
lftp ftp.put.io -u $PUTIO_PASSWD -e &quot;set ftp:ssl-allow no; cd Hook; rm $filename; exit;&quot; 2&gt; /dev/null</code></pre>
<p>With these scripts I need to pay zero attention to the contents, the push would be sent to my Telegram, it’s all customizable and I could turn off the app notification completely. All I need to make sure there is health check script that would remind me when the server is down (with the warnings also sent by Telegram since I don’t need a full solution of health monitoring).</p>
<p>Next milestone I would turn to the browser automation by developing my own browser extension. Due to my heavy usage pattern of Read It Later, I accumulate thousands of links over the years and I am trying to design a workflow that would help me read the information more effective, but not just by dumping the information into Pocket and pretend that I would read it and create a delusion that makes myself feel satisfied. I hope that it could automatically tagging the information and create Trello card so that I could put the tasks into my personal planning priorization. Once I feel the workflow is running well I would have another post on that.</p>
]]></summary>
</entry>
<entry>
    <title>Changing the License of my open source projects</title>
    <link href="https://blog.paulme.ng/posts/2019-08-23-changing-the-license-of-my-open-source-projects.html" />
    <id>https://blog.paulme.ng/posts/2019-08-23-changing-the-license-of-my-open-source-projects.html</id>
    <published>2019-08-23T00:00:00Z</published>
    <updated>2019-08-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>After listening to the <a href="http://lucien.cc/about-2/">Lucien C.H. Lin</a>’s talk at <a href="https://coscup.org/2019/">Coscup 2019</a> about his experience on working with corporates dealing with FLOSS communities and licenses, I learn more about the practice in the industry. It also makes me to think about the license I would be using for my open source projects. <a href="http://neilmitchell.blogspot.com/2018/08/licensing-my-haskell-packages.html">This</a> article from Neil Mitchell in Haskell community explained his stance very well, and by carefully reading his article I would tend to agree with his intent. I would be inclined to change all of my projects to be Apache-2.0 and BSD-3 dual license so that it has patent grant protection and at the same time compatible with GPL. Rust is using Apache-2.0 and MIT dual license but to my personal I like the clause in BSD-3 where it requires you can’t use my name to endorse things you build, that makes sense to me. I would relicense my projects one by one from now on.</p>
]]></summary>
</entry>
<entry>
    <title>go-pdqsort - Pattern Defeating Quicksort in Go</title>
    <link href="https://blog.paulme.ng/posts/2019-08-21-go-pdqsort---pattern-defeating-quicksort-in-go.html" />
    <id>https://blog.paulme.ng/posts/2019-08-21-go-pdqsort---pattern-defeating-quicksort-in-go.html</id>
    <published>2019-08-21T00:00:00Z</published>
    <updated>2019-08-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p><a href="https://github.com/MnO2/go-pdqsort">go-pdqsort</a> is my implementation of pattern defeating sort in golang. I knew about pattern defeating sort from rust’s standard library documentation. I’ve never heard about this algorithm and it immediately intrigues my interest. I googled about it and found the original <a href="https://github.com/orlp/pdqsort">implementation</a> and their discussion threads on <a href="https://news.ycombinator.com/item?id=14661659">hacker news</a> and <a href="https://www.reddit.com/r/cpp/comments/2z6hgx/patterndefeating_quicksort/">reddit</a>. Then I read the <a href="https://paperpile.com/view/c3820723-6d9c-0a86-ab02-8e46dc22aec5">technical report</a> from Orson Peters. The key observation from the algorithm is that merely reducing the miss rate from CPU branch prediction, it would make the sorting speed greatly improved (no need to flush the cache line etc), so the technique adopted was to put the index of the array in the buckets and don’t swap them immediately, but put them in the right bin and swap them in one batch at the end of the loop. This works perfectly in the language with zero cost abstraction like C/C++ and Rust. I wasn’t sure about that would work in the heap-managed languages like golang, python and ruby etc, since most of the things are on the heap (with pointer indirection) and often results into cache misses, the impact of branch prediction miss rate might be neglectable. (For sure it would depend on the escape analysis in golang compiler, it would put the things on stack if it doesn’t escape in general). Though my hunch was that it would be slower, it still a good practice to implement the algorithm by myself so that I would get to know the details of the algorithm. And here are the results</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">implementation</th>
<th style="text-align: center;">speed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">pdqsort</td>
<td style="text-align: center;">80874 ns/op</td>
</tr>
<tr class="even">
<td style="text-align: center;">std-lib sort</td>
<td style="text-align: center;">69828 ns/op</td>
</tr>
</tbody>
</table>
<p>I am not sure about what std-lib’s sort is, but it looks like introsort since it switches to heapsort when the recursion is too deep, and it incorporates the techniques from shellsort as well, but the main part is still Bently’s quicksort technique. You can tell from the result that pdqsort is significantly slower than std-lib’s sort, probably due to the overhead of those memory batch swap, where it triggers extra allocation or cache eviction, pretty much as expected.</p>
<p>If you are interested in the algorithm, you could check out the <a href="https://github.com/MnO2/go-pdqsort">code</a> by yourself.</p>
]]></summary>
</entry>
<entry>
    <title>Travel Planning with Trello</title>
    <link href="https://blog.paulme.ng/posts/2019-08-20-travel-planning-with-trello.html" />
    <id>https://blog.paulme.ng/posts/2019-08-20-travel-planning-with-trello.html</id>
    <published>2019-08-20T00:00:00Z</published>
    <updated>2019-08-20T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>As a frequent traveller who has been to 55+ countries/regions, I know that travel planning is a time consuming process. I enjoyed the process pretty much as it would get me to know the destination and the culture more. Being to the destination is only small part of it. However, planning it with the right tool definitely would make your life much easier.</p>
<p>I was quite used to put everything into one Google Doc. I have been using the template I created for myself all over these years. I would revise the template from the experience from the journey to make sure the template is thorough and it could handle 90% of the scenario. Whenever I am about to plan my next trip, I’ll just copy from my template and dump the materials and the links to the wikitravel and blog posts into the Google Doc and use it as a draft to plan my trip.</p>
<p>Until I find a significant better way to do it. It’s part of the movement I set for myself to make most of the things in my life to be managed by Trello. I would centralize the notification/update by integrating the crawler and Trello API. But soon I found that Trello is especially suitable for travel planning. This blog post is not sponsored by Trello but I whole-heartedly think so.</p>
<p>Travel planning is a classic type of tasks that has the following properties:</p>
<ol type="1">
<li>Start from a simple idea and expanded into details and converge to a final plan</li>
<li>The final outcome is a detailed itinerary with contingency plan.</li>
<li>Itinerary alone is not enough. Analytical reporting is required since you need budgeting.</li>
</ol>
<p>Therefore at least to me, a good software for travel planning need to have the following features:</p>
<ol type="1">
<li>A pin board to collect the materials you are gonna to read.</li>
<li>Easy to re-organize and label items to suit your need or your mind map looks like.</li>
<li>Rich media supports</li>
<li>It provides APIs so that you could export and analyze it with a proper tooling.</li>
<li>Synchronization and Offline access.</li>
</ol>
<p>And Trello just matches all of them to me as the time of Aug 2019, and this is how I use it.</p>
<p>At the brainstorming phase, I would install the Send-to-Trello button to my Firefox. Whenever I looked up a relevant blog post and I don’t have time to read it full, I would use Send-to-Trello button to save it later. Usually I would create a List in Trello and name it as <code>Lead</code>.</p>
<p><img src="/images/2019-08-20/send_to_trello.png" /></p>
<p>And I would read them and create the digested steps into Cards and organize them into days. This is an iterative process, I would first focus on the ballpark schedule so that I could put everything into the my scheduled numbers of days, and make sure the big transporation items are possible to tickets in the budget. In the later iterations, I would add more detailed steps including the ticket and transfer information etc, so that I could just follow the instructions when I am on the road. The Card design just makes this iterative process easy and intuitive.</p>
<p><img src="/images/2019-08-20/trello_cards.png" /></p>
<p>With the Adds-on, Trello could even show you the Map View of your cards, it’s especially useful when you are planning the places you would like to visit in the city. You could see the obvious clusters for the markers on the map, and put them in the same day.</p>
<p><img src="/images/2019-08-20/trello_map_view.png" /></p>
<p>Also I would leverage on the Custom Fields adds-on to create extra field to document the cost and duration of the transportation. These are for further analysis at the later phase.</p>
<p>After finishing the planning and make sure I was correctly labeling the cards. I would run a script against Trello API to generate the Google Sheets budget spreadsheet.</p>
<pre><code>#!/usr/bin/env ruby

require &quot;google_drive&quot;
require &#39;trello&#39;
require &#39;time&#39;
require &#39;json&#39;

spreadsheet_id = &#39;&lt;Your Spread Sheet ID&gt;&#39;
target_currency = &#39;TWD&#39;

config_file = File.read(&quot;config/trello.json&quot;)
config_for_trello = JSON.parse(config_file)

Trello.configure do |config|
  config.developer_public_key = config_for_trello[&#39;developer_public_key&#39;] 
  config.member_token = config_for_trello[&#39;member_token&#39;] 
end

cards = {}

Trello::Board.find(&#39;&lt;Your Board ID&gt;&#39;).lists.each do |list|
  Trello::List.find(list.id).cards.each do |card|
    cards[card.id] = { :name =&gt; card.name }
    card.custom_field_items.each do |item|
      pp item.custom_field_id
      if item.custom_field_id == &#39;5d5978f014ab2f17fcb6a2cd&#39;
        if not item.value[&#39;number&#39;].nil?
          cards[item.model_id][:cost] = item.value[&#39;number&#39;]
        end
      elsif item.custom_field_id == &#39;5d59795638dd318d1a53471a&#39;
        if not item.option_value().nil?
          cards[item.model_id][:currency] = item.option_value()[&#39;text&#39;]
        end
      end
    end
  end
end

pp cards

row = 1
session = GoogleDrive::Session.from_config(&quot;config/google.json&quot;)
ws = session.spreadsheet_by_key(spreadsheet_id).worksheets[0]

cards.each do |card_id, card|
  if not card[:cost].nil? and not card[:currency].nil?
    ws[row, 1] = card[:name]
    ws[row, 2] = card[:cost]
    ws[row, 3] = card[:currency]
    ws[row, 4] = target_currency

    row += 1
  end
end
ws.save</code></pre>
<p>And here is what it looks like in Google Sheets.</p>
<p><img src="/images/2019-08-20/budget_table.png" /></p>
<p>I used it to plan my trip to Kyoto in the upcoming month, and it is so much better than what I was doing by Google Doc, reducing the big share of time to do copy pasting and adjusting the format. I would use this method to plan my trip in the future.</p>
]]></summary>
</entry>
<entry>
    <title>logq - Analyzing log files in SQL with command-line toolkit, implemented in Rust</title>
    <link href="https://blog.paulme.ng/posts/2019-08-16-logq---analyzing-log-files-in-sql-with-command-line-toolkit%2C-implemented-in-rust.html" />
    <id>https://blog.paulme.ng/posts/2019-08-16-logq---analyzing-log-files-in-sql-with-command-line-toolkit%2C-implemented-in-rust.html</id>
    <published>2019-08-16T00:00:00Z</published>
    <updated>2019-08-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p><a href="https://github.com/MnO2/logq">logq</a> is my latest project and it has reached a reasonably qualitative milestone for me to comfortably introduce it and share about its technical detail.</p>
<p>Let’s start by an example. Imagine that you are in the middle of troubleshooting a production incident and you are suspecting a certain endpoint from the web-server is having problem and result into high latency. Since it is application level and it is not provided by the AWS CloudWatch. It is also the first time that it happened so there isn’t any in-house datadog or application level instarumentation setup. And it occurs to you that the access log contains the relevant information and it would be possible for you to calculate the metrics from the log. You download the log from the archive storage and piece together an ad-hoc script in 30 minutes and run the metrics against the log, and the script is implemented in python it gets to pretty slow if the log size is large. Wouldn’t it be great if there were command line where you could handle these kind of ad-hoc analysis situation easily? Where no extra dependency setup like ELK or library is needed. That is the motivation to drive to the development of <a href="https://github.com/MnO2/logq">logq</a>, where you could answer the question of “What are the 95th latencies with 5 seconds time frame against application handlers by ignoring the second path segments” easily.</p>
<pre><code>&gt; logq query &#39;select time_bucket(&quot;5 seconds&quot;, timestamp) as t, url_path_bucket(request, 1, &quot;_&quot;) as r, percentile_disc(0.95) within group (order by sent_bytes asc) as bps from elb group by t, r&#39; data/AWSLogs.log
+----------------------------+----------------------------------------------------------------------+----------+
| 2019-06-07 18:45:30 +00:00 | /img/_/300/2r0/54558148eab71c6c2517f1d9.jpg                          | 0.046108 |
+----------------------------+----------------------------------------------------------------------+----------+
| 2019-06-07 18:45:30 +00:00 | /img/_/300/2r0/546db9fd11bcd3c15f0a4ada.jpg                          | 0.073435 |
+----------------------------+----------------------------------------------------------------------+----------+
| 2019-06-07 18:45:30 +00:00 | /img/_/bound/2r0/559153d381974f0e5289a5e4.webm                       | 0.055385 |
+----------------------------+----------------------------------------------------------------------+----------+
| 2019-06-07 18:45:30 +00:00 | /api/_/conversation/5590921f9d37a84e20286e3e                         | 0.04181  |
+----------------------------+----------------------------------------------------------------------+----------+
| 2019-06-07 18:45:35 +00:00 | /img/_/bound/2r0/551c6e78a107308d47055c96.webp                       | 0.063701 |
+----------------------------+----------------------------------------------------------------------+----------+
...</code></pre>
<p><a href="https://github.com/MnO2/logq">logq</a> inspired by my own need at daily work when troubleshooting production incident. I noticed that there are a few drawbacks by using glueing scripts to analyze the log</p>
<ol type="1">
<li>You spend a lot of time to parse of the log format, but not focus on calculating the metrics helping to troubleshoot your production issues.</li>
<li>Most of the log formats are commonly seen and we should ideally abstract it and have every one benefit from the shared abstraction</li>
<li>For web-server log cases, the log volume usually is huge, it could be several hundred MB or even a few GB. Doing it in scripting langauges would make yourself impatiently waiting it is running at your local.</li>
</ol>
<p>For sure you could finely tuned the analytical tooling like AWS Athena or ELK to analyze the very large volume of logs in tens of or hundreds of gigabytes, but often times you just want to ad-hocly analyze logs and don’t bother to set things up and cost extra money. Also, the modern laptop/PC is actually powerful enough to analyze gigabytes of log volumes, just that the implementation is not efficient enough for doing that. Implementing <a href="https://github.com/MnO2/logq">logq</a> in Rust is in hope to resolve those inconvenience and concerns.</p>
<p>To check out more query examples, please check out the <a href="https://github.com/MnO2/logq/blob/master/README.md">README</a> it has the commonly used queries (at least to the author).</p>
<p>The rest part of the blog post would be focus on the technical detail I faced when I implemented it in Rust.</p>
<h2 id="using-nom-to-parse-the-sql-syntax">Using nom to parse the SQL syntax</h2>
<p>When I was evaluating the parsing approach, I was considering among <a href="https://github.com/Geal/nom">nom</a>, <a href="https://github.com/Marwes/combine">combine</a>, <a href="https://github.com/pest-parser/pest">pest</a> and rolling my own lexer / parser. Initially I was rolling my own lexer / parser since it is the approach what most of the modern industrial strength compilers would do to cope with the incremental parsing and better error message etc. However I found it too laborious to do it at the prototype phase. I turned to look for a parser combinator library instead soon. I wasn’t considering <a href="https://github.com/pest-parser/pest">pest</a> since I am not familiar with PEG and I had the impression where the parser generated from generator are hard to fine tuned for better error message based on my experience of using lex/yacc. Therefore the only options left are <a href="https://github.com/Geal/nom">nom</a> and <a href="https://github.com/Marwes/combine">combine</a>. I was intimidated by the macro approach adopted by <a href="https://github.com/Geal/nom">nom</a> (before 5). I read the blog post I could understand it is due to the limitation in the compiler but it’s just looks like a horse hard to harness if anything goes wrong due to macro system. I feel a pity not be able to adopt a library where it is fine tuned for performance. Just when I was about to turn to <a href="https://github.com/Marwes/combine">combine</a>, I found that starting from <a href="https://github.com/Geal/nom">nom</a> version 5, it is no longer required to use macro approach. The only drawback is that the documentation for nom 5’s complete new approach is still lacking, but it looks like I could quickly grasp its concept since I had experience of using <a href="http://hackage.haskell.org/package/parsec">Parsec</a> in Haskell. And in the end it is the library I decided to stick with. The type like <code>IResult</code> and its type parameters are easy to understand, and most of the combinators are battery included in the library, but it still took me some time to figure it out when the thing I would like to do is not included.</p>
<h3 id="parsing-identifier">Parsing identifier</h3>
<p>It wasn’t so clear on how to express the rules of a valid identifier</p>
<ol type="1">
<li>It consists of underscore, alphabet, digit</li>
<li>It should not start with number</li>
<li>It shouldn’t be all underscore</li>
</ol>
<p>After checking out the <a href="https://github.com/Geal/nom/blob/master/examples/s_expression.rs#L74">example</a> provided by the nom repository, I found that you could actually do it in procedural, pretty much you would do if you roll your own parser with <code>Result</code> as the returning type. In nom’s case you just need to rely on the already provided <code>ParseError</code> and <code>split_at_position1_complete</code> instead. By providing helping function working on the character level then it is easy to do that.</p>
<pre><code>fn identifier&lt;&#39;a, E: nom::error::ParseError&lt;&amp;&#39;a str&gt;&gt;(input: &amp;&#39;a str) -&gt; IResult&lt;&amp;&#39;a str, &amp;&#39;a str, E&gt;
where
{
    fn is_alphanumeric_or_underscore(chr: char) -&gt; bool {
        chr.is_alphanumeric() || chr == &#39;_&#39;
    }

    fn start_with_number(s: &amp;str) -&gt; bool {
        if let Some(c) = s.chars().next() {
            &quot;0123456789&quot;.contains(c)
        } else {
            false
        }
    }

    fn all_underscore(s: &amp;str) -&gt; bool {
        for c in s.chars() {
            if c != &#39;_&#39; {
                return false;
            }
        }

        true
    }

    let result = input.split_at_position1_complete(
        |item| !is_alphanumeric_or_underscore(item.as_char()),
        nom::error::ErrorKind::Alpha,
    );

    match result {
        Ok((i, o)) =&gt; {
            if start_with_number(o) || all_underscore(o) || KEYWORDS.contains(&amp;o) {
                Err(nom::Err::Failure(nom::error::ParseError::from_error_kind(
                    i,
                    nom::error::ErrorKind::Alpha,
                )))
            } else {
                Ok((i, o))
            }
        }
        Err(e) =&gt; Err(e),
    }
}</code></pre>
<h3 id="precedence-climbing-expression-parsing">Precedence climbing expression parsing</h3>
<p>Traditionally to parse an expression with precedence, you either encode it in the grammar or use bottom-up approach. It is a long resolved problem and the early day approach is Dijkstra’s Shunting Yard algorithm. However, most of the manually implemented modern industrial strength compiler use recursively descent approach, which is top down. One approach is by switching to the bottom-up approach when parsing to a point where operator precedence parser is needed. I think it is also possible to do that with parser combinator approach but we need to treat an operator precedence parser as a standalone black box and it doesn’t combine elegantly with the rest of the code. And here is the point where I would like to introduce Precedence climbing algorithm. It just combine so elegantly with the parser combinator approach, since it is a top down approach it just looks seamlessly with the rest of the combinators’ code. If you are not familiar with Precedence climbing, Eli Bendersky has a very good <a href="https://eli.thegreenplace.net/2012/08/02/parsing-expressions-by-precedence-climbing">introduction</a>.</p>
<p>In just a few short lines of code and the whole precedence parsing problem is resolved.</p>
<pre><code>fn parse_expression_at_precedence&lt;&#39;a&gt;(
    i0: &amp;&#39;a str,
    current_precedence: u32,
    precedence_table: &amp;HashMap&lt;String, (u32, bool)&gt;,
) -&gt; IResult&lt;&amp;&#39;a str, ast::Expression, VerboseError&lt;&amp;&#39;a str&gt;&gt; {
    let (mut i1, mut expr) = parse_expression_atom(i0)?;
    while let Ok((i2, op)) = parse_expression_op(i1) {
        let (op_precedence, op_left_associative) = *precedence_table.get(op).unwrap();

        if op_precedence &lt; current_precedence {
            break;
        }

        let (i3, b) = if op_left_associative {
            parse_expression_at_precedence(i2, op_precedence + 1, precedence_table)?
        } else {
            parse_expression_at_precedence(i2, op_precedence, precedence_table)?
        };

        let op = ast::BinaryOperator::from_str(op).unwrap();
        expr = ast::Expression::BinaryOperator(op, Box::new(expr), Box::new(b));

        i1 = i3;
    }

    Ok((i1, expr))
}</code></pre>
<p>It also worths to mention that I was actually got lazy to encode the precedence in the grammar when I was doing the syntax part in the first iteration and leave the precedence parsing to later to resolve. It is a major refactoring when the project need to shift the approach when I was affirmed that the precedence climbing is the way to go. Without Rust’s type system’s help, it is a hard take. I only needed to follow the compiler’s complaint and clear out all of the warnings/errors then the task is pretty much done. I simply could not imagine how I could do that in a dynamic-typed programming language.</p>
<h2 id="failure-crate">Failure crate</h2>
<p>I would like to briefly talk about the <code>failure</code> crate as well. I have had experience of using <code>failure</code> crate in some of the mini projects. At the time I was feeling that it is quite convenient that you could derive everything by putting the label on the Error struct. In this larger project, with the help of <code>impl From</code> and <code>#[cause]</code> label, and <code>?</code> operator. I have the strong feeling that it help shape your structuring of error handling into monadic style. <code>impl From</code> plays the role kind of like monad-transformer where lift the application from one kind of error monad to another kind, but without having you to stick your head around complicated type combination but it could influence you to put down the error handling in the similar structure.</p>
<h2 id="compiling-sql-to-graph">Compiling SQL to Graph</h2>
<p>Then here comes the core part. Writing a compiler is all about parsing and manipulating on graphs. For translating SQL into computation graph is relatively elegant than translating other procedural programming languages, since if you think it thoroughly, it is translating into the the structure that is pretty functional.</p>
<p>For a simple select statement, like “select a from foo”. It is actually could be translated into a term you are familiar in functional world. That is <code>map</code>. Suppose that you <code>foo</code> table has its schema defined as (“a”, “b”, “c”). Then “select a” is just saying, please project to “a” from (“a”, “b”, “c”), or you could say it a synonym to <code>map</code>.</p>
<p>For the <code>from</code> clause, it is just saying a data source where you could stream the data record from, it could be a file name or any other data source, either physically or virtually referred to.</p>
<p><code>where</code> clause is a no brainer, it is just interpreting a <code>filter</code> against the projected data records from <code>map</code>. The expressions to evaluated against are provided to <code>where</code> in the form of <code>a = 1</code> etc.</p>
<p>The relatively complicated is <code>group by</code>, but it is simply just doing a <code>fold</code> against a key-value data structure, with the key as the distinct values specified by the fields in the <code>group by</code> clause, and the rest of the columns in the values, or sent to the “aggregator”. If you are familiar with Haskell you know that HashMap is <a href="http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Foldable.html#t:Foldable">foldable</a>, group by is basically a fold against HashMap if it is the backing storage. In other languages like ruby, <code>reduce</code> is probably the method used in this kind of action instead.</p>
<p>So to sum up, the whole SQL translation is basically just parse the abstract the syntax tree into a map-reduce chain, streaming the source data from the upstream and produce it to the downstream. in the main stream language this type of structures are called <code>iterator</code>, so it could be seen as a series of iterators chained together as well.</p>
<p>For sure there are details where you would like to rewrite the whole graph to remove the const expression etc, or push down the complicated logic into the data source if the data source supports complicated compute logic (in practice they are smart key-value store supports transaction and cursor). But the spirit of the whole compilation is pretty much as the above mentioned.</p>
<h2 id="t-digest">T-Digest</h2>
<p>In <a href="https://github.com/MnO2/logq">logq</a>, the aggregate function of <code>percentile_disc</code> is supported since it is a common need to calculate the 99th percentile of latency in analyzing web traffic. However, to calculate the exact percentile you have to sort the sequence, which is a very expensive operation if the log file size is huge. <code>percentile_disc</code> in logq is compiled to group by with sorting in each of the partition, and it would remember every elements so that it is able to sort.</p>
<p>In order to make it more efficient in terms of the memory footprint and to support the parallel aggregation feature on the roadmap, I implemented <code>approx_percentile</code> aggregate function by T-Digest algorithm. T-Digest is an interesting data sketching algorithm. It leverages on the “scaling function” to make the both ends of the percentiles (for example, 1th percentile or 99th percentile) very accurate and the percentile in the middle less accurate. It matches the use cases we often need. The core concept behind is still by binning the data points, just that it uses scaling function to control the binning process, it makes the binning on both ends more granular so that we could keeps the info as the original as possible and the binning in the middle more blunt (larger binning size). And the choice of the scaling function is pretty flexible, the only requirements are that it has to be non-decreasing, and its derivative starts at 0 and ends at 1. My implementation is based on what facebook’s folly provided. In the original paper and many reference implementation it is using <code>arcsin</code>, but in folly it is using sqrt since it more computation efficient, and practically not much statistical impact.</p>
<p>I released the T-Digest part as a separate <a href="https://crates.io/crates/tdigest">tdigest</a> crate. It is not fully tested with generated statistical data, but it should be reasonably correct since I am following folly’s implementation.</p>
<h2 id="post-words">Post Words</h2>
<p>This is the first non-trivial project implemented in Rust, the line reported by <code>cloc</code> on tag <code>v0.1.4</code> is around 4800 lines of Rust.</p>
<pre><code>github.com/AlDanial/cloc v 1.82  T=0.05 s (305.6 files/s, 120809.5 lines/s)
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Rust                            13            651             17           4835
YAML                             1              0              0             32
-------------------------------------------------------------------------------
SUM:                            14            651             17           4867
-------------------------------------------------------------------------------</code></pre>
<p>To do major refactoring on this size of code usually it is a pain but at least so far I can still feel comfortable to do any major changes without breaking too much things.</p>
<p>There are plenty of features I plan to put on the roadmap, performance optimization is definitely on the top of the list since I didn’t consider too much performance when I coded the proof of concept. There are plenty of room to speed up. Other things on the roadmap are</p>
<ul>
<li>Conforms to the much more complicated SQL syntax as sqlite</li>
<li>Performance optimization, avoid unnecessary parsing</li>
<li>More supported functions</li>
<li>time_bucket with arbitrary interval (begin from epoch)</li>
<li>Window Function</li>
<li>Implementing approximate_percentile_disc with t-digest algorithm when the input is large.</li>
<li>Streaming mode to work with tail -f</li>
<li>Customizable Reader, to follow GoAccess’s style</li>
<li>More supported log format</li>
<li>Plugin quickjs for user-defined functions</li>
</ul>
<p>Hope I can constantly have spare time to work on this many things on the roadmap.</p>
]]></summary>
</entry>
<entry>
    <title>「ProtonMail」に移行して、「Gmail」を辞めます</title>
    <link href="https://blog.paulme.ng/posts/2019-07-30-%E3%80%8Cprotonmail%E3%80%8D%E3%81%AB%E7%A7%BB%E8%A1%8C%E3%81%97%E3%81%A6%E3%80%81%E3%80%8Cgmail%E3%80%8D%E3%82%92%E8%BE%9E%E3%82%81%E3%81%BE%E3%81%99.html" />
    <id>https://blog.paulme.ng/posts/2019-07-30-%E3%80%8Cprotonmail%E3%80%8D%E3%81%AB%E7%A7%BB%E8%A1%8C%E3%81%97%E3%81%A6%E3%80%81%E3%80%8Cgmail%E3%80%8D%E3%82%92%E8%BE%9E%E3%82%81%E3%81%BE%E3%81%99.html</id>
    <published>2019-07-30T00:00:00Z</published>
    <updated>2019-07-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Googleが想像以上にいろんな情報を取得した。何か調べ物がある時はGoogleで検索し、ブラウザはChromeを使って、メールはGmailで送信し、どこかに行く時Googleマープで所在地を確認し、YouTubeの動画を見て、Googleドライブに入れて、仕事はGoogle Docsを使いて。すべての閲覧履歴をGoogleが保存します。インターネット生活のほぼ全てをGoogleに絡め取られているの事は、良いことではない。そのため、Googleの依存度を減らしたい。</p>
<p>最初の辞めるのサービスは Gmail。メールは全ての情報は含まれている、クレジットカードでも、応募の情報でも、旅行情報でも、メールサービスは自分の情報を取得している、それが飛行機のチケット予約すると、Googleがカレンダーに予約を作成できるの理由。</p>
<h2 id="protonmail-の特徴">ProtonMail の特徴</h2>
<p>ProtonMailのメールデータは全て暗号化された状態でサーバー上に保存されています。データはすべてのステップで暗号化されていることから傍受のリスクはないとのこと。もっと知りたいたら、<a href="https://www.ted.com/talks/andy_yen_think_your_email_s_private_think_again?language=en">Andy Yen: Think your email’s private? Think again</a>の TED トークの動画を見てください。</p>
<h2 id="購入後1ヶ月間使用した感想">購入後1ヶ月間、使用した感想</h2>
<p>移入が大体上手くいく、ProtonMail はカスタムドメインをセットアップすることがあります。paulme.ng のドメインをProtonMailに移行しました。スパムフィルタが ProtonMail より Gmail の方がうまい。ProtonMailのフィルタはが手動セットアップあとで、メールの分類正しいになりました。暗号化と便利のトレード・オフがと思います。暗号化したら、情報を取得できないだから、訓練データになし、機械学習的手法を使っていない、スパムメッセージの分類器を作っていこどができません。それ以外は、満足しています。</p>
]]></summary>
</entry>
<entry>
    <title>Managing references with Paperpile</title>
    <link href="https://blog.paulme.ng/posts/2019-07-28-managing-references-with-paperpile.html" />
    <id>https://blog.paulme.ng/posts/2019-07-28-managing-references-with-paperpile.html</id>
    <published>2019-07-28T00:00:00Z</published>
    <updated>2019-07-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>For a long time I have the problem of managing papers and slides I am reading. It’s not only for the papers and slides from computer science, but also the financial reports and the letters to shareholder for my investment readings and researches. Over time I just downloaded a bunch of PDFs to my laptop and they scattered around in my Download folder without a systematic naming and indexing. It makes it hard to clean up the disk space for my laptop since I am not sure which one is what I needed if not by opening them up and review them one by one. It makes thing painful when you are backing up your system and migrating to the other. A couple of weeks back I found that I need to resolve this problem.</p>
<p>An ideal reference management software to me in 2019 should provide the following features.</p>
<ol type="1">
<li>To be able to store in the cloud so that you are able to access it everywhere. It would be great to let you choose among the mainstream cloud storage providers, and provide options to make some of the papers accessible offline, but not sync everything to local.</li>
<li>Good metadata auto completion. Not everything has a standard to label it or a good database to cross match and automatically label them, but it should be good for the parts that we are able to do it.</li>
<li>Cross platform and reasonably performant.</li>
<li>Providing API so that you could customise it for personal needs.</li>
</ol>
<p>I know the famous softwares like Zotero, Mendeley and Endnote. They have existed for a long time, but all of them feel more focus for scientific editing. I played with them for a little bit and Zotero maybe the closest but it still feels quite the exact thing I want. Until I noticed a new upcoming competitor: Paperpile. They seem to launch for some years but not that widely well-known. I tried it out and it gives me the feeling it is the closest reference management software I would like. I have lots of PDFs and my main purpose is not for scientific editing, I just need a software to edit it and help me rename them, and easy to look things up. It is a big-bonus to me that Paperpile is based on Google Drive. It resolves the issue 1, 3, 4 by leveraging on Google Drive as the backing storage. With Google Drive you could choose which file to stay offline and providing Google Drive API to let you trigger the hook, for example, to send a new card to my Trello when a new paper is added. The downside for sure is vendor lock-in, but it’s less a priority comparing to solving my need.</p>
<p>I have tried it out for 3 weeks and I can say it is the software that’s closest to my ideal, though not perfect yet. I would stick to it before anything even better come up.</p>
]]></summary>
</entry>
<entry>
    <title>cedarwood: Efficiently-Updatable Double Array Trie in Rust</title>
    <link href="https://blog.paulme.ng/posts/2019-07-14-cedarwood%3A-efficiently-updatable-double-array-trie-in-rust.html" />
    <id>https://blog.paulme.ng/posts/2019-07-14-cedarwood%3A-efficiently-updatable-double-array-trie-in-rust.html</id>
    <published>2019-07-14T00:00:00Z</published>
    <updated>2019-07-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p><a href="https://github.com/MnO2/cedarwood">Cedarwood</a> is an effort to <a href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html">speed up jieba-rs</a>, an efficient implementation of trie is needed in order to satisfying the following needs.</p>
<ul>
<li>To be able to list the prefixes that exist in the dictionary with a given string. For example, given the dictionary to be <code>["a", "ab", "abc", "z", "xz", "xy"]</code> and the input string <code>abcdefg</code>. We should be able to efficiently list the prefixes <code>["a", "ab", "abc"]</code> that exist in the dictionary.</li>
<li>Due to the support of dictionary provided dynamically from the user, we need to support dynamic insertion into the data structure and at the same time still get us efficient common prefixes operation.</li>
<li>To be able to efficiently tell if the given string is in the dictionary or not.</li>
</ul>
<p>As mentioned <a href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html">previously</a> that aho-corasick performs quite slow from my brief testing, I didn’t spend extra time to dig out why and I just accepted it. My focus was turned to refine the double array trie implementation, where at the time was <a href="https://github.com/andelf/rust-darts">rust-darts</a>. It performs quite well on the reading operations but due to its implementation was based on the <a href="http://chasen.org/~taku/software/darts/">darts</a>, a C++ implementation of static double array trie. It failed the requirement 2 mentioned above. Either we have to drop the API and getting farther away from the original python’s implementation of Jieba, or we have to find alternatives to support dynamic insertion. I did contribute to the <code>rust-darts</code> with <a href="https://github.com/andelf/rust-darts/pull/25">dynamic insertion</a>, it resolves the feature lacking issue but the speed is still quite slow. It would make the loading time and testing time for jieba to be slow as well. Then I implemented the techniques mentioned <a href="https://linux.thai.net/~thep/datrie/#Alloc">here</a> to make the trie building time much faster (from 30+ seconds down to around 5 seconds on my machine). It is much much better but I still think if we can do any better. To be fair to the <code>darts</code> implementation, the vanilla double array trie was invented by Aoe in the 1989 paper: <a href="https://ieeexplore.ieee.org/document/17222">An efficient digital search algorithm by using a double-array structure</a> with mostly read operation in mind. It did support the update operations but it also said the update speed is quite slow. There are a few techniques to speed the update operations up and I already implemented <a href="https://linux.thai.net/~thep/datrie/#Alloc">one of them</a> for the clean build.</p>
<h2 id="look-for-better-solutions">Look for better solutions</h2>
<p>I googled around to see if there are any better solution, then I found <a href="https://en.wikipedia.org/wiki/HAT-trie">HAT Trie</a>, which seems to be the state of the art implementation of the trie, with the cache in consideration. It is based on the paper published by Askitis Nikolas and Sinha Ranjan: <code>HAT-trie: A Cache-conscious Trie-based Data Structure for Strings.</code>. And it is good that <a href="https://github.com/Tessil">Tessil</a> already had <a href="https://github.com/Tessil/hat-trie">C++ implementation</a> on github with <a href="https://github.com/Tessil/hat-trie/blob/master/README.md">very detailed benchmark comparisons</a>. However, when I looked at the interface provided it seems like it only supports finding the longest prefix, and iterating through the string predictions in the dictionary matching a given prefix. It doesn’t provide the interface for the requirement 1. I looked into the <a href="https://github.com/Tessil/hat-trie/blob/master/include/tsl/htrie_hash.h#L1574">code</a> and it seems that it could potentially exposed from the implementation to support that, just that I am not familiar enough of the algorithm to do that. The concept of HAT-trie is also based on hash table with the concept of “burst” is unfamiliar to me, and since it is stored in unordered, my hunch stopped me from investing my time further to read the not-so-simple algorithm. Though the hindsight now is that the operation I need is not based on the requirement of the order in keys but the traversal of the trie, which should be supported by the longest prefix operation, just need to change the implementation. I would do that if I have more time.</p>
<h2 id="cedar-and-cedarwood">Cedar and Cedarwood</h2>
<p>The main character in this post is actually one of the candidates listed in the benchmark provided by Tessil. An double array trie implementation named <a href="http://www.tkl.iis.u-tokyo.ac.jp/~ynaga/cedar/">cedar</a> caught my eyes, since its update seems to be very efficient, it is comparable with the other fast implementations in the list. And it’s read access speed is also on the same ballpark with HAT-trie in the Dr. Askitis dataset, where the median key length is shorter than what is in the wikipedia title dataset. It definitely worths a closer look to me. The read access speed seems to be slower than HAT-trie in the long key length cases but for the Chinese segmentation scenario, the shorter key seems to be the case. We are not working on a query completion use case anyway. The author of cedar is <a href="http://www.tkl.iis.u-tokyo.ac.jp/~ynaga/">Naoki Yoshinaga</a>. He pointed us to the paper of <code>A Self-adaptive Classifier for Efficient Text-stream Processing</code> for further reading on his webpage but it is mostly an NLP paper but not a data structure paper, it only has one or two paragraphs describing the working on his improvement on double array trie. In the paper he cited another one, but you couldn’t find the paper on any English website, even scihub. It turned out it is a paper in Japanese and I found the pdf on the Japanese website with the title of <code>タブル配列による動的辞書の構成と評価</code>. Even though I can understand basic to intermediate Japanese, it still didn’t address that much into the detail in that Japanese paper. Therefore I decided to read the code directly.</p>
<p>It is a header-only C++ implementation and the code is very much shortened to reduce the header size, it is generally ok to read with a C++ formatter from IDE or VSCode, but it also took me a while to get the core concept behind it on improvement techniques due to the lack of comments. And with the <code>cedarwood</code> rust porting implementation, I added much more comments so it should be much easier to understand how it is working, but it is a required reading to read the original double array trie paper so that at least you know how the <code>base</code> and <code>check</code> works in double array trie cases. The following I’ll briefly talk about the concept behind the skills.</p>
<h2 id="the-key-concepts-in-the-algorithm">The Key Concepts in the Algorithm</h2>
<p>The inefficiency in the update operation of the vanilla double array trie is caused by the free slot searching. The original paper simply implies that you could use brute-force approach to iterate through the index in <code>check</code> and see if they are marked as owned, and iterate through until the location where you have the free slots distribution exactly match what you want (suppose you are relocating an existing trie node to a new free location). The technique specified <a href="https://linux.thai.net/~thep/datrie/#Alloc">here</a> is basically leveraging on the value space on each block, and you can use negative integer to specify the free slot location and make them into an array-index based doubly linked-list, and you could make the brute forte iteration down to a fast-skipping linked list traversal. However, that technique doesn’t address the case where you still need to iterate your char set (2^21 in the unicode scalar case, or 2^8 if you are working on UTF-8) to check every slot and make sure the slots distributions match your need. The technique used in cedar is basically to address this problem, by maintain the bookkeeping of two kind of information: <code>NInfo</code> and <code>Block</code></p>
<pre><code>struct NInfo {
    sibling: u8, // the index of right sibling, it is 0 if it doesn&#39;t have a sibling.
    child: u8,   // the index of the first child
}</code></pre>
<pre><code>struct Block {
    prev: i32,   // previous block&#39;s index, 3 bytes width
    next: i32,   // next block&#39;s index, 3 bytes width
    num: i16,    // the number of slots that is free, the range is 0-256
    reject: i16, // a heuristic number to make the search for free space faster, it is the minimum number of iteration in each trie node it has to try before we can conclude that we can reject this block. If the number of kids for the block we are looking for is less than this number then this block is worthy of searching.
    trial: i32,  // the number of times this block has been probed by `find_places` for the free block.
    e_head: i32, // the index of the first empty elemenet in this block
}</code></pre>
<p>The <code>NInfo</code> is probably standing for “Trie Node Information”, it maintains the trie parent-children and siblings information, since these are the information needed when relocating the trie node around. You have to know which node has smaller size in its children, you could just traverse down and counting the number of children one by one. And you could iterate through the sibling chain when you really need to do the move and compare if the slots fit the potential free spaces.</p>
<p>As for <code>Block</code>, it maintains the book-keeping of the free space selection, simulating how you look at it of the each block (256 in size) when you squint at the data structure. It book keeps the information like how many free slots this block still have so that you could quickly prune the branch if the number of the free slots is less than the number of children the trie node you are relocating. Further more, the algorithm categorize the blocks into three kinds</p>
<ul>
<li>Full: The block where all of the slots are taken</li>
<li>Closed: The block where all of the slots are taken except for one</li>
<li>Open: The rest, but most of the time it is the free slots that just allocated at the end of the <code>base</code> and <code>check</code> when you resize the array.</li>
</ul>
<p>Each of them is put in the doubly-linked-list of their own kind. During the search process, you only need to look for the free space from <code>Closed</code> type block if you are inserting a single-child node, since it only needs one slot. You only need to look for <code>Open</code> block when you are relocating a node with more children. And since it is linked list all you need to do is just insert and remove the block from the linked-list, which is constant time. And insert / remove from the right kind of linked list after you update the node with inserted <code>label</code>.</p>
<h2 id="benchmarks">Benchmarks</h2>
<p>With the above algorithm mentioned, let’s take a look of <code>cedarwood</code>’s benchmark, the rust implementation of <code>cedar</code>. My laptop’s spec is as follows:</p>
<pre><code>MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports)
2.5 GHz Intel Core i7
16 GB 2133 MHz LPDDR3</code></pre>
<p>And here is the benchmark against C++ version and <code>rust-darts</code></p>
<h3 id="build">Build</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">impl</th>
<th style="text-align: left;">time</th>
<th style="text-align: left;">version</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">cedar</td>
<td style="text-align: left;">71 ms</td>
<td style="text-align: left;">2014-06-24</td>
</tr>
<tr class="even">
<td style="text-align: left;">cedarwood</td>
<td style="text-align: left;">64 ms</td>
<td style="text-align: left;">0.4.1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">rust-darts</td>
<td style="text-align: left;">201 ms</td>
<td style="text-align: left;">b1a6813</td>
</tr>
</tbody>
</table>
<h3 id="query">Query</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">impl</th>
<th style="text-align: left;">time</th>
<th style="text-align: left;">version</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">cedar</td>
<td style="text-align: left;">10 ms</td>
<td style="text-align: left;">2014-06-24</td>
</tr>
<tr class="even">
<td style="text-align: left;">cedarwood</td>
<td style="text-align: left;">10 ms</td>
<td style="text-align: left;">0.4.1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">rust-darts</td>
<td style="text-align: left;">18 ms</td>
<td style="text-align: left;">b1a6813</td>
</tr>
</tbody>
</table>
<p>For the <code>rust-darts</code> it is significantly slower than both of <code>cedar</code> and <code>cedarwood</code>, and it has extra requirement on the construction of the dictionary to be that the dictionary has to be lexicographically sorted. For <code>cedar</code> building contains the time for file reading (with 65536 buffer size) so it actually even slightly faster, but the querying time contains memory-only operation. I am too lazy to change the benchmarking code since it is using no-copying techniques by moving the pointer until the next new line and it is troublesome for me to change the rust implementation to be the same way so that to compare apple to apple. The <code>cedar</code> code was compiled with <code>-O3</code> and rust’s code is compiled with <code>--release</code> and only measure the time for the operations in the memory. You can see that the build time, that is <code>update</code> are roughly the same for C++ and Rust. And in the query case the Rust version is slightly slower than C++ but basically comparable. I couldn’t find where I could further optimize the Rust version since the code for the lookup case is quite simple and not much space to optimize on the source code level without going to the <code>unsafe</code> resort. Not sure where I could tune to speed it up, please let me know if you identify the part I could do further.</p>
<h2 id="lesson-learned">Lesson Learned</h2>
<p>Rust’s implementation is as comparable as C++’s implementation without much effort, I don’t need to go with <code>unsafe</code> and keep the majority of the code compiler-checked for memory safety. There is only one place where I need to go by <code>unsafe</code> block due to linked-list update but not due to the performance. There are a few handy features in C++ that I would miss, like const generics to make the data structure parameterized on the type level, but overall it is not a big issue in the cedar case. Or template specialization where you could leverage on different implementations based on the type you specify. Since <code>cedarwood</code> only save the word id in normal mode. It might require that for the support of reduced-trie feature where the value is stored in place, but right now <code>cedarwood</code> only has limited support on that so it is no big issue. Overall it is a smooth experience on porting.</p>
]]></summary>
</entry>
<entry>
    <title>最佳化 jieba-rs 中文斷詞性能測試 (快于 cppjieba 33%)</title>
    <link href="https://blog.paulme.ng/posts/2019-07-01-%E6%9C%80%E4%BD%B3%E5%8C%96jieba-rs%E4%B8%AD%E6%96%87%E6%96%B7%E8%A9%9E%E6%80%A7%E8%83%BD%E6%B8%AC%E8%A9%A6%28%E5%BF%AB%E4%BA%8Ecppjieba-33%25%29.html" />
    <id>https://blog.paulme.ng/posts/2019-07-01-%E6%9C%80%E4%BD%B3%E5%8C%96jieba-rs%E4%B8%AD%E6%96%87%E6%96%B7%E8%A9%9E%E6%80%A7%E8%83%BD%E6%B8%AC%E8%A9%A6%28%E5%BF%AB%E4%BA%8Ecppjieba-33%25%29.html</id>
    <published>2019-07-01T00:00:00Z</published>
    <updated>2019-07-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>昨晚寫了一篇關於最佳化 <a href="https://github.com/messense/jieba-rs">jieba-rs</a> 英文的<a href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html">介紹</a>，但想說 jieba 的使用者多半還是在中文圈，對於宣傳來講 hacker news 跟 reddit 可能無法觸及到真正會使用的使用者，於是為了宣傳，也是為了讓 search engine 可以搜尋到，就來把性能的部分另外寫成中文的一篇。關於過程我就不再重新用中文再寫一次了，實在太累人了。有興趣的人可以閱讀<a href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html">英文版</a></p>
<p>測試機器的機器規格如下</p>
<pre><code>MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports)
2.5 GHz Intel Core i7
16 GB 2133 MHz LPDDR3</code></pre>
<p>測試過程仿照<a href="http://yanyiwu.com/work/2015/06/14/jieba-series-performance-test.html">結巴(Jieba)中文分詞系列性能評測</a>所描述，先一行一行讀取檔案圍城到一個陣列裡，然後循環 50 次對圍城每行文字作為一個句子進行斷詞。 分詞算法都是採用精確模式，也就是包含了 HMM 的部分。</p>
<p>耗時的資料如下，從高到低排序</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">實作</th>
<th style="text-align: center;">耗時</th>
<th style="text-align: center;">版本 .</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">cppjieba</td>
<td style="text-align: center;">6.219s</td>
<td style="text-align: center;">866d0e8</td>
</tr>
<tr class="even">
<td style="text-align: left;">jieba-rs (master)</td>
<td style="text-align: center;">4.330s</td>
<td style="text-align: center;">a198e44</td>
</tr>
<tr class="odd">
<td style="text-align: left;">jieba-rs (darts)</td>
<td style="text-align: center;">4.138s</td>
<td style="text-align: center;">ab2fbfe</td>
</tr>
</tbody>
</table>
<p>以上耗時都是計算斷詞過程的耗時，不包括字典載入的耗時。</p>
<p>這篇會著重於評測只是為了宣傳，並不想陷入語言之爭，這也是我英文版有寫主要是分享關於用 Rust 最佳化的經驗，也是為了我自己衡量可以在工作中多認真使用 Rust 為目的。</p>
]]></summary>
</entry>

</feed>
