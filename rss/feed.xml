<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Paul Meng's Blog</title>
    <link href="https://blog.paulme.ng/rss/feed.xml" rel="self" />
    <link href="https://blog.paulme.ng" />
    <id>https://blog.paulme.ng/rss/feed.xml</id>
    <author>
        <name>Paul Meng</name>
        <email>me@paulme.ng</email>
    </author>
    <updated>2019-08-23T00:00:00Z</updated>
    <entry>
    <title>Changing the License of my open source projects</title>
    <link href="https://blog.paulme.ng/posts/2019-08-23-changing-the-license-of-my-open-source-projects.html" />
    <id>https://blog.paulme.ng/posts/2019-08-23-changing-the-license-of-my-open-source-projects.html</id>
    <published>2019-08-23T00:00:00Z</published>
    <updated>2019-08-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>After listening to the <a href="http://lucien.cc/about-2/">Lucien C.H. Lin</a>’s talk at <a href="https://coscup.org/2019/">Coscup 2019</a> about his experience on working with corporates dealing with FLOSS communities and licenses, I learn more about the practice in the industry. It also makes me to think about the license I would be using for my open source projects. <a href="http://neilmitchell.blogspot.com/2018/08/licensing-my-haskell-packages.html">This</a> article from Neil Mitchell in Haskell community explained his stance very well, and by carefully reading his article I would tend to agree with his intent. I would be inclined to change all of my projects to be Apache-2.0 and BSD-3 dual license so that it has patent grant protection and at the same time compatible with GPL. Rust is using Apache-2.0 and MIT dual license but to my personal I like the clause in BSD-3 where it requires you can’t use my name to endorse things you build, that makes sense to me. I would relicense my projects one by one from now on.</p>
]]></summary>
</entry>
<entry>
    <title>go-pdqsort - Pattern Defeating Quicksort in Go</title>
    <link href="https://blog.paulme.ng/posts/2019-08-21-go-pdqsort---pattern-defeating-quicksort-in-go.html" />
    <id>https://blog.paulme.ng/posts/2019-08-21-go-pdqsort---pattern-defeating-quicksort-in-go.html</id>
    <published>2019-08-21T00:00:00Z</published>
    <updated>2019-08-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p><a href="https://github.com/MnO2/go-pdqsort">go-pdqsort</a> is my implementation of pattern defeating sort in golang. I knew about pattern defeating sort from rust’s standard library documentation. I’ve never heard about this algorithm and it immediately intrigues my interest. I googled about it and found the original <a href="https://github.com/orlp/pdqsort">implementation</a> and their discussion threads on <a href="https://news.ycombinator.com/item?id=14661659">hacker news</a> and <a href="https://www.reddit.com/r/cpp/comments/2z6hgx/patterndefeating_quicksort/">reddit</a>. Then I read the <a href="https://paperpile.com/view/c3820723-6d9c-0a86-ab02-8e46dc22aec5">technical report</a> from Orson Peters. The key observation from the algorithm is that merely reducing the miss rate from CPU branch prediction, it would make the sorting speed greatly improved (no need to flush the cache line etc), so the technique adopted was to put the index of the array in the buckets and don’t swap them immediately, but put them in the right bin and swap them in one batch at the end of the loop. This works perfectly in the language with zero cost abstraction like C/C++ and Rust. I wasn’t sure about that would work in the heap-managed languages like golang, python and ruby etc, since most of the things are on the heap (with pointer indirection) and often results into cache misses, the impact of branch prediction miss rate might be neglectable. (For sure it would depend on the escape analysis in golang compiler, it would put the things on stack if it doesn’t escape in general). Though my hunch was that it would be slower, it still a good practice to implement the algorithm by myself so that I would get to know the details of the algorithm. And here are the results</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">implementation</th>
<th style="text-align: center;">speed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">pdqsort</td>
<td style="text-align: center;">80874 ns/op</td>
</tr>
<tr class="even">
<td style="text-align: center;">std-lib sort</td>
<td style="text-align: center;">69828 ns/op</td>
</tr>
</tbody>
</table>
<p>I am not sure about what std-lib’s sort is, but it looks like introsort since it switches to heapsort when the recursion is too deep, and it incorporates the techniques from shellsort as well, but the main part is still Bently’s quicksort technique. You can tell from the result that pdqsort is significantly slower than std-lib’s sort, probably due to the overhead of those memory batch swap, where it triggers extra allocation or cache eviction, pretty much as expected.</p>
<p>If you are interested in the algorithm, you could check out the <a href="https://github.com/MnO2/go-pdqsort">code</a> by yourself.</p>
]]></summary>
</entry>
<entry>
    <title>Travel Planning with Trello</title>
    <link href="https://blog.paulme.ng/posts/2019-08-20-travel-planning-with-trello.html" />
    <id>https://blog.paulme.ng/posts/2019-08-20-travel-planning-with-trello.html</id>
    <published>2019-08-20T00:00:00Z</published>
    <updated>2019-08-20T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>As a frequent traveller who has been to 55+ countries/regions, I know that travel planning is a time consuming process. I enjoyed the process pretty much as it would get me to know the destination and the culture more. Being to the destination is only small part of it. However, planning it with the right tool definitely would make your life much easier.</p>
<p>I was quite used to put everything into one Google Doc. I have been using the template I created for myself all over these years. I would revise the template from the experience from the journey to make sure the template is thorough and it could handle 90% of the scenario. Whenever I am about to plan my next trip, I’ll just copy from my template and dump the materials and the links to the wikitravel and blog posts into the Google Doc and use it as a draft to plan my trip.</p>
<p>Until I find a significant better way to do it. It’s part of the movement I set for myself to make most of the things in my life to be managed by Trello. I would centralize the notification/update by integrating the crawler and Trello API. But soon I found that Trello is especially suitable for travel planning. This blog post is not sponsored by Trello but I whole-heartedly think so.</p>
<p>Travel planning is a classic type of tasks that has the following properties:</p>
<ol type="1">
<li>Start from a simple idea and expanded into details and converge to a final plan</li>
<li>The final outcome is a detailed itinerary with contingency plan.</li>
<li>Itinerary alone is not enough. Analytical reporting is required since you need budgeting.</li>
</ol>
<p>Therefore at least to me, a good software for travel planning need to have the following features:</p>
<ol type="1">
<li>A pin board to collect the materials you are gonna to read.</li>
<li>Easy to re-organize and label items to suit your need or your mind map looks like.</li>
<li>Rich media supports</li>
<li>It provides APIs so that you could export and analyze it with a proper tooling.</li>
<li>Synchronization and Offline access.</li>
</ol>
<p>And Trello just matches all of them to me as the time of Aug 2019, and this is how I use it.</p>
<p>At the brainstorming phase, I would install the Send-to-Trello button to my Firefox. Whenever I looked up a relevant blog post and I don’t have time to read it full, I would use Send-to-Trello button to save it later. Usually I would create a List in Trello and name it as <code>Lead</code>.</p>
<p><img src="/images/2019-08-20/send_to_trello.png" /></p>
<p>And I would read them and create the digested steps into Cards and organize them into days. This is an iterative process, I would first focus on the ballpark schedule so that I could put everything into the my scheduled numbers of days, and make sure the big transporation items are possible to tickets in the budget. In the later iterations, I would add more detailed steps including the ticket and transfer information etc, so that I could just follow the instructions when I am on the road. The Card design just makes this iterative process easy and intuitive.</p>
<p><img src="/images/2019-08-20/trello_cards.png" /></p>
<p>With the Adds-on, Trello could even show you the Map View of your cards, it’s especially useful when you are planning the places you would like to visit in the city. You could see the obvious clusters for the markers on the map, and put them in the same day.</p>
<p><img src="/images/2019-08-20/trello_map_view.png" /></p>
<p>Also I would leverage on the Custom Fields adds-on to create extra field to document the cost and duration of the transportation. These are for further analysis at the later phase.</p>
<p>After finishing the planning and make sure I was correctly labeling the cards. I would run a script against Trello API to generate the Google Sheets budget spreadsheet.</p>
<pre><code>#!/usr/bin/env ruby

require &quot;google_drive&quot;
require &#39;trello&#39;
require &#39;time&#39;
require &#39;json&#39;

spreadsheet_id = &#39;&lt;Your Spread Sheet ID&gt;&#39;
target_currency = &#39;TWD&#39;

config_file = File.read(&quot;config/trello.json&quot;)
config_for_trello = JSON.parse(config_file)

Trello.configure do |config|
  config.developer_public_key = config_for_trello[&#39;developer_public_key&#39;] 
  config.member_token = config_for_trello[&#39;member_token&#39;] 
end

cards = {}

Trello::Board.find(&#39;&lt;Your Board ID&gt;&#39;).lists.each do |list|
  Trello::List.find(list.id).cards.each do |card|
    cards[card.id] = { :name =&gt; card.name }
    card.custom_field_items.each do |item|
      pp item.custom_field_id
      if item.custom_field_id == &#39;5d5978f014ab2f17fcb6a2cd&#39;
        if not item.value[&#39;number&#39;].nil?
          cards[item.model_id][:cost] = item.value[&#39;number&#39;]
        end
      elsif item.custom_field_id == &#39;5d59795638dd318d1a53471a&#39;
        if not item.option_value().nil?
          cards[item.model_id][:currency] = item.option_value()[&#39;text&#39;]
        end
      end
    end
  end
end

pp cards

row = 1
session = GoogleDrive::Session.from_config(&quot;config/google.json&quot;)
ws = session.spreadsheet_by_key(spreadsheet_id).worksheets[0]

cards.each do |card_id, card|
  if not card[:cost].nil? and not card[:currency].nil?
    ws[row, 1] = card[:name]
    ws[row, 2] = card[:cost]
    ws[row, 3] = card[:currency]
    ws[row, 4] = target_currency

    row += 1
  end
end
ws.save</code></pre>
<p>And here is what it looks like in Google Sheets.</p>
<p><img src="/images/2019-08-20/budget_table.png" /></p>
<p>I used it to plan my trip to Kyoto in the upcoming month, and it is so much better than what I was doing by Google Doc, reducing the big share of time to do copy pasting and adjusting the format. I would use this method to plan my trip in the future.</p>
]]></summary>
</entry>
<entry>
    <title>logq - Analyzing log files in SQL with command-line toolkit, implemented in Rust</title>
    <link href="https://blog.paulme.ng/posts/2019-08-16-logq---analyzing-log-files-in-sql-with-command-line-toolkit%2C-implemented-in-rust.html" />
    <id>https://blog.paulme.ng/posts/2019-08-16-logq---analyzing-log-files-in-sql-with-command-line-toolkit%2C-implemented-in-rust.html</id>
    <published>2019-08-16T00:00:00Z</published>
    <updated>2019-08-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p><a href="https://github.com/MnO2/logq">logq</a> is my latest project and it has reached a reasonably qualitative milestone for me to comfortably introduce it and share about its technical detail.</p>
<p>Let’s start by an example. Imagine that you are in the middle of troubleshooting a production incident and you are suspecting a certain endpoint from the web-server is having problem and result into high latency. Since it is application level and it is not provided by the AWS CloudWatch. It is also the first time that it happened so there isn’t any in-house datadog or application level instarumentation setup. And it occurs to you that the access log contains the relevant information and it would be possible for you to calculate the metrics from the log. You download the log from the archive storage and piece together an ad-hoc script in 30 minutes and run the metrics against the log, and the script is implemented in python it gets to pretty slow if the log size is large. Wouldn’t it be great if there were command line where you could handle these kind of ad-hoc analysis situation easily? Where no extra dependency setup like ELK or library is needed. That is the motivation to drive to the development of <a href="https://github.com/MnO2/logq">logq</a>, where you could answer the question of “What are the 95th latencies with 5 seconds time frame against application handlers by ignoring the second path segments” easily.</p>
<pre><code>&gt; logq query &#39;select time_bucket(&quot;5 seconds&quot;, timestamp) as t, url_path_bucket(request, 1, &quot;_&quot;) as r, percentile_disc(0.95) within group (order by sent_bytes asc) as bps from elb group by t, r&#39; data/AWSLogs.log
+----------------------------+----------------------------------------------------------------------+----------+
| 2019-06-07 18:45:30 +00:00 | /img/_/300/2r0/54558148eab71c6c2517f1d9.jpg                          | 0.046108 |
+----------------------------+----------------------------------------------------------------------+----------+
| 2019-06-07 18:45:30 +00:00 | /img/_/300/2r0/546db9fd11bcd3c15f0a4ada.jpg                          | 0.073435 |
+----------------------------+----------------------------------------------------------------------+----------+
| 2019-06-07 18:45:30 +00:00 | /img/_/bound/2r0/559153d381974f0e5289a5e4.webm                       | 0.055385 |
+----------------------------+----------------------------------------------------------------------+----------+
| 2019-06-07 18:45:30 +00:00 | /api/_/conversation/5590921f9d37a84e20286e3e                         | 0.04181  |
+----------------------------+----------------------------------------------------------------------+----------+
| 2019-06-07 18:45:35 +00:00 | /img/_/bound/2r0/551c6e78a107308d47055c96.webp                       | 0.063701 |
+----------------------------+----------------------------------------------------------------------+----------+
...</code></pre>
<p><a href="https://github.com/MnO2/logq">logq</a> inspired by my own need at daily work when troubleshooting production incident. I noticed that there are a few drawbacks by using glueing scripts to analyze the log</p>
<ol type="1">
<li>You spend a lot of time to parse of the log format, but not focus on calculating the metrics helping to troubleshoot your production issues.</li>
<li>Most of the log formats are commonly seen and we should ideally abstract it and have every one benefit from the shared abstraction</li>
<li>For web-server log cases, the log volume usually is huge, it could be several hundred MB or even a few GB. Doing it in scripting langauges would make yourself impatiently waiting it is running at your local.</li>
</ol>
<p>For sure you could finely tuned the analytical tooling like AWS Athena or ELK to analyze the very large volume of logs in tens of or hundreds of gigabytes, but often times you just want to ad-hocly analyze logs and don’t bother to set things up and cost extra money. Also, the modern laptop/PC is actually powerful enough to analyze gigabytes of log volumes, just that the implementation is not efficient enough for doing that. Implementing <a href="https://github.com/MnO2/logq">logq</a> in Rust is in hope to resolve those inconvenience and concerns.</p>
<p>To check out more query examples, please check out the <a href="https://github.com/MnO2/logq/blob/master/README.md">README</a> it has the commonly used queries (at least to the author).</p>
<p>The rest part of the blog post would be focus on the technical detail I faced when I implemented it in Rust.</p>
<h2 id="using-nom-to-parse-the-sql-syntax">Using nom to parse the SQL syntax</h2>
<p>When I was evaluating the parsing approach, I was considering among <a href="https://github.com/Geal/nom">nom</a>, <a href="https://github.com/Marwes/combine">combine</a>, <a href="https://github.com/pest-parser/pest">pest</a> and rolling my own lexer / parser. Initially I was rolling my own lexer / parser since it is the approach what most of the modern industrial strength compilers would do to cope with the incremental parsing and better error message etc. However I found it too laborious to do it at the prototype phase. I turned to look for a parser combinator library instead soon. I wasn’t considering <a href="https://github.com/pest-parser/pest">pest</a> since I am not familiar with PEG and I had the impression where the parser generated from generator are hard to fine tuned for better error message based on my experience of using lex/yacc. Therefore the only options left are <a href="https://github.com/Geal/nom">nom</a> and <a href="https://github.com/Marwes/combine">combine</a>. I was intimidated by the macro approach adopted by <a href="https://github.com/Geal/nom">nom</a> (before 5). I read the blog post I could understand it is due to the limitation in the compiler but it’s just looks like a horse hard to harness if anything goes wrong due to macro system. I feel a pity not be able to adopt a library where it is fine tuned for performance. Just when I was about to turn to <a href="https://github.com/Marwes/combine">combine</a>, I found that starting from <a href="https://github.com/Geal/nom">nom</a> version 5, it is no longer required to use macro approach. The only drawback is that the documentation for nom 5’s complete new approach is still lacking, but it looks like I could quickly grasp its concept since I had experience of using <a href="http://hackage.haskell.org/package/parsec">Parsec</a> in Haskell. And in the end it is the library I decided to stick with. The type like <code>IResult</code> and its type parameters are easy to understand, and most of the combinators are battery included in the library, but it still took me some time to figure it out when the thing I would like to do is not included.</p>
<h3 id="parsing-identifier">Parsing identifier</h3>
<p>It wasn’t so clear on how to express the rules of a valid identifier</p>
<ol type="1">
<li>It consists of underscore, alphabet, digit</li>
<li>It should not start with number</li>
<li>It shouldn’t be all underscore</li>
</ol>
<p>After checking out the <a href="https://github.com/Geal/nom/blob/master/examples/s_expression.rs#L74">example</a> provided by the nom repository, I found that you could actually do it in procedural, pretty much you would do if you roll your own parser with <code>Result</code> as the returning type. In nom’s case you just need to rely on the already provided <code>ParseError</code> and <code>split_at_position1_complete</code> instead. By providing helping function working on the character level then it is easy to do that.</p>
<pre><code>fn identifier&lt;&#39;a, E: nom::error::ParseError&lt;&amp;&#39;a str&gt;&gt;(input: &amp;&#39;a str) -&gt; IResult&lt;&amp;&#39;a str, &amp;&#39;a str, E&gt;
where
{
    fn is_alphanumeric_or_underscore(chr: char) -&gt; bool {
        chr.is_alphanumeric() || chr == &#39;_&#39;
    }

    fn start_with_number(s: &amp;str) -&gt; bool {
        if let Some(c) = s.chars().next() {
            &quot;0123456789&quot;.contains(c)
        } else {
            false
        }
    }

    fn all_underscore(s: &amp;str) -&gt; bool {
        for c in s.chars() {
            if c != &#39;_&#39; {
                return false;
            }
        }

        true
    }

    let result = input.split_at_position1_complete(
        |item| !is_alphanumeric_or_underscore(item.as_char()),
        nom::error::ErrorKind::Alpha,
    );

    match result {
        Ok((i, o)) =&gt; {
            if start_with_number(o) || all_underscore(o) || KEYWORDS.contains(&amp;o) {
                Err(nom::Err::Failure(nom::error::ParseError::from_error_kind(
                    i,
                    nom::error::ErrorKind::Alpha,
                )))
            } else {
                Ok((i, o))
            }
        }
        Err(e) =&gt; Err(e),
    }
}</code></pre>
<h3 id="precedence-climbing-expression-parsing">Precedence climbing expression parsing</h3>
<p>Traditionally to parse an expression with precedence, you either encode it in the grammar or use bottom-up approach. It is a long resolved problem and the early day approach is Dijkstra’s Shunting Yard algorithm. However, most of the manually implemented modern industrial strength compiler use recursively descent approach, which is top down. One approach is by switching to the bottom-up approach when parsing to a point where operator precedence parser is needed. I think it is also possible to do that with parser combinator approach but we need to treat an operator precedence parser as a standalone black box and it doesn’t combine elegantly with the rest of the code. And here is the point where I would like to introduce Precedence climbing algorithm. It just combine so elegantly with the parser combinator approach, since it is a top down approach it just looks seamlessly with the rest of the combinators’ code. If you are not familiar with Precedence climbing, Eli Bendersky has a very good <a href="https://eli.thegreenplace.net/2012/08/02/parsing-expressions-by-precedence-climbing">introduction</a>.</p>
<p>In just a few short lines of code and the whole precedence parsing problem is resolved.</p>
<pre><code>fn parse_expression_at_precedence&lt;&#39;a&gt;(
    i0: &amp;&#39;a str,
    current_precedence: u32,
    precedence_table: &amp;HashMap&lt;String, (u32, bool)&gt;,
) -&gt; IResult&lt;&amp;&#39;a str, ast::Expression, VerboseError&lt;&amp;&#39;a str&gt;&gt; {
    let (mut i1, mut expr) = parse_expression_atom(i0)?;
    while let Ok((i2, op)) = parse_expression_op(i1) {
        let (op_precedence, op_left_associative) = *precedence_table.get(op).unwrap();

        if op_precedence &lt; current_precedence {
            break;
        }

        let (i3, b) = if op_left_associative {
            parse_expression_at_precedence(i2, op_precedence + 1, precedence_table)?
        } else {
            parse_expression_at_precedence(i2, op_precedence, precedence_table)?
        };

        let op = ast::BinaryOperator::from_str(op).unwrap();
        expr = ast::Expression::BinaryOperator(op, Box::new(expr), Box::new(b));

        i1 = i3;
    }

    Ok((i1, expr))
}</code></pre>
<p>It also worths to mention that I was actually got lazy to encode the precedence in the grammar when I was doing the syntax part in the first iteration and leave the precedence parsing to later to resolve. It is a major refactoring when the project need to shift the approach when I was affirmed that the precedence climbing is the way to go. Without Rust’s type system’s help, it is a hard take. I only needed to follow the compiler’s complaint and clear out all of the warnings/errors then the task is pretty much done. I simply could not imagine how I could do that in a dynamic-typed programming language.</p>
<h2 id="failure-crate">Failure crate</h2>
<p>I would like to briefly talk about the <code>failure</code> crate as well. I have had experience of using <code>failure</code> crate in some of the mini projects. At the time I was feeling that it is quite convenient that you could derive everything by putting the label on the Error struct. In this larger project, with the help of <code>impl From</code> and <code>#[cause]</code> label, and <code>?</code> operator. I have the strong feeling that it help shape your structuring of error handling into monadic style. <code>impl From</code> plays the role kind of like monad-transformer where lift the application from one kind of error monad to another kind, but without having you to stick your head around complicated type combination but it could influence you to put down the error handling in the similar structure.</p>
<h2 id="compiling-sql-to-graph">Compiling SQL to Graph</h2>
<p>Then here comes the core part. Writing a compiler is all about parsing and manipulating on graphs. For translating SQL into computation graph is relatively elegant than translating other procedural programming languages, since if you think it thoroughly, it is translating into the the structure that is pretty functional.</p>
<p>For a simple select statement, like “select a from foo”. It is actually could be translated into a term you are familiar in functional world. That is <code>map</code>. Suppose that you <code>foo</code> table has its schema defined as (“a”, “b”, “c”). Then “select a” is just saying, please project to “a” from (“a”, “b”, “c”), or you could say it a synonym to <code>map</code>.</p>
<p>For the <code>from</code> clause, it is just saying a data source where you could stream the data record from, it could be a file name or any other data source, either physically or virtually referred to.</p>
<p><code>where</code> clause is a no brainer, it is just interpreting a <code>filter</code> against the projected data records from <code>map</code>. The expressions to evaluated against are provided to <code>where</code> in the form of <code>a = 1</code> etc.</p>
<p>The relatively complicated is <code>group by</code>, but it is simply just doing a <code>fold</code> against a key-value data structure, with the key as the distinct values specified by the fields in the <code>group by</code> clause, and the rest of the columns in the values, or sent to the “aggregator”. If you are familiar with Haskell you know that HashMap is <a href="http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Foldable.html#t:Foldable">foldable</a>, group by is basically a fold against HashMap if it is the backing storage. In other languages like ruby, <code>reduce</code> is probably the method used in this kind of action instead.</p>
<p>So to sum up, the whole SQL translation is basically just parse the abstract the syntax tree into a map-reduce chain, streaming the source data from the upstream and produce it to the downstream. in the main stream language this type of structures are called <code>iterator</code>, so it could be seen as a series of iterators chained together as well.</p>
<p>For sure there are details where you would like to rewrite the whole graph to remove the const expression etc, or push down the complicated logic into the data source if the data source supports complicated compute logic (in practice they are smart key-value store supports transaction and cursor). But the spirit of the whole compilation is pretty much as the above mentioned.</p>
<h2 id="t-digest">T-Digest</h2>
<p>In <a href="https://github.com/MnO2/logq">logq</a>, the aggregate function of <code>percentile_disc</code> is supported since it is a common need to calculate the 99th percentile of latency in analyzing web traffic. However, to calculate the exact percentile you have to sort the sequence, which is a very expensive operation if the log file size is huge. <code>percentile_disc</code> in logq is compiled to group by with sorting in each of the partition, and it would remember every elements so that it is able to sort.</p>
<p>In order to make it more efficient in terms of the memory footprint and to support the parallel aggregation feature on the roadmap, I implemented <code>approx_percentile</code> aggregate function by T-Digest algorithm. T-Digest is an interesting data sketching algorithm. It leverages on the “scaling function” to make the both ends of the percentiles (for example, 1th percentile or 99th percentile) very accurate and the percentile in the middle less accurate. It matches the use cases we often need. The core concept behind is still by binning the data points, just that it uses scaling function to control the binning process, it makes the binning on both ends more granular so that we could keeps the info as the original as possible and the binning in the middle more blunt (larger binning size). And the choice of the scaling function is pretty flexible, the only requirements are that it has to be non-decreasing, and its derivative starts at 0 and ends at 1. My implementation is based on what facebook’s folly provided. In the original paper and many reference implementation it is using <code>arcsin</code>, but in folly it is using sqrt since it more computation efficient, and practically not much statistical impact.</p>
<p>I released the T-Digest part as a separate <a href="https://crates.io/crates/tdigest">tdigest</a> crate. It is not fully tested with generated statistical data, but it should be reasonably correct since I am following folly’s implementation.</p>
<h2 id="post-words">Post Words</h2>
<p>This is the first non-trivial project implemented in Rust, the line reported by <code>cloc</code> on tag <code>v0.1.4</code> is around 4800 lines of Rust.</p>
<pre><code>github.com/AlDanial/cloc v 1.82  T=0.05 s (305.6 files/s, 120809.5 lines/s)
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Rust                            13            651             17           4835
YAML                             1              0              0             32
-------------------------------------------------------------------------------
SUM:                            14            651             17           4867
-------------------------------------------------------------------------------</code></pre>
<p>To do major refactoring on this size of code usually it is a pain but at least so far I can still feel comfortable to do any major changes without breaking too much things.</p>
<p>There are plenty of features I plan to put on the roadmap, performance optimization is definitely on the top of the list since I didn’t consider too much performance when I coded the proof of concept. There are plenty of room to speed up. Other things on the roadmap are</p>
<ul>
<li>Conforms to the much more complicated SQL syntax as sqlite</li>
<li>Performance optimization, avoid unnecessary parsing</li>
<li>More supported functions</li>
<li>time_bucket with arbitrary interval (begin from epoch)</li>
<li>Window Function</li>
<li>Implementing approximate_percentile_disc with t-digest algorithm when the input is large.</li>
<li>Streaming mode to work with tail -f</li>
<li>Customizable Reader, to follow GoAccess’s style</li>
<li>More supported log format</li>
<li>Plugin quickjs for user-defined functions</li>
</ul>
<p>Hope I can constantly have spare time to work on this many things on the roadmap.</p>
]]></summary>
</entry>
<entry>
    <title>「ProtonMail」に移行して、「Gmail」を辞めます</title>
    <link href="https://blog.paulme.ng/posts/2019-07-30-%E3%80%8Cprotonmail%E3%80%8D%E3%81%AB%E7%A7%BB%E8%A1%8C%E3%81%97%E3%81%A6%E3%80%81%E3%80%8Cgmail%E3%80%8D%E3%82%92%E8%BE%9E%E3%82%81%E3%81%BE%E3%81%99.html" />
    <id>https://blog.paulme.ng/posts/2019-07-30-%E3%80%8Cprotonmail%E3%80%8D%E3%81%AB%E7%A7%BB%E8%A1%8C%E3%81%97%E3%81%A6%E3%80%81%E3%80%8Cgmail%E3%80%8D%E3%82%92%E8%BE%9E%E3%82%81%E3%81%BE%E3%81%99.html</id>
    <published>2019-07-30T00:00:00Z</published>
    <updated>2019-07-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Googleが想像以上にいろんな情報を取得した。何か調べ物がある時はGoogleで検索し、ブラウザはChromeを使って、メールはGmailで送信し、どこかに行く時Googleマープで所在地を確認し、YouTubeの動画を見て、Googleドライブに入れて、仕事はGoogle Docsを使いて。すべての閲覧履歴をGoogleが保存します。インターネット生活のほぼ全てをGoogleに絡め取られているの事は、良いことではない。そのため、Googleの依存度を減らしたい。</p>
<p>最初の辞めるのサービスは Gmail。メールは全ての情報は含まれている、クレジットカードでも、応募の情報でも、旅行情報でも、メールサービスは自分の情報を取得している、それが飛行機のチケット予約すると、Googleがカレンダーに予約を作成できるの理由。</p>
<h2 id="protonmail-の特徴">ProtonMail の特徴</h2>
<p>ProtonMailのメールデータは全て暗号化された状態でサーバー上に保存されています。データはすべてのステップで暗号化されていることから傍受のリスクはないとのこと。もっと知りたいたら、<a href="https://www.ted.com/talks/andy_yen_think_your_email_s_private_think_again?language=en">Andy Yen: Think your email’s private? Think again</a>の TED トークの動画を見てください。</p>
<h2 id="購入後1ヶ月間使用した感想">購入後1ヶ月間、使用した感想</h2>
<p>移入が大体上手くいく、ProtonMail はカスタムドメインをセットアップすることがあります。paulme.ng のドメインをProtonMailに移行しました。スパムフィルタが ProtonMail より Gmail の方がうまい。ProtonMailのフィルタはが手動セットアップあとで、メールの分類正しいになりました。暗号化と便利のトレード・オフがと思います。暗号化したら、情報を取得できないだから、訓練データになし、機械学習的手法を使っていない、スパムメッセージの分類器を作っていこどができません。それ以外は、満足しています。</p>
]]></summary>
</entry>
<entry>
    <title>Managing references with Paperpile</title>
    <link href="https://blog.paulme.ng/posts/2019-07-28-managing-references-with-paperpile.html" />
    <id>https://blog.paulme.ng/posts/2019-07-28-managing-references-with-paperpile.html</id>
    <published>2019-07-28T00:00:00Z</published>
    <updated>2019-07-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>For a long time I have the problem of managing papers and slides I am reading. It’s not only for the papers and slides from computer science, but also the financial reports and the letters to shareholder for my investment readings and researches. Over time I just downloaded a bunch of PDFs to my laptop and they scattered around in my Download folder without a systematic naming and indexing. It makes it hard to clean up the disk space for my laptop since I am not sure which one is what I needed if not by opening them up and review them one by one. It makes thing painful when you are backing up your system and migrating to the other. A couple of weeks back I found that I need to resolve this problem.</p>
<p>An ideal reference management software to me in 2019 should provide the following features.</p>
<ol type="1">
<li>To be able to store in the cloud so that you are able to access it everywhere. It would be great to let you choose among the mainstream cloud storage providers, and provide options to make some of the papers accessible offline, but not sync everything to local.</li>
<li>Good metadata auto completion. Not everything has a standard to label it or a good database to cross match and automatically label them, but it should be good for the parts that we are able to do it.</li>
<li>Cross platform and reasonably performant.</li>
<li>Providing API so that you could customise it for personal needs.</li>
</ol>
<p>I know the famous softwares like Zotero, Mendeley and Endnote. They have existed for a long time, but all of them feel more focus for scientific editing. I played with them for a little bit and Zotero maybe the closest but it still feels quite the exact thing I want. Until I noticed a new upcoming competitor: Paperpile. They seem to launch for some years but not that widely well-known. I tried it out and it gives me the feeling it is the closest reference management software I would like. I have lots of PDFs and my main purpose is not for scientific editing, I just need a software to edit it and help me rename them, and easy to look things up. It is a big-bonus to me that Paperpile is based on Google Drive. It resolves the issue 1, 3, 4 by leveraging on Google Drive as the backing storage. With Google Drive you could choose which file to stay offline and providing Google Drive API to let you trigger the hook, for example, to send a new card to my Trello when a new paper is added. The downside for sure is vendor lock-in, but it’s less a priority comparing to solving my need.</p>
<p>I have tried it out for 3 weeks and I can say it is the software that’s closest to my ideal, though not perfect yet. I would stick to it before anything even better come up.</p>
]]></summary>
</entry>
<entry>
    <title>cedarwood: Efficiently-Updatable Double Array Trie in Rust</title>
    <link href="https://blog.paulme.ng/posts/2019-07-14-cedarwood%3A-efficiently-updatable-double-array-trie-in-rust.html" />
    <id>https://blog.paulme.ng/posts/2019-07-14-cedarwood%3A-efficiently-updatable-double-array-trie-in-rust.html</id>
    <published>2019-07-14T00:00:00Z</published>
    <updated>2019-07-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p><a href="https://github.com/MnO2/cedarwood">Cedarwood</a> is an effort to <a href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html">speed up jieba-rs</a>, an efficient implementation of trie is needed in order to satisfying the following needs.</p>
<ul>
<li>To be able to list the prefixes that exist in the dictionary with a given string. For example, given the dictionary to be <code>["a", "ab", "abc", "z", "xz", "xy"]</code> and the input string <code>abcdefg</code>. We should be able to efficiently list the prefixes <code>["a", "ab", "abc"]</code> that exist in the dictionary.</li>
<li>Due to the support of dictionary provided dynamically from the user, we need to support dynamic insertion into the data structure and at the same time still get us efficient common prefixes operation.</li>
<li>To be able to efficiently tell if the given string is in the dictionary or not.</li>
</ul>
<p>As mentioned <a href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html">previously</a> that aho-corasick performs quite slow from my brief testing, I didn’t spend extra time to dig out why and I just accepted it. My focus was turned to refine the double array trie implementation, where at the time was <a href="https://github.com/andelf/rust-darts">rust-darts</a>. It performs quite well on the reading operations but due to its implementation was based on the <a href="http://chasen.org/~taku/software/darts/">darts</a>, a C++ implementation of static double array trie. It failed the requirement 2 mentioned above. Either we have to drop the API and getting farther away from the original python’s implementation of Jieba, or we have to find alternatives to support dynamic insertion. I did contribute to the <code>rust-darts</code> with <a href="https://github.com/andelf/rust-darts/pull/25">dynamic insertion</a>, it resolves the feature lacking issue but the speed is still quite slow. It would make the loading time and testing time for jieba to be slow as well. Then I implemented the techniques mentioned <a href="https://linux.thai.net/~thep/datrie/#Alloc">here</a> to make the trie building time much faster (from 30+ seconds down to around 5 seconds on my machine). It is much much better but I still think if we can do any better. To be fair to the <code>darts</code> implementation, the vanilla double array trie was invented by Aoe in the 1989 paper: <a href="https://ieeexplore.ieee.org/document/17222">An efficient digital search algorithm by using a double-array structure</a> with mostly read operation in mind. It did support the update operations but it also said the update speed is quite slow. There are a few techniques to speed the update operations up and I already implemented <a href="https://linux.thai.net/~thep/datrie/#Alloc">one of them</a> for the clean build.</p>
<h2 id="look-for-better-solutions">Look for better solutions</h2>
<p>I googled around to see if there are any better solution, then I found <a href="https://en.wikipedia.org/wiki/HAT-trie">HAT Trie</a>, which seems to be the state of the art implementation of the trie, with the cache in consideration. It is based on the paper published by Askitis Nikolas and Sinha Ranjan: <code>HAT-trie: A Cache-conscious Trie-based Data Structure for Strings.</code>. And it is good that <a href="https://github.com/Tessil">Tessil</a> already had <a href="https://github.com/Tessil/hat-trie">C++ implementation</a> on github with <a href="https://github.com/Tessil/hat-trie/blob/master/README.md">very detailed benchmark comparisons</a>. However, when I looked at the interface provided it seems like it only supports finding the longest prefix, and iterating through the string predictions in the dictionary matching a given prefix. It doesn’t provide the interface for the requirement 1. I looked into the <a href="https://github.com/Tessil/hat-trie/blob/master/include/tsl/htrie_hash.h#L1574">code</a> and it seems that it could potentially exposed from the implementation to support that, just that I am not familiar enough of the algorithm to do that. The concept of HAT-trie is also based on hash table with the concept of “burst” is unfamiliar to me, and since it is stored in unordered, my hunch stopped me from investing my time further to read the not-so-simple algorithm. Though the hindsight now is that the operation I need is not based on the requirement of the order in keys but the traversal of the trie, which should be supported by the longest prefix operation, just need to change the implementation. I would do that if I have more time.</p>
<h2 id="cedar-and-cedarwood">Cedar and Cedarwood</h2>
<p>The main character in this post is actually one of the candidates listed in the benchmark provided by Tessil. An double array trie implementation named <a href="http://www.tkl.iis.u-tokyo.ac.jp/~ynaga/cedar/">cedar</a> caught my eyes, since its update seems to be very efficient, it is comparable with the other fast implementations in the list. And it’s read access speed is also on the same ballpark with HAT-trie in the Dr. Askitis dataset, where the median key length is shorter than what is in the wikipedia title dataset. It definitely worths a closer look to me. The read access speed seems to be slower than HAT-trie in the long key length cases but for the Chinese segmentation scenario, the shorter key seems to be the case. We are not working on a query completion use case anyway. The author of cedar is <a href="http://www.tkl.iis.u-tokyo.ac.jp/~ynaga/">Naoki Yoshinaga</a>. He pointed us to the paper of <code>A Self-adaptive Classifier for Efficient Text-stream Processing</code> for further reading on his webpage but it is mostly an NLP paper but not a data structure paper, it only has one or two paragraphs describing the working on his improvement on double array trie. In the paper he cited another one, but you couldn’t find the paper on any English website, even scihub. It turned out it is a paper in Japanese and I found the pdf on the Japanese website with the title of <code>タブル配列による動的辞書の構成と評価</code>. Even though I can understand basic to intermediate Japanese, it still didn’t address that much into the detail in that Japanese paper. Therefore I decided to read the code directly.</p>
<p>It is a header-only C++ implementation and the code is very much shortened to reduce the header size, it is generally ok to read with a C++ formatter from IDE or VSCode, but it also took me a while to get the core concept behind it on improvement techniques due to the lack of comments. And with the <code>cedarwood</code> rust porting implementation, I added much more comments so it should be much easier to understand how it is working, but it is a required reading to read the original double array trie paper so that at least you know how the <code>base</code> and <code>check</code> works in double array trie cases. The following I’ll briefly talk about the concept behind the skills.</p>
<h2 id="the-key-concepts-in-the-algorithm">The Key Concepts in the Algorithm</h2>
<p>The inefficiency in the update operation of the vanilla double array trie is caused by the free slot searching. The original paper simply implies that you could use brute-force approach to iterate through the index in <code>check</code> and see if they are marked as owned, and iterate through until the location where you have the free slots distribution exactly match what you want (suppose you are relocating an existing trie node to a new free location). The technique specified <a href="https://linux.thai.net/~thep/datrie/#Alloc">here</a> is basically leveraging on the value space on each block, and you can use negative integer to specify the free slot location and make them into an array-index based doubly linked-list, and you could make the brute forte iteration down to a fast-skipping linked list traversal. However, that technique doesn’t address the case where you still need to iterate your char set (2^21 in the unicode scalar case, or 2^8 if you are working on UTF-8) to check every slot and make sure the slots distributions match your need. The technique used in cedar is basically to address this problem, by maintain the bookkeeping of two kind of information: <code>NInfo</code> and <code>Block</code></p>
<pre><code>struct NInfo {
    sibling: u8, // the index of right sibling, it is 0 if it doesn&#39;t have a sibling.
    child: u8,   // the index of the first child
}</code></pre>
<pre><code>struct Block {
    prev: i32,   // previous block&#39;s index, 3 bytes width
    next: i32,   // next block&#39;s index, 3 bytes width
    num: i16,    // the number of slots that is free, the range is 0-256
    reject: i16, // a heuristic number to make the search for free space faster, it is the minimum number of iteration in each trie node it has to try before we can conclude that we can reject this block. If the number of kids for the block we are looking for is less than this number then this block is worthy of searching.
    trial: i32,  // the number of times this block has been probed by `find_places` for the free block.
    e_head: i32, // the index of the first empty elemenet in this block
}</code></pre>
<p>The <code>NInfo</code> is probably standing for “Trie Node Information”, it maintains the trie parent-children and siblings information, since these are the information needed when relocating the trie node around. You have to know which node has smaller size in its children, you could just traverse down and counting the number of children one by one. And you could iterate through the sibling chain when you really need to do the move and compare if the slots fit the potential free spaces.</p>
<p>As for <code>Block</code>, it maintains the book-keeping of the free space selection, simulating how you look at it of the each block (256 in size) when you squint at the data structure. It book keeps the information like how many free slots this block still have so that you could quickly prune the branch if the number of the free slots is less than the number of children the trie node you are relocating. Further more, the algorithm categorize the blocks into three kinds</p>
<ul>
<li>Full: The block where all of the slots are taken</li>
<li>Closed: The block where all of the slots are taken except for one</li>
<li>Open: The rest, but most of the time it is the free slots that just allocated at the end of the <code>base</code> and <code>check</code> when you resize the array.</li>
</ul>
<p>Each of them is put in the doubly-linked-list of their own kind. During the search process, you only need to look for the free space from <code>Closed</code> type block if you are inserting a single-child node, since it only needs one slot. You only need to look for <code>Open</code> block when you are relocating a node with more children. And since it is linked list all you need to do is just insert and remove the block from the linked-list, which is constant time. And insert / remove from the right kind of linked list after you update the node with inserted <code>label</code>.</p>
<h2 id="benchmarks">Benchmarks</h2>
<p>With the above algorithm mentioned, let’s take a look of <code>cedarwood</code>’s benchmark, the rust implementation of <code>cedar</code>. My laptop’s spec is as follows:</p>
<pre><code>MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports)
2.5 GHz Intel Core i7
16 GB 2133 MHz LPDDR3</code></pre>
<p>And here is the benchmark against C++ version and <code>rust-darts</code></p>
<h3 id="build">Build</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">impl</th>
<th style="text-align: left;">time</th>
<th style="text-align: left;">version</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">cedar</td>
<td style="text-align: left;">71 ms</td>
<td style="text-align: left;">2014-06-24</td>
</tr>
<tr class="even">
<td style="text-align: left;">cedarwood</td>
<td style="text-align: left;">64 ms</td>
<td style="text-align: left;">0.4.1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">rust-darts</td>
<td style="text-align: left;">201 ms</td>
<td style="text-align: left;">b1a6813</td>
</tr>
</tbody>
</table>
<h3 id="query">Query</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">impl</th>
<th style="text-align: left;">time</th>
<th style="text-align: left;">version</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">cedar</td>
<td style="text-align: left;">10 ms</td>
<td style="text-align: left;">2014-06-24</td>
</tr>
<tr class="even">
<td style="text-align: left;">cedarwood</td>
<td style="text-align: left;">10 ms</td>
<td style="text-align: left;">0.4.1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">rust-darts</td>
<td style="text-align: left;">18 ms</td>
<td style="text-align: left;">b1a6813</td>
</tr>
</tbody>
</table>
<p>For the <code>rust-darts</code> it is significantly slower than both of <code>cedar</code> and <code>cedarwood</code>, and it has extra requirement on the construction of the dictionary to be that the dictionary has to be lexicographically sorted. For <code>cedar</code> building contains the time for file reading (with 65536 buffer size) so it actually even slightly faster, but the querying time contains memory-only operation. I am too lazy to change the benchmarking code since it is using no-copying techniques by moving the pointer until the next new line and it is troublesome for me to change the rust implementation to be the same way so that to compare apple to apple. The <code>cedar</code> code was compiled with <code>-O3</code> and rust’s code is compiled with <code>--release</code> and only measure the time for the operations in the memory. You can see that the build time, that is <code>update</code> are roughly the same for C++ and Rust. And in the query case the Rust version is slightly slower than C++ but basically comparable. I couldn’t find where I could further optimize the Rust version since the code for the lookup case is quite simple and not much space to optimize on the source code level without going to the <code>unsafe</code> resort. Not sure where I could tune to speed it up, please let me know if you identify the part I could do further.</p>
<h2 id="lesson-learned">Lesson Learned</h2>
<p>Rust’s implementation is as comparable as C++’s implementation without much effort, I don’t need to go with <code>unsafe</code> and keep the majority of the code compiler-checked for memory safety. There is only one place where I need to go by <code>unsafe</code> block due to linked-list update but not due to the performance. There are a few handy features in C++ that I would miss, like const generics to make the data structure parameterized on the type level, but overall it is not a big issue in the cedar case. Or template specialization where you could leverage on different implementations based on the type you specify. Since <code>cedarwood</code> only save the word id in normal mode. It might require that for the support of reduced-trie feature where the value is stored in place, but right now <code>cedarwood</code> only has limited support on that so it is no big issue. Overall it is a smooth experience on porting.</p>
]]></summary>
</entry>
<entry>
    <title>最佳化 jieba-rs 中文斷詞性能測試 (快于 cppjieba 33%)</title>
    <link href="https://blog.paulme.ng/posts/2019-07-01-%E6%9C%80%E4%BD%B3%E5%8C%96jieba-rs%E4%B8%AD%E6%96%87%E6%96%B7%E8%A9%9E%E6%80%A7%E8%83%BD%E6%B8%AC%E8%A9%A6%28%E5%BF%AB%E4%BA%8Ecppjieba-33%25%29.html" />
    <id>https://blog.paulme.ng/posts/2019-07-01-%E6%9C%80%E4%BD%B3%E5%8C%96jieba-rs%E4%B8%AD%E6%96%87%E6%96%B7%E8%A9%9E%E6%80%A7%E8%83%BD%E6%B8%AC%E8%A9%A6%28%E5%BF%AB%E4%BA%8Ecppjieba-33%25%29.html</id>
    <published>2019-07-01T00:00:00Z</published>
    <updated>2019-07-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>昨晚寫了一篇關於最佳化 <a href="https://github.com/messense/jieba-rs">jieba-rs</a> 英文的<a href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html">介紹</a>，但想說 jieba 的使用者多半還是在中文圈，對於宣傳來講 hacker news 跟 reddit 可能無法觸及到真正會使用的使用者，於是為了宣傳，也是為了讓 search engine 可以搜尋到，就來把性能的部分另外寫成中文的一篇。關於過程我就不再重新用中文再寫一次了，實在太累人了。有興趣的人可以閱讀<a href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html">英文版</a></p>
<p>測試機器的機器規格如下</p>
<pre><code>MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports)
2.5 GHz Intel Core i7
16 GB 2133 MHz LPDDR3</code></pre>
<p>測試過程仿照<a href="http://yanyiwu.com/work/2015/06/14/jieba-series-performance-test.html">結巴(Jieba)中文分詞系列性能評測</a>所描述，先一行一行讀取檔案圍城到一個陣列裡，然後循環 50 次對圍城每行文字作為一個句子進行斷詞。 分詞算法都是採用精確模式，也就是包含了 HMM 的部分。</p>
<p>耗時的資料如下，從高到低排序</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">實作</th>
<th style="text-align: center;">耗時</th>
<th style="text-align: center;">版本 .</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">cppjieba</td>
<td style="text-align: center;">6.219s</td>
<td style="text-align: center;">866d0e8</td>
</tr>
<tr class="even">
<td style="text-align: left;">jieba-rs (master)</td>
<td style="text-align: center;">4.330s</td>
<td style="text-align: center;">a198e44</td>
</tr>
<tr class="odd">
<td style="text-align: left;">jieba-rs (darts)</td>
<td style="text-align: center;">4.138s</td>
<td style="text-align: center;">ab2fbfe</td>
</tr>
</tbody>
</table>
<p>以上耗時都是計算斷詞過程的耗時，不包括字典載入的耗時。</p>
<p>這篇會著重於評測只是為了宣傳，並不想陷入語言之爭，這也是我英文版有寫主要是分享關於用 Rust 最佳化的經驗，也是為了我自己衡量可以在工作中多認真使用 Rust 為目的。</p>
]]></summary>
</entry>
<entry>
    <title>优化 jieba-rs 中文分词性能评测 (快于 cppjieba 33%)</title>
    <link href="https://blog.paulme.ng/posts/2019-07-01-%E4%BC%98%E5%8C%96-jieba-rs-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D-%E6%80%A7%E8%83%BD%E8%AF%84%E6%B5%8B%EF%BC%88%E5%BF%AB%E4%BA%8E-cppjieba-33percent%29.html" />
    <id>https://blog.paulme.ng/posts/2019-07-01-%E4%BC%98%E5%8C%96-jieba-rs-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D-%E6%80%A7%E8%83%BD%E8%AF%84%E6%B5%8B%EF%BC%88%E5%BF%AB%E4%BA%8E-cppjieba-33percent%29.html</id>
    <published>2019-07-01T00:00:00Z</published>
    <updated>2019-07-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>昨晚写了一篇关于优化 <a href="https://github.com/messense/jieba-rs">jieba-rs</a> 英文的<a href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html">介绍</a>，但想说 jieba 的使用者多半还是在中文圈，对于宣传来讲 hacker news 跟 reddit 可能无法触及到真正会使用的用户群，于是为了宣传，也是为了让 search engine 可以搜索到，就来把性能的部分另外写成中文的一篇。关于过程我就不再重新用中文再写一次了，实在太累人。有兴趣的人可以阅读<a href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html">英文版</a></p>
<p>测试机器的机器规格如下</p>
<pre><code>MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports)
2.5 GHz Intel Core i7
16 GB 2133 MHz LPDDR3</code></pre>
<p>测试过程仿照<a href="http://yanyiwu.com/work/2015/06/14/jieba-series-performance-test.html">结巴(Jieba)中文分词系列性能评测</a>所描述，先按行读取文本围城到一个数组里，然后循环 50 次对围城每行文字作为一个句子进行分词。 分词算法都是采用精确模式，也就是包含了 HMM 的部分。</p>
<p>耗时数据如下，从高到低排序</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">实作</th>
<th style="text-align: left;">耗时</th>
<th style="text-align: left;">版本 .</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">cppjieba</td>
<td style="text-align: left;">6.219s</td>
<td style="text-align: left;">866d0e8</td>
</tr>
<tr class="even">
<td style="text-align: left;">jieba-rs (master)</td>
<td style="text-align: left;">4.330s</td>
<td style="text-align: left;">a198e44</td>
</tr>
<tr class="odd">
<td style="text-align: left;">jieba-rs (darts)</td>
<td style="text-align: left;">4.138s</td>
<td style="text-align: left;">ab2fbfe</td>
</tr>
</tbody>
</table>
<p>以上耗时都是计算分词过程的耗时，不包括词典载入的耗时。</p>
<p>这篇会着重于评测只是为了宣传，并不想陷入语言之争，这也是我英文版有写主要是分享关于用 Rust 优化的经验，也是为了我自己衡量可以在工作中多认真使用 Rust 为目的。</p>
]]></summary>
</entry>
<entry>
    <title>Optimizing jieba-rs to be 33% faster than cppjieba</title>
    <link href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html" />
    <id>https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html</id>
    <published>2019-06-30T00:00:00Z</published>
    <updated>2019-06-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>To start this blog post with a disclaimer: I admit that it is a clickbait title since every benchmark is a lie in its own, and I have no intention to start another programming language flame war. This blog post is mainly to share my experience on taking an emerging programming language’s ecosystem seriously and evaluating it by working on a serious project, and see how far we can go in terms of performance and development experience.</p>
<p>The project I chose as mentioned in the title is <a href="https://github.com/messense/jieba-rs">jieba-rs</a>, the rust implementation of a popular Chinese word segmentation library: <a href="https://github.com/fxsjy/jieba">Jieba</a>. A quick background introduction about <a href="https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation">word segmentation</a> for the readers who don’t speak Chinese. Chinese and Japanese in their written scripts are only delimited up to the sentence but words are not delimited. For example, the following sentence could be literally translated to “Today’s weather very good” in the same order, but you have no idea where the word boundaries are.</p>
<pre><code>今天的天气真好</code></pre>
<p>After the word segmentation, the sentence could be broken up as follows (space delimited):</p>
<pre><code>今天(Today)  的(&#39;s) 天气(weather) 真(very) 好(good)</code></pre>
<p>It’s unlike English where spaces are added between words. This makes the natural language processing harder since most of the methodologies requires the granularity of words. Therefore the word segmentation is a core part when it comes to the tasks like search engine and language understanding.</p>
<p><a href="https://github.com/fxsjy/jieba">Jieba</a> is the original implementation and it is implemented in python. It became so popular in the wild not only due to it strikes a good trade-off between the segmentation result and the performance. It may not be the most state-of-the-art or the most accurate word segmentation library but it’s a very popular engine and got ported into different programming languages like <a href="https://github.com/wangbin/jiebago">go</a> and the bindings to the <a href="https://github.com/yanyiwu/cppjieba">cppjieba</a>. Rust as a language focus on zero-cost abstraction, naturally I would like to benchmark the rust version against cpp version.</p>
<p>About a couple of weeks back I found that <a href="https://github.com/messense/">messense</a> already had a good version of the rust implementation with the maximum probability approach and the viterbi decoding finished. The code is well-written and self explainable therefore I decided to contribute and collaborate with messense. Even though I started following rust’s development occasionally ever since it was in 0.4, I haven’t catched up the latest 2018 edition, I started the contribution by implementing the left-out keyword extraction features using TextRank and TFIDF to get myself familiar with the language. In the meantime, I was curious how performant the rust implementation is, then I run the weicheng executable in the example folder, which is the same benchmark test used in cppjieba, by running the word segmentation on the weicheng script for 50 times and times the running time. I wasn’t not satisfied with the result since the cppjieba runs about 5-6s on the following spec of Macbook.</p>
<pre><code>MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports)
2.5 GHz Intel Core i7
16 GB 2133 MHz LPDDR3</code></pre>
<p>However, jieba-rs at the time was running like 10s-ish. It was almost the double of the running time of cppjieba.</p>
<p>With a couple of weeks of effort, now as the time of this blog post on June 30th, 2019. The running time of the jieba-rs is now around 33% faster than cppjieba as shown in the following.</p>
<p>Branch <code>darts</code> with the commit: ab2fbfe</p>
<pre><code>➜  jieba-rs git:(darts) ./target/release/weicheng
4138ms</code></pre>
<p>Branch <code>master</code> with the commit: a198e44</p>
<pre><code>➜  jieba-rs git:(master) ./target/release/weicheng
4330ms</code></pre>
<p>Branch <code>master</code> of cppjieba with the commit: 866d0e8</p>
<pre><code>➜  build git:(master) ✗ ./load_test
process [100 %]
Cut: [6.219 seconds]time consumed.
process [100 %]
Extract: [0.679 seconds]time consumed.</code></pre>
<p>You can see that 4138ms / 6219ms is roughly 66.5% of the running time, with the double array trie implementation adopted (not yet merged into the master).</p>
<p>I would also like to mention the <a href="https://github.com/andelf/rust-darts">rust-darts</a> implementation finished by <a href="https://github.com/andelf/">andelf</a>. He helped a lot on updating the three-year-old library so that I could import the library into jieba-rs to try it out. It’s on my roadmap to use DARTS as the underlying data structure once the required <a href="https://github.com/andelf/rust-darts/issues/24">features</a> are implemented.</p>
<p>Now, I’d like move on to the key steps I did to reduce the running time as much as possible.</p>
<h3 id="identifying-the-bottleneck-with-instrument">Identifying the bottleneck with Instrument</h3>
<p>The first step of performance tuning is to measure it and identifying the critical sections so you could put your efforts at the slowest part. The write-up from <a href="https://github.com/siddontang">siddontang</a> about <a href="https://www.jianshu.com/p/a80010878def">optimizing TiKV on Mac</a> is very helpful. By recoding the stack traces of the weicheng binary, it is easy to tell that the program spends its time on two parts: Regex and Viterbi Decoding.</p>
<h3 id="regex">Regex</h3>
<p><a href="https://github.com/BurntSushi">BurntSushi</a>’s regex library is very performance in general since it’s executed in DFA. The <a href="https://github.com/rust-lang/regex/blob/master/PERFORMANCE.md">documentation</a> also marked it clear where you should be looking at when the performance is the consideration you have.</p>
<pre><code>Advice: Prefer in this order: is_match, find, captures.</code></pre>
<p>A few lines of <a href="https://github.com/messense/jieba-rs/commit/3d013211f3d76d00680f6f670c4f96b808f43571">changes</a> from using Captures to Matches reduce the running time by 2s.</p>
<h3 id="using-dynamic-programming-to-trace-the-best-path-in-viterbi-decoding">Using dynamic programming to trace the best path in Viterbi decoding</h3>
<p>I have speech processing and natural language processing background so I am quite familiar with hidden markov model training and viterbi decoding. It used to be just a midterm course assignment when I was the TA of the course. I am pretty sure the viterbi decoding and path reconstruction could be done in two two-dimension matrices. In the original python implementation it has done extra memory allocation by copying the best path so far, where actually you only need to track it by a prev matrix to remember the last state. <a href="https://github.com/messense/jieba-rs/commit/2d1418092f595a3799d2d90f7a314b6855898261">changing this</a> earned us another 2s of speedup.</p>
<h3 id="move-memory-allocation-out-of-the-loop">Move memory allocation out of the loop</h3>
<p>With the convenient Vec and BTreeMap provided in the standard library, we often forget there are actually expensive memory allocation hidden behind the abstraction. The word segmentation is basically a task where you are running a main loop to loop through the character stream to see if it could be broken up here. By allocating the memory outside the loop would obviously boost the performance, and with the handy resize you only need to reallocate when you need more memory.</p>
<h3 id="remove-allocation-as-much-as-possible">Remove allocation as much as possible</h3>
<p>Transient Vec storage is convenient and it could make the code more readable, but when it comes to the scale of milli-second, it would be better to rethink if you really need them. Some cases you only need to track the states by one or two variables.</p>
<h3 id="swisstable-is-damn-fast">SwissTable is damn fast</h3>
<p>Right now we are using HashMap to store the dictionary and all of the prefixes of those words on the master branch. We know the HashMap in the standard library now is the hashbrown implementation, where it is based on Google’s Swiss Table. You can watch the introduction talk Google Engineer gave at <a href="https://www.youtube.com/watch?v=ncHmEUmJZf4">CppCon 2017</a>, it’s very inspiring on the approaches. SIMD instructions like SSE was used and therefore it’s damn fast when you are using Intel’s CPU. Even with the naive iteration through the prefixes of the strings to construct the DAG, it’s still performant since the scalar is so small. A theoretical better algorithm but bigger scalar absorbed into the big-O could perform far worse than naive approaches with SwissTable.</p>
<h3 id="adopting-double-array-trie.">Adopting Double Array Trie.</h3>
<p>There has been <a href="http://www.hankcs.com/program/algorithm/double-array-trie-vs-aho-corasick-double-array-trie.html">researches</a> comparing aho-corasik and double array trie ’s speed with respective the maximum forward-matching word segmentation algorithm, and the conclusion was double array trie performs better in the case of Chinese word segmentation due to the data access pattern. I tried it out both and the conclusion matches, at least for the implementation we have as the time of June 2019.</p>
<p>For aho-corasick I was using the library implemented by BurntSushi, so I assumed it is the best in class. It performs much worse than DARTS, therefore I briefly looked into the code base, it seems that there are indirection on whether to use NFA or DFA when <a href="https://github.com/BurntSushi/aho-corasick/blob/master/src/ahocorasick.rs#L1001">the methods are called</a>, I am not sure this is the main reason resulting into the significant slow down though.</p>
<p>For DARTS implementation I am using the one ported by <a href="https://github.com/andelf/">andelf</a> from <a href="http://chasen.org/~taku/software/darts/">darts</a> in C++. Its code is much easier to read than libdatrie though lacking the features of dynamic insertion and deletion and tail compression. I am intended to add those features into the <a href="https://github.com/andelf/rust-darts">rust-darts</a>.</p>
<h2 id="lesson-learned-and-afterword">Lesson Learned and Afterword</h2>
<p>Over the course of performance tuning and development I am convinced that Rust ecosystem has its potential and it seems to get a lot momentum in 2019, with so many Rustcons host around the world. Its spectrum of libraries is still far from the richness and matureness where you can find in C++ and Python world, but the standard library and a few core libraries contributed by star developers are very high quality and you could learn a lot by merely reading their code base. And its affine type system makes you feel safe to do a major refactoring for the performance tuning etc since you know compiler would call you dumb and catch those errors. If you are careless about a few of the details, you could still easily spit out poorly performing code. However, it’s also easy for you to improve it once you gain some experience on performance tuning and know where you should look at.</p>
<p>Specially thanks again the <a href="https://github.com/messense/">messense</a> and <a href="https://github.com/andelf/">andelf</a> on the quick response to the pull requests and reviews. It’s a pleasant experience to work on these projects in Rust with good developers.</p>
]]></summary>
</entry>

</feed>
