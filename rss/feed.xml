<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Paul Meng's Blog</title>
    <link href="https://blog.paulme.ng/rss/feed.xml" rel="self" />
    <link href="https://blog.paulme.ng" />
    <id>https://blog.paulme.ng/rss/feed.xml</id>
    <author>
        <name>Paul Meng</name>
        <email>me@paulme.ng</email>
    </author>
    <updated>2019-10-18T00:00:00Z</updated>
    <entry>
    <title>所謂量化與質化的思考</title>
    <link href="https://blog.paulme.ng/posts/2019-10-18-%E6%89%80%E8%AC%82%E9%87%8F%E5%8C%96%E8%88%87%E8%B3%AA%E5%8C%96%E7%9A%84%E6%80%9D%E8%80%83.html" />
    <id>https://blog.paulme.ng/posts/2019-10-18-%E6%89%80%E8%AC%82%E9%87%8F%E5%8C%96%E8%88%87%E8%B3%AA%E5%8C%96%E7%9A%84%E6%80%9D%E8%80%83.html</id>
    <published>2019-10-18T00:00:00Z</published>
    <updated>2019-10-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>看到在 Facebook 上有對於 Bill Nygren 回覆關於有什麼他希望他當年開始投資的時候希望他自己能知道的事。他的回答大致是說希望對看中本質而不要只依賴於量化。在他的經驗裡沒有一個完全依靠量化模型賺錢的。</p>
<p>這剛好也是我最近在思考的，AQR 之前也有一集 Podcast 講量化跟質化哪個比較好。不過我注意到大家在講量化的時候沒有一個統一的定義。金融領域之中真的充滿了缺乏嚴謹定義的名詞跟 Misnomer。 我覺得要討論必須對【量化】這個東西下定義，討論才會比較均衡。同樣的·問題拿去問 Jim Simons 跟 Ed Thorp 大概會得出完全不同的答案。</p>
<ul>
<li>Jim Simons 早期也是做 Fundamental 的，但他因為一些經歷後覺得質化分析不夠可靠，最後跟他研究團隊最後決定全部走量化</li>
<li>Edward Thorp 早年其實有跟巴菲特接觸過，那時候它就有預測他會變成全美最有錢的人，但他因為覺得自己擅長量化，又不想整天搞財報拜訪公司，所以就自己最後就變成統計套利的始祖。</li>
</ul>
<p>當然，很明顯 Bill Nygren 的量化跟 Jim Simons, Edward Thorp 的量化是完全不同的東西。我覺得 Bill Nygren 也只能說是他自己的優勢他做不到。但我覺得對於數學天才還是可以做得很好。而且質化還是要面對各種人性的弱點，只是這些弱點不好顯現，或是這些說話的人天生比別人要強。最終還是回歸你【相信】你自己的優勢在哪裏。但說實在相信也是容易造成 confirmation bias，也許就是想要這麼相信所以才找跟你體系相近的投資人的答案。</p>
<p>隨著科技的演進，我相信量化跟質化的差異會越來越小，以前覺得只有人能判斷的部分都可能隨著自然語言處理的進步而進入量化系統中，有聽說華爾街也是有投資一些這方面的資金在分析 twitter 上面的舉動的。只要研究有一天能夠順利做出 encode 人類的邏輯進電腦的邏輯，那純粹人的質化優勢會越來越小。</p>
<p>我目前的思考是，人類質化在強人工智慧來臨之前。他其實很重要的一點是能判斷什麼東西儘管沒看過，但可以用邏輯判斷有違於常識。這就是有一個判斷基準說某一套邏輯不會 work，畢竟以金融市場這樣會因參與者行為改變的系統（也就是索羅斯說的反身性），沒有永遠 work 的系統。而 Ed Thorp 說過要用一個系統非常重要的是你必須事先設定一個判斷說什麼條件發生的時候你的系統有問題，而一般人用量化系統的時候沒有考慮這層（先不說亂用 R-sqaure 的結論），而那是靠人腦的質化研究可以簡單提供的。</p>
]]></summary>
</entry>
<entry>
    <title>目前我看懂的幾種辨別投資機會的方式</title>
    <link href="https://blog.paulme.ng/posts/2019-10-15-%E7%9B%AE%E5%89%8D%E6%88%91%E7%9C%8B%E6%87%82%E7%9A%84%E5%B9%BE%E7%A8%AE%E8%BE%A8%E5%88%A5%E6%8A%95%E8%B3%87%E6%A9%9F%E6%9C%83%E7%9A%84%E6%96%B9%E5%BC%8F.html" />
    <id>https://blog.paulme.ng/posts/2019-10-15-%E7%9B%AE%E5%89%8D%E6%88%91%E7%9C%8B%E6%87%82%E7%9A%84%E5%B9%BE%E7%A8%AE%E8%BE%A8%E5%88%A5%E6%8A%95%E8%B3%87%E6%A9%9F%E6%9C%83%E7%9A%84%E6%96%B9%E5%BC%8F.html</id>
    <published>2019-10-15T00:00:00Z</published>
    <updated>2019-10-15T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>來寫一下我目前的理解。寫作也是為了理清自己的思路。以下要講的都是公開市場的機會，對於像是投資銀行跟 Private Equity 他們的投資及會對於一般人太遠了。我目前看懂的大致分為下面幾種:</p>
<ol type="1">
<li>Organic Growth</li>
<li>Roll-up</li>
<li>Asset Play</li>
<li>Turn Around</li>
<li>Commodity Cycle</li>
</ol>
<h2 id="organic-growth">Organic Growth</h2>
<p>這一類通常都是初學者會接觸的，也是電視名嘴特別愛講的。特別是巴菲特晚期的一些名言透過媒體變得廣為所知後，一大堆投資部落客都愛說自己是遵循巴菲特哲學，滿嘴巴菲特但其實他們九成以上都沒有仔細去研究巴菲特早年 Partnership 的投資模式。不過巴菲特晚期的哲學的確是王道，這一類也的確都是優質的企業，如果可以用合理或是便宜的價格買到真的就是不要輕易放掉。只是主要的問題是等到你知道的時候都已經漲上去了。譬如說，現在 Visa 跟 Google 的簡易指標都很高</p>
<ul>
<li>Visa: EV/EBITDA = 25</li>
<li>GOOG: EV/EBITDA = 16</li>
</ul>
<p>EV/EBITDA 大致代表的意義是付出一定的價錢買的的現金流或是收入回收所需要的時間。跟 P/E 不同的是 EV 計入了債權一起進去考量，並且扣點 Balance Sheet 上的現金，用會計上的收入扣掉折舊等等來當作現金流的快速計算。所以 EV/EBITDA = 25 就是我花錢買 Visa 的股票假設經營狀況不變的情況下至少要 25 年才能回本。在沒有爆發性成長的前提下這是一個很貴的價錢。根據經驗還有我聽其他投資人的分享，一般來說都是 EV/EBITDA = 10 是合理價，EV/EBITDA &lt;= 8 是便宜，EV/EBITDA &lt;= 5 是大甩賣。不過也是要看市場，在日本的話企業經營都比較保守，帳上會有很多現金，因此經常會看到 EV/EBITDA &lt;= 8</p>
<p>雖說也是可以追高，特別是現在有指數基金提供動能的情況下追高的動能有可能還是可以運作好一陣子，這些也都是好企業，但這就表示你的投資邏輯是奠基於</p>
<ol type="1">
<li>業績會不斷如預期增長，甚至超越</li>
<li>之後有人願意用更高的價錢接受</li>
</ol>
<p>這預測的難度其實是不低的，只要你預測出錯就是除了一個很貴的價錢卡在那邊。特別是大企業多少投資法人請最好的人全職去研究，就算不說法人通常明顯的優勢大家都看得出來，也就是你沒有特別有優勢。你要能夠贏過其他人，通常有可能的就是</p>
<ol type="1">
<li>這是你的本業，而且你是行業中的佼佼者。</li>
<li>因為一些因素你先看到了，可能是因為你看到的生意是地區性的而你就住在那邊。</li>
<li>你對行業也有足夠了解。</li>
</ol>
<p>衡量競爭優勢跟成長性都沒有想像中容易。隨著時代轉變，所謂競爭優勢也不斷出現非傳統的護城河，像是 Netflix。但多多少少企業不到短短幾年又被後來者追過。所以巴菲特才會喜歡消費股，因為被科技演進淘汰的機率較小。這一點會牽扯到你抱不抱得住股票</p>
<ul>
<li>過程並不平順，而且人會不斷有新的想法或改變想法而抱不住。就算職業投資人也非常難。</li>
<li>外行人不好評估跑道多長。</li>
<li>非職業投資人的話一生能抓到一兩個 10 bagger （10 倍股) 就不錯了</li>
</ul>
<p>針對如上的各個困難點，要能夠彌補的話並沒有什麼好方法</p>
<ol type="1">
<li>盡量挑你本業相關的，或是相近的產業。</li>
<li>除非是金融危機千載難逢的機會你可以用低價買到好的大企業，不然 Micro-cap or Nano-cap 也是有機會的，可以避開跟法人做對手。只是盡量在歐美法治公司治理比較好的地方比較不會碰到老千，香港跟台灣的問題就是公司治理問題會讓人不敢碰小企業。</li>
</ol>
<h2 id="roll-up-leveraged-roll-up">Roll-up / Leveraged Roll-up</h2>
<p>跟自然地業績成長不同，這一類型的公司是不斷透過收購合併來成長。最有名的就是 John Malone 的 TCI 還有他建立的 Liberty Complex。巴菲特投資 Liberty Complex 也是有頗長的歷史，如果把大大小小合併起來的話可能佔他頭幾大部位。</p>
<p>這類的玩法原理說起來很簡單，但實際做起來不容易。要簡單解釋的話就是他很類似某一類房地產玩家的做法。</p>
<ol type="1">
<li>我選好適合出租的房子，背槓桿買下房子改裝出租。如果自己一開始出 20% 股債比就是 1:4。也就是如果租金收入對房價是 4% 的話 (ROA = 4%)，那 ROE 就放大成將近 20%。不及 20% 是因為還要付利息。</li>
<li>出租的租金足夠多，而且穩定。就可以依靠他來付房子的貸款，然後長期降槓桿。如果付掉一半的話股債比就降到 1:1，ROE 也會下降到約 10%</li>
<li>如果這時候房子的房價太高了，那就把房子脫手賺取泡沫的估值價差。</li>
<li>如果沒有泡沫，同時有看到其他不錯適合出租的物件，就用現在手上的房子再貸出更多錢買下房子出租，回到步驟一如此重複。ROE 又回到 20% 以上</li>
</ol>
<p>同一套玩法也可以拿來套在做企業上，而這就是 John Malone 的玩法。但這個玩法有很多前提。</p>
<ol type="1">
<li>現金流的穩定度很重要，因為你要拿來付債息。</li>
<li>降息的環境也重要，這跟槓桿的資金成本密切相關。</li>
<li>業務要有整合性，房子的話是相對容易。但企業整合的話會有文化的問題。員工可能會反彈。你要整合業務省錢的話，也是會有原有企業動機不足的問題。</li>
<li>產業要足夠零碎讓你有整合的空間，並且業務不複雜。太複雜的業務整合不易。</li>
</ol>
<p>失敗的案例就是 Bill Ackman 曾經的最大持股 Valeant。最後爆掉也是因為整合後提升藥價受到整體社會反彈。而且後期付出收購企業的錢越來越高，因為產業不夠零碎，人家知道你的玩法後就提價讓你收購價高到無法負荷。</p>
<p>通常房地產在社會發展前期會有這樣發展的空間，房地產商也通常是這樣起家的，從小做到大，只是他們不是用出租而是用販賣物件的方式風險，但槓桿的原理是類似。但因為也是這樣因為槓桿現金流不足爆掉的風險會更大，多半在經濟週期後段爆掉也是因為太貪心或沒遠見而沒有適時降槓桿。</p>
<h2 id="asset-play-net-net">Asset Play / Net-Net</h2>
<p>這就是巴菲特的啟蒙者 Ben Graham 所開發出來的一種方法。巴菲特早期在合夥人時期也幾乎都是用這種方法，他是到波克夏時期遇到查理孟格才逐漸轉變投資方式。</p>
<p>這種方法的核心是奠基在邏輯推導，如果一個企業的市值小於他的淨現金部位，也就是【現金 + 待收貨款 + 折價後的存貨跟短期投資 - 所有負債】。那這個企業理論上不應該存在，因為清算還比你繼續經營還划得來。巴菲特早期就靠入股這種企業然後清算賺了很多錢，不是像現在他在媒體上慈眉善目的形象。</p>
<p>計算資產的時候現金跟待收貨款是最好，存貨的話像是流行服飾清算的話就沒什麼價值，但如果是勞力士之類的話二手市場是存在價值的。而其他硬資產像是土地是最好，飛機通常也能賣到五折，在做這架的時候要根據產業不同用不同估算方法。</p>
<p>這種方法的缺陷就是你對企業沒有直接的控制權，除非你想巴菲特直接買到最大持股去影響董事會。所以最初的方法論是你要買一籃子這種企業，而且平均分散在不同產業。搭配上一些基本指標譬如說不要太多負債等等，然後靜靜地等待未知的催化劑，有可能是均值回歸，或是清算，或繼續賠錢而倒閉。 Ben Graham 提倡的是你買入一籃子然後等三年或賺 50% 賣掉。</p>
<p>這種方法的缺點是，現在已經廣為所知，所以在比較有效率的市場都已經被套利完了。剩下就是日本比較多，所以後來就有點拓展到其他方法。</p>
<ol type="1">
<li>公司有隱蔽性資產，譬如公司有一筆很早買的土地，在 Balance Sheet 上沒有根據市價重估。</li>
<li>公司有一些經營危機，但如果分拆賣個 Private Equity 或競爭對手的話，可以賣到比繼續經營好的價錢</li>
</ol>
<p>但這些的缺點都是要有 Active Investors 去處理，因為職業經理人只是打工的，沒有動機去做。因為企業清算或是賣掉了管理層就失業了。比起清算還不如繼續吸血領高薪。而 Active Investors 的能力目標利益是否一致也是很重要。這類型都是理論上會 Work，但最後因為人為因素搞砸的機率也是有。</p>
<h2 id="commodity-cycle">Commodity Cycle</h2>
<p>原物料一個特性是他有週期，而且週期很明顯，如果能夠大致抓對時機耐心等待的話就能夠賺到錢。只是要抓對能夠撐過週期的企業不容易，而某些商品因為是國際性商品，特別難以預測，像是石油就是，因為參與的國家太多，賽局太複雜。俄羅斯，沙烏地阿拉伯，美國，委內瑞拉等等。通常連專家都預測不太準。一般來講都是要搭配總經數據，看到等到確定的信號再投進去，放棄頭尾段只賺中間的部分。股價特性常常是低迷很長一段時間然後暴衝。</p>
<p>但產業中有一些特別的部分，像是有煤業是只賺權利金的部分的企業，也就是他不出資開採，他是幫別人開採然後賺綁定通膨的差價部分，就會比較抗週期。或是有一些開採後的成品有運送成本的限制，像是水泥的原料 Lime Stone，那如果公司是在內陸的話，那就會有對於國際供應的護城河。或是看天然氣裡面擁有的地的開採成本比其他地方都低，那在賽局中大家都不得不擴產賺取現金的時候，他就可以撐比較久，然後在最後的賽局中活下來。</p>
<p>同樣這一類難度也是不低，但好的是你幾乎可以確定會有下一次週期，不像有些產業被革命了就再也回不來了。</p>
<h2 id="turn-around-over-pessimistic">Turn Around / Over-Pessimistic</h2>
<p>這一類很像是買賣二手物品一樣，許多二手物品都有瑕疵，但稍微整理一下都能買到好價錢。如果有在二手拍賣網站像是 Carousell 買賣過東西一樣，就知道有一類二手商整天就在上面看有沒有人大甩賣商品，他用超低的價格撿到，稍微整理一下就可以賣出差價。而股票也是，這類通常都是有些問題的企業，但股價超便宜。如果成功 Turn around 的話，估值修復很容易至少成為 2-bagger 或 3-bagger 股。或是用房地產舉例，說有人開玩笑一百塊錢台幣賣你一棟鬼屋你要不要？有些人不理性不計算就覺得給我鬼屋我也不能住送我也不要，但理性人應該是計算整理後是否能賣出合理價，扣除稅費後是不是比一百塊便宜，可以想見只要價錢夠低，就算是爛物件很高的機率你的推論會是對的。</p>
<p>這類企業有時候跟 Asset Play 重疊，譬如說 Balance Sheet 裡面有其他值錢的投資，可以 cover 掉負債。但很多時候你對產業有些瞭解，知道一些眉角。譬如說在零售業裡面店租是很高的成本，但你發現某家零售業雖然業績下滑但 Operational Lease 裡面是 Cancellable 也就是可以退租不用被合約綁住五年的，如果 Balance Sheet 上面沒有負債的話變成 0 的機率就更低。或是香菸雖然被法規限制，但法規限制的同時，小企業沒辦法付出法遵的成本，所以實際上會造成寡占。或是企業雖然槓桿很高但現金流沒有惡化。並且債券到期日是平均分散的。債多不是問題，流動性才是問題。也有可能是因為減稅或會計規則的變化而大多數人漏算了某部分的資產。如果 CEO / CFO / COO 有自己買進股票並在股東會上透露一些訊息通常會是一個 confirmation。而通常公司業績下降在商業發達的地方都會很有動機拉回來的，畢竟股東不想賠錢，管理層不想失業。最差可能就賣個競爭對手多少回收一些比清算要好的價錢。</p>
<p>但玩這套最好要要在法治跟商業道德比較好，還有 Investor 比較容易介入的地方會比較容易。台灣跟香港有太多公司治理的問題，大陸就更不用說了。而且大陸很多地方政策，法治，公共政策資料也不透明。而且產業跟地區最好分散，這樣比較不會有系統性連動的問題。可參見我講指數基金的問題那篇。</p>
<p>如果你有一籃子，假設股價彼此沒有強相關性。用簡單的數學計算：保守估計平均 3 年內 Turn Around，不然歸零，你如果有 7 成的機率猜對，那期望值如下。</p>
<pre><code>2 * 0.7 + 0 * 0.3 = 1.4</code></pre>
<p>年化報酬率有 12%。你如果有 8 成的機率猜對，那期望值如下。</p>
<pre><code>2 * 0.8 + 0 * 0.2 = 1.6</code></pre>
<p>年化報酬率有 17%。</p>
<p>如果長期關注的話，小部位多做多觀察加大經驗可以隨著交易經驗越來越多，學習越來愈多產業經驗，辨別失敗跟詐騙的情況會越來越準而增加獲勝機率。</p>
<p>比較怕的是有時候你會看到邏輯上特別好的機會而賭大部位，但實際上是 Value Trap，被稱為下一個巴菲特的 Eddie Lampert 就因為 Sears 都死在這裡。包含巴菲特自己都中過好幾個爆掉的投資，Berkshire Hathway 這個殼就是其中一個爆掉的，他把原本業務清算掉變成投資公司。所以有人也認為巴菲特有可能是倖存者偏差。現今巴菲特的左右手 Ted 也是靠賭大的這類企業賭對的而發家的。</p>
<p>經過之前經驗我覺得還是小部位分散多個會比較穩定，這種方式其實有一點像 Morgage Backed Security，慢慢賺讓期望值站在你這邊。然後隨著更多的資訊的揭露 Buying up，就好像玩撲克的時候隨著牌越開越多加賭注一樣。等逆轉了被確定再加一些，但嚴格控制部位上限。</p>
<h2 id="dumped-by-mechanical-device">Dumped by Mechanical Device</h2>
<p>這一類就是某些因為機械性原因而被無條件拋棄，最簡單的就是被 Index delist 的股票，反過來如果被加進 Index 的話也可能漲上許多。或是一些基金因為股票太便宜而不能購買，或是有一些道德因素而必須被清掉。Spin-off 雖然不是機械性的原因，但也很容易因為投資人不熟而無條件賣掉。Joel Greenblatt 就是靠 Spinoff 發家的。</p>
<h2 id="結語">結語</h2>
<p>除了基本的方法論之外，具體的產業知識是很重要的，這些都藏在 CEO 的談話中，或是有些秘密只能反向去推敲，沒有人會跟你講。都是要長期累積，但了解的話勝率會提高不少。</p>
]]></summary>
</entry>
<entry>
    <title>Life Reliability Engineering</title>
    <link href="https://blog.paulme.ng/posts/2019-10-06-life-reliability-engineering.html" />
    <id>https://blog.paulme.ng/posts/2019-10-06-life-reliability-engineering.html</id>
    <published>2019-10-06T00:00:00Z</published>
    <updated>2019-10-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I have the feeling that we have an epidemic of stress, unhappiness and anxiety in the tech industry. It may be due to the working culture, perfectionism or simply because of toxic working environment. You could tell that from the suicide of Facebook employee a couple of weeks back, and Amazon employee in the last year. From time to time I would also experience such kind of tide in emotion in life. In order to tackle this I have developed my own kind of system to keep my mental health in control. And I didn’t find a good way to describe it by metaphor to explain it to another engineer until I am more familiar with the terminology of Site Reliability Engineering.</p>
<p>First, we have to identify catastrophic patterns to cause your system downtime. In life, it means you go depressed, and want to kill yourself, or simply lack of motivation to anything. One important observation is that the stuffs you might consider important are correlated to each other. The life could give you a straight of bloody Marry due to this. One bad thing could lead to the other. Even you are born in a middle-class family and have a good enough income, or slightly better in recent year as a software engineer. Your life could be easily destroyed. It doesn’t have to be financially, if you are depressed and committed suicide even you have billions in bank it is still a checkmate to your life. For example, it could be your marriage affects your work performance and get you fired, and you lose your income. Or it could also be your important partner is sick, it not only costs you a fortune to pay the medical bill if you are living in countries like U.S.A or Singapore, where medical bills are expensive comparing to the median incomes. And since you have financial burden, your significant partners could be tempted to abandon you since they would not like to be involved etc. In order to prevent the chain reaction, you have to manage your “components” to be more resilient against each other, and not be over-confident to have too many components in your life. For example: getting overly leverage financially on your housing but at the same time raising kids, after all raising kids and marriage are liabilities that is off balance sheet. And if you already have components you are reluctant to get rid off, make sure you have insurance covered in a proper way to reduce the over all risks, at least stopping the chain effect when it happens. The best, for sure, to have low liability.</p>
<p>Once you find a way to manage your portfolio of liability, a portfolio of “bomb shelters” or “chambers” where you could find peaceful time to recover is important. Over-stretching on a specific goal is dangerous since if that goal fails your mental health would be seriously damaged. For example, if you would like to climb the ladder of corporate or build a company. You have to go all out but make sure not to over stretch that could cause a total meltdown. If your goal fails, if you still have other goals like playing sports/games where you could find small achievement, that’s a hedge for your mental meltdown when you fail to build a company or climb the ladder. It’s a diversification of your goal, to make sure it’s unlikely that they correlate each other. If your secondary goal is to tied to your kids or partner, due to human minds are not stable. They might not care or understand your situation when you are down for those goals. It could be a shelter but not a reliable hedge. Having nice food is a good one since it’s unlikely that it would be affected by human factors, but having it too often it could hurt your lookings or physical health as well.</p>
<p>Other than the diversification, I find stoicism is also useful to me. Here is a video from Tim Ferriss.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/5J6jAC6XxAI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>I am not a fan of what’s been in American main stream culture on the alpha-male like of “Leadership.” Human are not infalable and actually quite easy to be broken. Overly stretching to the image of “confidence” and “overcome your poster syndrome by pretend it until you make it.” would easily make yourself vulnerable to a strike-back. I believe the Newton’s third law of motion applies here as well, if you stretch too much, you are exerting the same magnitude of force in the opposite in the same time.</p>
<p>These are the “tips” or “speculation” that I have in mind for years. It’s not a science or corollary that would be deemed as universal and could be applied any kind of extreme situation (for example, war time or you are put in concentration camp). But at least it kind of works for me in modern time living in big cities.</p>
]]></summary>
</entry>
<entry>
    <title>市值權重指數基金與系統性設計問題</title>
    <link href="https://blog.paulme.ng/posts/2019-10-04-%E5%B8%82%E5%80%BC%E6%AC%8A%E9%87%8D%E6%8C%87%E6%95%B8%E5%9F%BA%E9%87%91%E8%88%87%E7%B3%BB%E7%B5%B1%E6%80%A7%E8%A8%AD%E8%A8%88%E5%95%8F%E9%A1%8C.html" />
    <id>https://blog.paulme.ng/posts/2019-10-04-%E5%B8%82%E5%80%BC%E6%AC%8A%E9%87%8D%E6%8C%87%E6%95%B8%E5%9F%BA%E9%87%91%E8%88%87%E7%B3%BB%E7%B5%B1%E6%80%A7%E8%A8%AD%E8%A8%88%E5%95%8F%E9%A1%8C.html</id>
    <published>2019-10-04T00:00:00Z</published>
    <updated>2019-10-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>前一陣子因為 The Big Short 而出名的 Michael Burry 回覆了 Bloomberg 對於市場看法的信而又引發了網路上一場論戰。因為他的回覆中關於 Index Fund 的說法提到了 “Bubble” 這個詞而許多人擺錯了重點，把重點擺在是不是有 Bubble 這件事。其實他的說法跟之前 Howard Marks 還有 Seth Klarman 提到的並沒有不同，其實關鍵的重點在於結構設計，而每個結構設計跟實體工程一樣都有他的極限在，特別是在金融市場中因為結構承重量或容錯量是不可見的，涉及抽象的概念。雖然我完全同意 Micael Burry 的說法，但我一直想不到一個好的比喻跟一般人解釋這件事。直到我前幾天看到 yinwang 關於自動駕駛責任的<a href="http://www.yinwang.org/blog-cn/2019/09/30/autopilot-responsibility">文章</a>。我覺得解釋得非常清楚，而同樣的邏輯謬誤也是可以套用在市值權重指數基金擁護者的回覆上。</p>
<p>要討論這件事涉及兩個層面</p>
<ol type="1">
<li>為什麼用【市值權重指數基金拿到的是市場報酬】不足以回應 Michael Burry 點出的風險？</li>
<li>市值權重指數基金的缺陷在哪？什麼情景下會觸發？</li>
</ol>
<p>首先用一個例子來說明第一點。在今年初發生了 Boeing 737 MAX 因為設計不良而造成的事故。一時間人心惶惶，而各國政府也相繼搬出政策禁飛 Boeing 737 MAX。為什麼大家會擔心這件事？不是說 “平均” 來講，發生空難的事件極低嗎？ 機率上來講你死在去機場的路上是遠高於你發生空難的機率。這主要的問題是因為 Boeing 737 MAX 有共同的因素驅動，也就是設計的缺陷。</p>
<p>就機率來解釋的話，就是</p>
<pre><code>P(其它 Boeing 737 MAX 墜機  | 一架 Boeing 737 MAX 墜機)</code></pre>
<p>高於</p>
<pre><code>P(其它非 Boeing 737 MAX 墜機 | 一架非 Boeing 737 MAX 墜機)</code></pre>
<p>因為他們不是類似於獨立事件。而是有共同的缺陷。假如每一家航空公司全部都是提供 Boeing 737 MAX 的話，說我身為一個個體明天搭的飛機墜機的機率趨近於平均發生空難的機率一點意義都沒有，因為平均的結果是大家一起因為起飛後因為設計的缺陷造成機頭直接往下墜毀死亡。</p>
<p>所以說【市值權重指數基金拿到的是市場報酬】並不足以回應。正確的回應要解釋為什麼市場報酬背後沒有共同驅動因素造成大家一起死的結果。這在大多數時候是對的，畢竟市場報酬是買方跟賣方資金平衡投票出來的，而且市場中每個人的想法不同，就好像是不同設計的飛機一樣。所以大多數時候市場的驅動的因子沒有共同背後的因素驅動。但接下來解釋的第二點，我會說明在某種特殊情況底下是有可能發生的，也就是市場上的參與玩家是同一個想法佔大多數的時候，而指數權重基金佔大多數也就是其中一個實現。</p>
<p>要了解第二點必須說明市值權重指數基金的運作，這邊舉一個簡單的小例子說明。台灣人很愛買房子出租，所以用房子來舉例，</p>
<p>今天市場中有三種分十年折舊的房子可以帶來租金收入，然後市場裡面有四個投資人 A, B, C, D</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">房子</th>
<th style="text-align: center;">年租金</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">i</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">ii</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td style="text-align: center;">iii</td>
<td style="text-align: center;">10</td>
</tr>
</tbody>
</table>
<p>期間是 10 年，十年後房子價值歸零。市場無風險利率是不會動的 1%，而對於房地產的投資人 A, B 風險利率要求是 2%，加上通膨 3% 折現率共 5%。那這些房子在他們兩個人間的平衡價錢應該是多少呢？用十年的 DCF 可以得到</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">房子</th>
<th style="text-align: center;">價格</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">i</td>
<td style="text-align: center;">7.72</td>
</tr>
<tr class="even">
<td style="text-align: center;">ii</td>
<td style="text-align: center;">15.44</td>
</tr>
<tr class="odd">
<td style="text-align: center;">iii</td>
<td style="text-align: center;">77.22</td>
</tr>
</tbody>
</table>
<p>而投資人 C, D 決定採取不一樣的觀點，不用類似 DCF 的方式衡量，他們決定編制 FOOBAR 市值權重指數，這個指數就是</p>
<pre><code>i 的市價 * (7.72 / 100.38) +
ii 的市價 * (15.44 / 100.38) +
iii 的市價 * (77.22 / 100.38)</code></pre>
<p>他們把他們的錢加起來成立市值權重指數基金，把他們共有的錢 100.38 照上面比例分配買。也就是 i 買 7.72 塊， ii 買 15.44 塊， iii 買 77.22 塊。所以投資人 C, D 認定的價值不是根據衡量物件本身，而是因為市場中其他人的價錢而分配他們的錢的分配。加入今天 A, B 覺得 iii 其實收不到 10 塊的年租金，其實只能收到 2 塊。那就會變成 15.44 塊。那 C, D 就會跟著把 iii 賣掉。因此你可以看到一般所謂的市值權重指數基金就是一種 Trend Following 的策略，是因為市場中其他對價值做出反應的人的定價而跟隨。在這個例子中，當 C, D 反應過來的時候，他們賣的價錢可能會低於 15.44 塊。因為他們的對手只有 A 跟 B。而 A, B 只願意用低於 15.44 塊的價格收，不賣的話就違背了他們指數的規則，所以他們一定要賣，這是機械性訂死的。 為了要賣掉他們就不得不降價，這邊的例子只有 C, D 兩個個體的資金，但你可以想像賣方是這個的 100 倍, 1000 倍然後【同時】想要賣的情況，也就是會造成 race to the bottom。最後平衡價可能遠遠低於 15.44，也許 7, 8 都有可能。</p>
<p>但市場有趣的地方就是上述情況又不是鐵定會發生。假如今天反而是 A, B 突然都變調，也投入 C, D 的指數基金。市場中就不再有人用 DCF 去定價了。所有人都認為權重永遠都是如下：</p>
<pre><code>i: (7.72 / 100.38) 
ii: (15.44 / 100.38)
iii: (77.22 / 100.38)</code></pre>
<p>整個 price discovery 的機制就不見了，不管底下的實際能收到的租金有多少，就是這樣定價，所以說股價都是所謂人類基金管理者亂炒作這說法並不公平，市值權重指數是一種跟隨策略，沒有人類基金管理者他也是不會動的。但這並不是一個穩定的平衡，遲早有人會跳出來點出國王的新衣，然後前面崩毀的情況就會發生。</p>
<p>從上面的邏輯推導與立論可以知道，市場大多數人都在使用某個系統時，而那個系統又因為一些背後共同問題造成時，平均就是大家一起平均死。沒錯，你拿到平均的結果，但你還是死了。但同時，請不要滑坡推論就說 Boeing 或是市值權重指數基金就是該死，身為渺小個體我們應該完全摒棄，因為</p>
<ol type="1">
<li>統計上 Boeing 跟 Airbus 造的飛機比起你自己亂造的飛機要強，儘管 Boeing 737 MAX 有問題。</li>
<li>統計上 Boeing 跟 Airbus 造的飛機比起你自己請的飛機廠商也要強，儘管 Boeing 737 MAX 有問題。</li>
</ol>
<p>但反面來說</p>
<ol type="1">
<li>統計上有瑕疵的立論並不能因此讓我不擔心 Boeing 737 MAX 的結構性問題，除非你能舉出有說服力的邏輯推論說明我舉出的情況不用擔心，因為儘管機率不高或是未知，但我一旦中了就死了。同樣對於市值權重指數基金也是，這關係到許多人畢生的積蓄。</li>
</ol>
<p>我想聲明，市值權重指數基金是非常偉大的發明，他讓大多數不懂金融的人都有機會分享到果實，能承受的資金量也非常巨大，至今都沒發生問題。</p>
<p>我並不是鼓勵說就不要使用指數基金，但與此同時，因為過去沒發生問題而認為以後也不會發生問題，不去用正確的邏輯回應問題。這種過度膨脹的心理而不去理解市值權重指數基金的結構所潛藏的缺陷並提醒大家，在我看來跟那些設計各種金融工程工具但不去點出工程極限的人沒有兩樣，都是不負責任的。</p>
<p>最後安插一個軼聞。Michael Burry 在 2000 年就成立 Scion Capital 來管理家族基金，但他也接受外部他喜歡的人的投資，早年完全是採取類似 Ben Graham 或是 Warren Buffett 在 Partnership 時代的策略，很多人是因為 The Big Short 才認識他，但不知道他是跟 Warren Buffett 同一風格的投資人。好巧不巧在 2001 年的時候，市值權重指數基金的教父曾經在 Forbes 專訪上嘴 Michale Burry。</p>
<pre><code>In the 2001 article, Forbes proposed the following as 
a working definition of a hedge fund: &quot;A hedge fund is 
any investment company that is unregulated, has limited
redemption privileges, and charges outrageous fees.&quot; 
The author also enlisted the help of John Bogle, the 
Vanguard index fund czar, to make the case against hedge 
funds. Bogle obliged, noting that it is not realistic 
for investors to expect the hedge fund industry of more 
than 6,000 funds and $500 billion in assets to outperform 
the rest of the market over the long term.

This is a reasonable observation, and I don&#39;t necessarily 
disagree with him. Unfortunately, he then went on to pick
a name at random from a hedge fund directory to disparage, 
saying: &quot;I don&#39;t know what to do about Scion Capital, 
started by Michael Burry M.D. after leaving his third year 
of residency in neurology.  He started it mostly with his 
own money, $1.4 million, and he&#39;s looking for more. His 
technique to manage risk is to buy on the cheap and, if 
he takes a short position -- I hope you&#39;re all sitting 
down for this -- it is because he believes the stock will 
decline.&quot;</code></pre>
<p>很可惜，他完全就挑錯了人來嘴，我們都知道 2008 發生了什麼事。</p>
]]></summary>
</entry>
<entry>
    <title>Workflow Automation</title>
    <link href="https://blog.paulme.ng/posts/2019-08-23-workflow-automation.html" />
    <id>https://blog.paulme.ng/posts/2019-08-23-workflow-automation.html</id>
    <published>2019-08-23T00:00:00Z</published>
    <updated>2019-08-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>In 2019 the information overflow has become a problem, not being able to fine tune my information channel has annoyed me for sometime, and they could be summarized as follows:</p>
<ol type="1">
<li>The service would mixed-in the Ads with the useful information you need. For example, if you install a comics app, apart from the newly released episode it also sends you the recommendation you are not interested into.</li>
<li>Even the service provides certain level of customization on what should be pushed. It’s not 100% matching what you really need or simply lacking some of the contents you care the most.</li>
<li>It’s the same for RSS, it only provides RSS for everything but you might be only interested into one specific category from this site.</li>
<li>Not to mention that the site doesn’t provide RSS at all and contains all of the Ads. It needs you to keep polling the site so that they could generate revenue from the Ads, or they could track your usage since the content is behind the paywall and requires your login.</li>
<li>Push notification could interrupt you at anytime, you could set it to be muted at certain time range but it is the OS level and not able to customise it to what you would prefer.</li>
</ol>
<p>I would like to have 100% control about my information consumption.</p>
<ol type="1">
<li>I could set the push notification to be delivered at certain time so that the context switch is reduced. The stress on information overflow could be eased without worrying that I would lose track of some of the information</li>
<li>Customise the push and pull model to my like.</li>
<li>Reduce the overhead of polling so that I could reclaim my hours to focus on the important stuffs. If I could reclaim 30 minutes every week, I could reclaim 2 hours a month. Given that a ballpark calculation that office workers has about 112 hours of disposable free time every month, reclaiming 2 hours is significant. (every work day for 2 hours on average and 9 hours each day for weekend excluding that you would like to get up late and fool around to get healthy rest, then in total is 28 hours per week and 112 hours per month).</li>
</ol>
<p>Initially I look up the site to see if there is any pre-existing service that I could leverage by simply just paying to automate the tasks, I tried out a few and decided to roll my own solution. Here are the ones I tried.</p>
<ol type="1">
<li><a href="https://ifttt.com/">IFTTT</a> has been around for a long time, but it is only for If-then-Else and the integration it provided are too simple. The only useful thing I could find is the task for putio.</li>
<li>iPhone’s Workflow is only for iPhone specific macro, it is not what I would like to have a constantly monitoring service and listening to the trigger.</li>
<li>Azure Workflow is better than IFTTT in terms of the complex tasks, and it is pricing model is more friendly if you don’t want to pay. However I felt its UI is unintuitive and buggy.</li>
<li>The best combination is <a href="https://zapier.com/">Zapier</a> and <a href="https://apify.com/">Apify</a>. Apify provides crawling site and turn it into the CSV and JSON, you could also set it to be cron jobs as well, if your crawling volume is not high then it’s free. Zapier maybe the most affordable integration service provider out there. The rest of them often charges several hundred dollars and it is business oriented. For multi-steps workflow you could chose the $25 a month plan from Zapier to integrate stuffs, and you could basically use Google Sheets as your database to wire everything. It is convenient by clicking through forms and set things up and you could test the setup by replay test. However, 25 dollars a month is simply too expensive for personal usage.</li>
</ol>
<p>I decided to roll my own solution in the end, the decision was based on</p>
<ol type="1">
<li>It is much much cheaper by paying 5 dollars a month to Virtual Hosting than 25 dollars a month to Zapier.</li>
<li>There are still automation that neither Apify nor Zapier could match where I need full programming environment.</li>
<li>It’s not significantly more complicated to setup headless-chrome and crawl by yourself to a developer. For sure it is not possible for non-tech-savvy group of people.</li>
</ol>
<p>headless-chrome and puppeteer maybe the best things in these two years to that could make your life easier. It makes the site very hard to tell the difference between bot and your normal activity. Therefore the content that used to be troublesome to crawl is so much easier now. For the content behind paywall you could just load your cookie from the local storage to pretend that you are logged-in and crawl the content. And for the content rendered by javascript you could simply just wait for certain elements to appear then dump the snapshot. As long as you rate-limited your requests, you don’t have to worry about the recaptcha since the site looks your activity pretty much like normal users.</p>
<p>Here is an example of crawling SeekingAlpha.</p>
<pre><code>import * as puppeteer from &#39;puppeteer&#39;;
import * as fs from &#39;fs&#39;;
import * as program from &#39;commander&#39;;

program
    .version(&#39;0.1.0&#39;)
    .option(&#39;-c, --cookie [file]&#39;, &#39;Use the cookie to crawl the website&#39;)
    .parse(process.argv);

if (program.args.length == 0) {
    program.outputHelp();
    process.exit(0)
}

let url_or_file = program.args[0];

function extractItems(): Array&lt;any&gt; {
    let single_articles = document.querySelectorAll(&#39;.author-single-article&#39;);

    var comments = [] as any;
    for (let single_article of single_articles) {
        let comment = single_article.querySelector(&#39;.author-comment-content&#39;);
        let article_link = single_article.querySelector(&#39;.article-link&#39;);

        if (comment != null &amp;&amp; article_link != null) {
            let o: any = {
                &quot;comment&quot;: comment.textContent,
                &quot;article_link&quot;: &quot;https://seekingalpha.com&quot; + article_link.getAttribute(&quot;href&quot;),
                &quot;article_title&quot;: article_link.textContent
            };

            comments.push(o);
        }
    }

    return comments
}

async function loadCookie(cookie_path, page): Promise&lt;void&gt; {
    var objects = JSON.parse(fs.readFileSync(program.cookie, &#39;utf8&#39;));
    if (objects.length) {
        for (let cookie of objects) {
            await page.setCookie(cookie);
        }
    }
}

(async () =&gt; {
    const browser = await puppeteer.launch();
    const page = await browser.newPage();

    if (program.cookie &amp;&amp; fs.existsSync(program.cookie)) {
        await loadCookie(program.cookie, page);
    }

    if (url_or_file.startsWith(&#39;http&#39;)) {
        await page.goto(url_or_file, { waitUntil: &#39;domcontentloaded&#39; });
        await page.waitForSelector(&#39;.author-comment-content&#39;);
    } else {
        let htmlContent = fs.readFileSync(url_or_file, &#39;utf-8&#39;);
        await page.setContent(htmlContent);
    }

    let items = await page.evaluate(extractItems);
    console.log(JSON.stringify(items));
    await browser.close();
})();</code></pre>
<p>It’s pretty intuitive and you could deploy the script to your virtual host. And deliver the content to Telegram as push notification. What you need to do is just to create a bot from Telegram and leverage the rubygem to send the text. The richness in Rubygem makes the glue programming quite easy and short. It’s not really significantly difficult to an experienced developer to do it than using the service like Zapier.</p>
<p>Since the bandwitdh on Virtual Host is also much faster than your home’s ADSL, it is also better to move the large files between services on the server. I could easily use the script to move the file from putio to google drive.</p>
<pre><code>#!/usr/bin/env bash
TARGET_DIR=$HOME/file_pipe

cd $HOME
filename=$(lftp ftp.put.io -u $PUTIO_PASSWD -e &quot;set ftp:ssl-allow no; cd Hook; ls; exit;&quot; 2&gt; /dev/null | awk &#39;{print $9}&#39; | head -1)

if [ -z &quot;$filename&quot; ]
  echo &#39;no file to pipe&#39;
  exit
fi

lftp ftp.put.io -u $PUTIO_PASSWD -e &quot;set ftp:ssl-allow no; cd Hook; get $filename; exit;&quot; 2&gt; /dev/null
mkdir -p $TARGET_DIR
mv $filename $TARGET_DIR
renamer.rb $TARGET_DIR/$filename

cd $HOME
for f in $(ls $TARGET_DIR)
do
    drive push -no-prompt --files $TARGET_DIR/$f
done

rm -rf $TARGET_DIR
lftp ftp.put.io -u $PUTIO_PASSWD -e &quot;set ftp:ssl-allow no; cd Hook; rm $filename; exit;&quot; 2&gt; /dev/null</code></pre>
<p>With these scripts I need to pay zero attention to the contents, the push would be sent to my Telegram, it’s all customizable and I could turn off the app notification completely. All I need to make sure there is health check script that would remind me when the server is down (with the warnings also sent by Telegram since I don’t need a full solution of health monitoring).</p>
<p>Next milestone I would turn to the browser automation by developing my own browser extension. Due to my heavy usage pattern of Read It Later, I accumulate thousands of links over the years and I am trying to design a workflow that would help me read the information more effective, but not just by dumping the information into Pocket and pretend that I would read it and create a delusion that makes myself feel satisfied. I hope that it could automatically tagging the information and create Trello card so that I could put the tasks into my personal planning priorization. Once I feel the workflow is running well I would have another post on that.</p>
]]></summary>
</entry>
<entry>
    <title>Changing the License of my open source projects</title>
    <link href="https://blog.paulme.ng/posts/2019-08-23-changing-the-license-of-my-open-source-projects.html" />
    <id>https://blog.paulme.ng/posts/2019-08-23-changing-the-license-of-my-open-source-projects.html</id>
    <published>2019-08-23T00:00:00Z</published>
    <updated>2019-08-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>After listening to the <a href="http://lucien.cc/about-2/">Lucien C.H. Lin</a>’s talk at <a href="https://coscup.org/2019/">Coscup 2019</a> about his experience on working with corporates dealing with FLOSS communities and licenses, I learn more about the practice in the industry. It also makes me to think about the license I would be using for my open source projects. <a href="http://neilmitchell.blogspot.com/2018/08/licensing-my-haskell-packages.html">This</a> article from Neil Mitchell in Haskell community explained his stance very well, and by carefully reading his article I would tend to agree with his intent. I would be inclined to change all of my projects to be Apache-2.0 and BSD-3 dual license so that it has patent grant protection and at the same time compatible with GPL. Rust is using Apache-2.0 and MIT dual license but to my personal I like the clause in BSD-3 where it requires you can’t use my name to endorse things you build, that makes sense to me. I would relicense my projects one by one from now on.</p>
]]></summary>
</entry>
<entry>
    <title>go-pdqsort - Pattern Defeating Quicksort in Go</title>
    <link href="https://blog.paulme.ng/posts/2019-08-21-go-pdqsort---pattern-defeating-quicksort-in-go.html" />
    <id>https://blog.paulme.ng/posts/2019-08-21-go-pdqsort---pattern-defeating-quicksort-in-go.html</id>
    <published>2019-08-21T00:00:00Z</published>
    <updated>2019-08-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p><a href="https://github.com/MnO2/go-pdqsort">go-pdqsort</a> is my implementation of pattern defeating sort in golang. I knew about pattern defeating sort from rust’s standard library documentation. I’ve never heard about this algorithm and it immediately intrigues my interest. I googled about it and found the original <a href="https://github.com/orlp/pdqsort">implementation</a> and their discussion threads on <a href="https://news.ycombinator.com/item?id=14661659">hacker news</a> and <a href="https://www.reddit.com/r/cpp/comments/2z6hgx/patterndefeating_quicksort/">reddit</a>. Then I read the <a href="https://paperpile.com/view/c3820723-6d9c-0a86-ab02-8e46dc22aec5">technical report</a> from Orson Peters. The key observation from the algorithm is that merely reducing the miss rate from CPU branch prediction, it would make the sorting speed greatly improved (no need to flush the cache line etc), so the technique adopted was to put the index of the array in the buckets and don’t swap them immediately, but put them in the right bin and swap them in one batch at the end of the loop. This works perfectly in the language with zero cost abstraction like C/C++ and Rust. I wasn’t sure about that would work in the heap-managed languages like golang, python and ruby etc, since most of the things are on the heap (with pointer indirection) and often results into cache misses, the impact of branch prediction miss rate might be neglectable. (For sure it would depend on the escape analysis in golang compiler, it would put the things on stack if it doesn’t escape in general). Though my hunch was that it would be slower, it still a good practice to implement the algorithm by myself so that I would get to know the details of the algorithm. And here are the results</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">implementation</th>
<th style="text-align: center;">speed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">pdqsort</td>
<td style="text-align: center;">80874 ns/op</td>
</tr>
<tr class="even">
<td style="text-align: center;">std-lib sort</td>
<td style="text-align: center;">69828 ns/op</td>
</tr>
</tbody>
</table>
<p>I am not sure about what std-lib’s sort is, but it looks like introsort since it switches to heapsort when the recursion is too deep, and it incorporates the techniques from shellsort as well, but the main part is still Bently’s quicksort technique. You can tell from the result that pdqsort is significantly slower than std-lib’s sort, probably due to the overhead of those memory batch swap, where it triggers extra allocation or cache eviction, pretty much as expected.</p>
<p>If you are interested in the algorithm, you could check out the <a href="https://github.com/MnO2/go-pdqsort">code</a> by yourself.</p>
]]></summary>
</entry>
<entry>
    <title>Travel Planning with Trello</title>
    <link href="https://blog.paulme.ng/posts/2019-08-20-travel-planning-with-trello.html" />
    <id>https://blog.paulme.ng/posts/2019-08-20-travel-planning-with-trello.html</id>
    <published>2019-08-20T00:00:00Z</published>
    <updated>2019-08-20T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>As a frequent traveller who has been to 55+ countries/regions, I know that travel planning is a time consuming process. I enjoyed the process pretty much as it would get me to know the destination and the culture more. Being to the destination is only small part of it. However, planning it with the right tool definitely would make your life much easier.</p>
<p>I was quite used to put everything into one Google Doc. I have been using the template I created for myself all over these years. I would revise the template from the experience from the journey to make sure the template is thorough and it could handle 90% of the scenario. Whenever I am about to plan my next trip, I’ll just copy from my template and dump the materials and the links to the wikitravel and blog posts into the Google Doc and use it as a draft to plan my trip.</p>
<p>Until I find a significant better way to do it. It’s part of the movement I set for myself to make most of the things in my life to be managed by Trello. I would centralize the notification/update by integrating the crawler and Trello API. But soon I found that Trello is especially suitable for travel planning. This blog post is not sponsored by Trello but I whole-heartedly think so.</p>
<p>Travel planning is a classic type of tasks that has the following properties:</p>
<ol type="1">
<li>Start from a simple idea and expanded into details and converge to a final plan</li>
<li>The final outcome is a detailed itinerary with contingency plan.</li>
<li>Itinerary alone is not enough. Analytical reporting is required since you need budgeting.</li>
</ol>
<p>Therefore at least to me, a good software for travel planning need to have the following features:</p>
<ol type="1">
<li>A pin board to collect the materials you are gonna to read.</li>
<li>Easy to re-organize and label items to suit your need or your mind map looks like.</li>
<li>Rich media supports</li>
<li>It provides APIs so that you could export and analyze it with a proper tooling.</li>
<li>Synchronization and Offline access.</li>
</ol>
<p>And Trello just matches all of them to me as the time of Aug 2019, and this is how I use it.</p>
<p>At the brainstorming phase, I would install the Send-to-Trello button to my Firefox. Whenever I looked up a relevant blog post and I don’t have time to read it full, I would use Send-to-Trello button to save it later. Usually I would create a List in Trello and name it as <code>Lead</code>.</p>
<p><img src="/images/2019-08-20/send_to_trello.png" /></p>
<p>And I would read them and create the digested steps into Cards and organize them into days. This is an iterative process, I would first focus on the ballpark schedule so that I could put everything into the my scheduled numbers of days, and make sure the big transporation items are possible to tickets in the budget. In the later iterations, I would add more detailed steps including the ticket and transfer information etc, so that I could just follow the instructions when I am on the road. The Card design just makes this iterative process easy and intuitive.</p>
<p><img src="/images/2019-08-20/trello_cards.png" /></p>
<p>With the Adds-on, Trello could even show you the Map View of your cards, it’s especially useful when you are planning the places you would like to visit in the city. You could see the obvious clusters for the markers on the map, and put them in the same day.</p>
<p><img src="/images/2019-08-20/trello_map_view.png" /></p>
<p>Also I would leverage on the Custom Fields adds-on to create extra field to document the cost and duration of the transportation. These are for further analysis at the later phase.</p>
<p>After finishing the planning and make sure I was correctly labeling the cards. I would run a script against Trello API to generate the Google Sheets budget spreadsheet.</p>
<pre><code>#!/usr/bin/env ruby

require &quot;google_drive&quot;
require &#39;trello&#39;
require &#39;time&#39;
require &#39;json&#39;

spreadsheet_id = &#39;&lt;Your Spread Sheet ID&gt;&#39;
target_currency = &#39;TWD&#39;

config_file = File.read(&quot;config/trello.json&quot;)
config_for_trello = JSON.parse(config_file)

Trello.configure do |config|
  config.developer_public_key = config_for_trello[&#39;developer_public_key&#39;] 
  config.member_token = config_for_trello[&#39;member_token&#39;] 
end

cards = {}

Trello::Board.find(&#39;&lt;Your Board ID&gt;&#39;).lists.each do |list|
  Trello::List.find(list.id).cards.each do |card|
    cards[card.id] = { :name =&gt; card.name }
    card.custom_field_items.each do |item|
      pp item.custom_field_id
      if item.custom_field_id == &#39;5d5978f014ab2f17fcb6a2cd&#39;
        if not item.value[&#39;number&#39;].nil?
          cards[item.model_id][:cost] = item.value[&#39;number&#39;]
        end
      elsif item.custom_field_id == &#39;5d59795638dd318d1a53471a&#39;
        if not item.option_value().nil?
          cards[item.model_id][:currency] = item.option_value()[&#39;text&#39;]
        end
      end
    end
  end
end

pp cards

row = 1
session = GoogleDrive::Session.from_config(&quot;config/google.json&quot;)
ws = session.spreadsheet_by_key(spreadsheet_id).worksheets[0]

cards.each do |card_id, card|
  if not card[:cost].nil? and not card[:currency].nil?
    ws[row, 1] = card[:name]
    ws[row, 2] = card[:cost]
    ws[row, 3] = card[:currency]
    ws[row, 4] = target_currency

    row += 1
  end
end
ws.save</code></pre>
<p>And here is what it looks like in Google Sheets.</p>
<p><img src="/images/2019-08-20/budget_table.png" /></p>
<p>I used it to plan my trip to Kyoto in the upcoming month, and it is so much better than what I was doing by Google Doc, reducing the big share of time to do copy pasting and adjusting the format. I would use this method to plan my trip in the future.</p>
]]></summary>
</entry>
<entry>
    <title>logq - Analyzing log files in SQL with command-line toolkit, implemented in Rust</title>
    <link href="https://blog.paulme.ng/posts/2019-08-16-logq---analyzing-log-files-in-sql-with-command-line-toolkit%2C-implemented-in-rust.html" />
    <id>https://blog.paulme.ng/posts/2019-08-16-logq---analyzing-log-files-in-sql-with-command-line-toolkit%2C-implemented-in-rust.html</id>
    <published>2019-08-16T00:00:00Z</published>
    <updated>2019-08-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p><a href="https://github.com/MnO2/logq">logq</a> is my latest project and it has reached a reasonably qualitative milestone for me to comfortably introduce it and share about its technical detail.</p>
<p>Let’s start by an example. Imagine that you are in the middle of troubleshooting a production incident and you are suspecting a certain endpoint from the web-server is having problem and result into high latency. Since it is application level and it is not provided by the AWS CloudWatch. It is also the first time that it happened so there isn’t any in-house datadog or application level instarumentation setup. And it occurs to you that the access log contains the relevant information and it would be possible for you to calculate the metrics from the log. You download the log from the archive storage and piece together an ad-hoc script in 30 minutes and run the metrics against the log, and the script is implemented in python it gets to pretty slow if the log size is large. Wouldn’t it be great if there were command line where you could handle these kind of ad-hoc analysis situation easily? Where no extra dependency setup like ELK or library is needed. That is the motivation to drive to the development of <a href="https://github.com/MnO2/logq">logq</a>, where you could answer the question of “What are the 95th latencies with 5 seconds time frame against application handlers by ignoring the second path segments” easily.</p>
<pre><code>&gt; logq query &#39;select time_bucket(&quot;5 seconds&quot;, timestamp) as t, url_path_bucket(request, 1, &quot;_&quot;) as r, percentile_disc(0.95) within group (order by sent_bytes asc) as bps from elb group by t, r&#39; data/AWSLogs.log
+----------------------------+----------------------------------------------------------------------+----------+
| 2019-06-07 18:45:30 +00:00 | /img/_/300/2r0/54558148eab71c6c2517f1d9.jpg                          | 0.046108 |
+----------------------------+----------------------------------------------------------------------+----------+
| 2019-06-07 18:45:30 +00:00 | /img/_/300/2r0/546db9fd11bcd3c15f0a4ada.jpg                          | 0.073435 |
+----------------------------+----------------------------------------------------------------------+----------+
| 2019-06-07 18:45:30 +00:00 | /img/_/bound/2r0/559153d381974f0e5289a5e4.webm                       | 0.055385 |
+----------------------------+----------------------------------------------------------------------+----------+
| 2019-06-07 18:45:30 +00:00 | /api/_/conversation/5590921f9d37a84e20286e3e                         | 0.04181  |
+----------------------------+----------------------------------------------------------------------+----------+
| 2019-06-07 18:45:35 +00:00 | /img/_/bound/2r0/551c6e78a107308d47055c96.webp                       | 0.063701 |
+----------------------------+----------------------------------------------------------------------+----------+
...</code></pre>
<p><a href="https://github.com/MnO2/logq">logq</a> inspired by my own need at daily work when troubleshooting production incident. I noticed that there are a few drawbacks by using glueing scripts to analyze the log</p>
<ol type="1">
<li>You spend a lot of time to parse of the log format, but not focus on calculating the metrics helping to troubleshoot your production issues.</li>
<li>Most of the log formats are commonly seen and we should ideally abstract it and have every one benefit from the shared abstraction</li>
<li>For web-server log cases, the log volume usually is huge, it could be several hundred MB or even a few GB. Doing it in scripting langauges would make yourself impatiently waiting it is running at your local.</li>
</ol>
<p>For sure you could finely tuned the analytical tooling like AWS Athena or ELK to analyze the very large volume of logs in tens of or hundreds of gigabytes, but often times you just want to ad-hocly analyze logs and don’t bother to set things up and cost extra money. Also, the modern laptop/PC is actually powerful enough to analyze gigabytes of log volumes, just that the implementation is not efficient enough for doing that. Implementing <a href="https://github.com/MnO2/logq">logq</a> in Rust is in hope to resolve those inconvenience and concerns.</p>
<p>To check out more query examples, please check out the <a href="https://github.com/MnO2/logq/blob/master/README.md">README</a> it has the commonly used queries (at least to the author).</p>
<p>The rest part of the blog post would be focus on the technical detail I faced when I implemented it in Rust.</p>
<h2 id="using-nom-to-parse-the-sql-syntax">Using nom to parse the SQL syntax</h2>
<p>When I was evaluating the parsing approach, I was considering among <a href="https://github.com/Geal/nom">nom</a>, <a href="https://github.com/Marwes/combine">combine</a>, <a href="https://github.com/pest-parser/pest">pest</a> and rolling my own lexer / parser. Initially I was rolling my own lexer / parser since it is the approach what most of the modern industrial strength compilers would do to cope with the incremental parsing and better error message etc. However I found it too laborious to do it at the prototype phase. I turned to look for a parser combinator library instead soon. I wasn’t considering <a href="https://github.com/pest-parser/pest">pest</a> since I am not familiar with PEG and I had the impression where the parser generated from generator are hard to fine tuned for better error message based on my experience of using lex/yacc. Therefore the only options left are <a href="https://github.com/Geal/nom">nom</a> and <a href="https://github.com/Marwes/combine">combine</a>. I was intimidated by the macro approach adopted by <a href="https://github.com/Geal/nom">nom</a> (before 5). I read the blog post I could understand it is due to the limitation in the compiler but it’s just looks like a horse hard to harness if anything goes wrong due to macro system. I feel a pity not be able to adopt a library where it is fine tuned for performance. Just when I was about to turn to <a href="https://github.com/Marwes/combine">combine</a>, I found that starting from <a href="https://github.com/Geal/nom">nom</a> version 5, it is no longer required to use macro approach. The only drawback is that the documentation for nom 5’s complete new approach is still lacking, but it looks like I could quickly grasp its concept since I had experience of using <a href="http://hackage.haskell.org/package/parsec">Parsec</a> in Haskell. And in the end it is the library I decided to stick with. The type like <code>IResult</code> and its type parameters are easy to understand, and most of the combinators are battery included in the library, but it still took me some time to figure it out when the thing I would like to do is not included.</p>
<h3 id="parsing-identifier">Parsing identifier</h3>
<p>It wasn’t so clear on how to express the rules of a valid identifier</p>
<ol type="1">
<li>It consists of underscore, alphabet, digit</li>
<li>It should not start with number</li>
<li>It shouldn’t be all underscore</li>
</ol>
<p>After checking out the <a href="https://github.com/Geal/nom/blob/master/examples/s_expression.rs#L74">example</a> provided by the nom repository, I found that you could actually do it in procedural, pretty much you would do if you roll your own parser with <code>Result</code> as the returning type. In nom’s case you just need to rely on the already provided <code>ParseError</code> and <code>split_at_position1_complete</code> instead. By providing helping function working on the character level then it is easy to do that.</p>
<pre><code>fn identifier&lt;&#39;a, E: nom::error::ParseError&lt;&amp;&#39;a str&gt;&gt;(input: &amp;&#39;a str) -&gt; IResult&lt;&amp;&#39;a str, &amp;&#39;a str, E&gt;
where
{
    fn is_alphanumeric_or_underscore(chr: char) -&gt; bool {
        chr.is_alphanumeric() || chr == &#39;_&#39;
    }

    fn start_with_number(s: &amp;str) -&gt; bool {
        if let Some(c) = s.chars().next() {
            &quot;0123456789&quot;.contains(c)
        } else {
            false
        }
    }

    fn all_underscore(s: &amp;str) -&gt; bool {
        for c in s.chars() {
            if c != &#39;_&#39; {
                return false;
            }
        }

        true
    }

    let result = input.split_at_position1_complete(
        |item| !is_alphanumeric_or_underscore(item.as_char()),
        nom::error::ErrorKind::Alpha,
    );

    match result {
        Ok((i, o)) =&gt; {
            if start_with_number(o) || all_underscore(o) || KEYWORDS.contains(&amp;o) {
                Err(nom::Err::Failure(nom::error::ParseError::from_error_kind(
                    i,
                    nom::error::ErrorKind::Alpha,
                )))
            } else {
                Ok((i, o))
            }
        }
        Err(e) =&gt; Err(e),
    }
}</code></pre>
<h3 id="precedence-climbing-expression-parsing">Precedence climbing expression parsing</h3>
<p>Traditionally to parse an expression with precedence, you either encode it in the grammar or use bottom-up approach. It is a long resolved problem and the early day approach is Dijkstra’s Shunting Yard algorithm. However, most of the manually implemented modern industrial strength compiler use recursively descent approach, which is top down. One approach is by switching to the bottom-up approach when parsing to a point where operator precedence parser is needed. I think it is also possible to do that with parser combinator approach but we need to treat an operator precedence parser as a standalone black box and it doesn’t combine elegantly with the rest of the code. And here is the point where I would like to introduce Precedence climbing algorithm. It just combine so elegantly with the parser combinator approach, since it is a top down approach it just looks seamlessly with the rest of the combinators’ code. If you are not familiar with Precedence climbing, Eli Bendersky has a very good <a href="https://eli.thegreenplace.net/2012/08/02/parsing-expressions-by-precedence-climbing">introduction</a>.</p>
<p>In just a few short lines of code and the whole precedence parsing problem is resolved.</p>
<pre><code>fn parse_expression_at_precedence&lt;&#39;a&gt;(
    i0: &amp;&#39;a str,
    current_precedence: u32,
    precedence_table: &amp;HashMap&lt;String, (u32, bool)&gt;,
) -&gt; IResult&lt;&amp;&#39;a str, ast::Expression, VerboseError&lt;&amp;&#39;a str&gt;&gt; {
    let (mut i1, mut expr) = parse_expression_atom(i0)?;
    while let Ok((i2, op)) = parse_expression_op(i1) {
        let (op_precedence, op_left_associative) = *precedence_table.get(op).unwrap();

        if op_precedence &lt; current_precedence {
            break;
        }

        let (i3, b) = if op_left_associative {
            parse_expression_at_precedence(i2, op_precedence + 1, precedence_table)?
        } else {
            parse_expression_at_precedence(i2, op_precedence, precedence_table)?
        };

        let op = ast::BinaryOperator::from_str(op).unwrap();
        expr = ast::Expression::BinaryOperator(op, Box::new(expr), Box::new(b));

        i1 = i3;
    }

    Ok((i1, expr))
}</code></pre>
<p>It also worths to mention that I was actually got lazy to encode the precedence in the grammar when I was doing the syntax part in the first iteration and leave the precedence parsing to later to resolve. It is a major refactoring when the project need to shift the approach when I was affirmed that the precedence climbing is the way to go. Without Rust’s type system’s help, it is a hard take. I only needed to follow the compiler’s complaint and clear out all of the warnings/errors then the task is pretty much done. I simply could not imagine how I could do that in a dynamic-typed programming language.</p>
<h2 id="failure-crate">Failure crate</h2>
<p>I would like to briefly talk about the <code>failure</code> crate as well. I have had experience of using <code>failure</code> crate in some of the mini projects. At the time I was feeling that it is quite convenient that you could derive everything by putting the label on the Error struct. In this larger project, with the help of <code>impl From</code> and <code>#[cause]</code> label, and <code>?</code> operator. I have the strong feeling that it help shape your structuring of error handling into monadic style. <code>impl From</code> plays the role kind of like monad-transformer where lift the application from one kind of error monad to another kind, but without having you to stick your head around complicated type combination but it could influence you to put down the error handling in the similar structure.</p>
<h2 id="compiling-sql-to-graph">Compiling SQL to Graph</h2>
<p>Then here comes the core part. Writing a compiler is all about parsing and manipulating on graphs. For translating SQL into computation graph is relatively elegant than translating other procedural programming languages, since if you think it thoroughly, it is translating into the the structure that is pretty functional.</p>
<p>For a simple select statement, like “select a from foo”. It is actually could be translated into a term you are familiar in functional world. That is <code>map</code>. Suppose that you <code>foo</code> table has its schema defined as (“a”, “b”, “c”). Then “select a” is just saying, please project to “a” from (“a”, “b”, “c”), or you could say it a synonym to <code>map</code>.</p>
<p>For the <code>from</code> clause, it is just saying a data source where you could stream the data record from, it could be a file name or any other data source, either physically or virtually referred to.</p>
<p><code>where</code> clause is a no brainer, it is just interpreting a <code>filter</code> against the projected data records from <code>map</code>. The expressions to evaluated against are provided to <code>where</code> in the form of <code>a = 1</code> etc.</p>
<p>The relatively complicated is <code>group by</code>, but it is simply just doing a <code>fold</code> against a key-value data structure, with the key as the distinct values specified by the fields in the <code>group by</code> clause, and the rest of the columns in the values, or sent to the “aggregator”. If you are familiar with Haskell you know that HashMap is <a href="http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Foldable.html#t:Foldable">foldable</a>, group by is basically a fold against HashMap if it is the backing storage. In other languages like ruby, <code>reduce</code> is probably the method used in this kind of action instead.</p>
<p>So to sum up, the whole SQL translation is basically just parse the abstract the syntax tree into a map-reduce chain, streaming the source data from the upstream and produce it to the downstream. in the main stream language this type of structures are called <code>iterator</code>, so it could be seen as a series of iterators chained together as well.</p>
<p>For sure there are details where you would like to rewrite the whole graph to remove the const expression etc, or push down the complicated logic into the data source if the data source supports complicated compute logic (in practice they are smart key-value store supports transaction and cursor). But the spirit of the whole compilation is pretty much as the above mentioned.</p>
<h2 id="t-digest">T-Digest</h2>
<p>In <a href="https://github.com/MnO2/logq">logq</a>, the aggregate function of <code>percentile_disc</code> is supported since it is a common need to calculate the 99th percentile of latency in analyzing web traffic. However, to calculate the exact percentile you have to sort the sequence, which is a very expensive operation if the log file size is huge. <code>percentile_disc</code> in logq is compiled to group by with sorting in each of the partition, and it would remember every elements so that it is able to sort.</p>
<p>In order to make it more efficient in terms of the memory footprint and to support the parallel aggregation feature on the roadmap, I implemented <code>approx_percentile</code> aggregate function by T-Digest algorithm. T-Digest is an interesting data sketching algorithm. It leverages on the “scaling function” to make the both ends of the percentiles (for example, 1th percentile or 99th percentile) very accurate and the percentile in the middle less accurate. It matches the use cases we often need. The core concept behind is still by binning the data points, just that it uses scaling function to control the binning process, it makes the binning on both ends more granular so that we could keeps the info as the original as possible and the binning in the middle more blunt (larger binning size). And the choice of the scaling function is pretty flexible, the only requirements are that it has to be non-decreasing, and its derivative starts at 0 and ends at 1. My implementation is based on what facebook’s folly provided. In the original paper and many reference implementation it is using <code>arcsin</code>, but in folly it is using sqrt since it more computation efficient, and practically not much statistical impact.</p>
<p>I released the T-Digest part as a separate <a href="https://crates.io/crates/tdigest">tdigest</a> crate. It is not fully tested with generated statistical data, but it should be reasonably correct since I am following folly’s implementation.</p>
<h2 id="post-words">Post Words</h2>
<p>This is the first non-trivial project implemented in Rust, the line reported by <code>cloc</code> on tag <code>v0.1.4</code> is around 4800 lines of Rust.</p>
<pre><code>github.com/AlDanial/cloc v 1.82  T=0.05 s (305.6 files/s, 120809.5 lines/s)
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Rust                            13            651             17           4835
YAML                             1              0              0             32
-------------------------------------------------------------------------------
SUM:                            14            651             17           4867
-------------------------------------------------------------------------------</code></pre>
<p>To do major refactoring on this size of code usually it is a pain but at least so far I can still feel comfortable to do any major changes without breaking too much things.</p>
<p>There are plenty of features I plan to put on the roadmap, performance optimization is definitely on the top of the list since I didn’t consider too much performance when I coded the proof of concept. There are plenty of room to speed up. Other things on the roadmap are</p>
<ul>
<li>Conforms to the much more complicated SQL syntax as sqlite</li>
<li>Performance optimization, avoid unnecessary parsing</li>
<li>More supported functions</li>
<li>time_bucket with arbitrary interval (begin from epoch)</li>
<li>Window Function</li>
<li>Implementing approximate_percentile_disc with t-digest algorithm when the input is large.</li>
<li>Streaming mode to work with tail -f</li>
<li>Customizable Reader, to follow GoAccess’s style</li>
<li>More supported log format</li>
<li>Plugin quickjs for user-defined functions</li>
</ul>
<p>Hope I can constantly have spare time to work on this many things on the roadmap.</p>
]]></summary>
</entry>
<entry>
    <title>「ProtonMail」に移行して、「Gmail」を辞めます</title>
    <link href="https://blog.paulme.ng/posts/2019-07-30-%E3%80%8Cprotonmail%E3%80%8D%E3%81%AB%E7%A7%BB%E8%A1%8C%E3%81%97%E3%81%A6%E3%80%81%E3%80%8Cgmail%E3%80%8D%E3%82%92%E8%BE%9E%E3%82%81%E3%81%BE%E3%81%99.html" />
    <id>https://blog.paulme.ng/posts/2019-07-30-%E3%80%8Cprotonmail%E3%80%8D%E3%81%AB%E7%A7%BB%E8%A1%8C%E3%81%97%E3%81%A6%E3%80%81%E3%80%8Cgmail%E3%80%8D%E3%82%92%E8%BE%9E%E3%82%81%E3%81%BE%E3%81%99.html</id>
    <published>2019-07-30T00:00:00Z</published>
    <updated>2019-07-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Googleが想像以上にいろんな情報を取得した。何か調べ物がある時はGoogleで検索し、ブラウザはChromeを使って、メールはGmailで送信し、どこかに行く時Googleマープで所在地を確認し、YouTubeの動画を見て、Googleドライブに入れて、仕事はGoogle Docsを使いて。すべての閲覧履歴をGoogleが保存します。インターネット生活のほぼ全てをGoogleに絡め取られているの事は、良いことではない。そのため、Googleの依存度を減らしたい。</p>
<p>最初の辞めるのサービスは Gmail。メールは全ての情報は含まれている、クレジットカードでも、応募の情報でも、旅行情報でも、メールサービスは自分の情報を取得している、それが飛行機のチケット予約すると、Googleがカレンダーに予約を作成できるの理由。</p>
<h2 id="protonmail-の特徴">ProtonMail の特徴</h2>
<p>ProtonMailのメールデータは全て暗号化された状態でサーバー上に保存されています。データはすべてのステップで暗号化されていることから傍受のリスクはないとのこと。もっと知りたいたら、<a href="https://www.ted.com/talks/andy_yen_think_your_email_s_private_think_again?language=en">Andy Yen: Think your email’s private? Think again</a>の TED トークの動画を見てください。</p>
<h2 id="購入後1ヶ月間使用した感想">購入後1ヶ月間、使用した感想</h2>
<p>移入が大体上手くいく、ProtonMail はカスタムドメインをセットアップすることがあります。paulme.ng のドメインをProtonMailに移行しました。スパムフィルタが ProtonMail より Gmail の方がうまい。ProtonMailのフィルタはが手動セットアップあとで、メールの分類正しいになりました。暗号化と便利のトレード・オフがと思います。暗号化したら、情報を取得できないだから、訓練データになし、機械学習的手法を使っていない、スパムメッセージの分類器を作っていこどができません。それ以外は、満足しています。</p>
]]></summary>
</entry>

</feed>
