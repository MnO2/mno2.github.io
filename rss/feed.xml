<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Paul Meng's Blog</title>
    <link href="https://blog.paulme.ng/rss/feed.xml" rel="self" />
    <link href="https://blog.paulme.ng" />
    <id>https://blog.paulme.ng/rss/feed.xml</id>
    <author>
        <name>Paul Meng</name>
        <email>me@paulme.ng</email>
    </author>
    <updated>2019-07-28T00:00:00Z</updated>
    <entry>
    <title>Managing references with Paperpile</title>
    <link href="https://blog.paulme.ng/posts/2019-07-28-managing-references-with-paperpile.html" />
    <id>https://blog.paulme.ng/posts/2019-07-28-managing-references-with-paperpile.html</id>
    <published>2019-07-28T00:00:00Z</published>
    <updated>2019-07-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>For a long time I have the problem of managing papers and slides I am reading. It’s not only for the papers and slides from computer science, but also the financial reports and the letters to shareholder for my investment readings and researches. Over time I just downloaded a bunch of PDFs to my laptop and they scattered around in my Download folder without a systematic naming and indexing. It makes it hard to clean up the disk space for my laptop since I am not sure which one is what I needed if not by opening them up and review them one by one. It makes thing painful when you are backing up your system and migrating to the other. A couple of weeks back I found that I need to resolve this problem.</p>
<p>An ideal reference management software to me in 2019 should provide the following features.</p>
<ol type="1">
<li>To be able to store in the cloud so that you are able to access it everywhere. It would be great to let you choose among the mainstream cloud storage providers, and provide options to make some of the papers accessible offline, but not sync everything to local.</li>
<li>Good metadata auto completion. Not everything has a standard to label it or a good database to cross match and automatically label them, but it should be good for the parts that we are able to do it.</li>
<li>Cross platform and reasonably performant.</li>
<li>Providing API so that you could customise it for personal needs.</li>
</ol>
<p>I know the famous softwares like Zotero, Mendeley and Endnote. They have existed for a long time, but all of them feel more focus for scientific editing. I played with them for a little bit and Zotero maybe the closest but it still feels quite the exact thing I want. Until I noticed a new upcoming competitor: Paperpile. They seem to launch for some years but not that widely well-known. I tried it out and it gives me the feeling it is the closest reference management software I would like. I have lots of PDFs and my main purpose is not for scientific editing, I just need a software to edit it and help me rename them, and easy to look things up. It is a big-bonus to me that Paperpile is based on Google Drive. It resolves the issue 1, 3, 4 by leveraging on Google Drive as the backing storage. With Google Drive you could choose which file to stay offline and providing Google Drive API to let you trigger the hook, for example, to send a new card to my Trello when a new paper is added. The downside for sure is vendor lock-in, but it’s less a priority comparing to solving my need.</p>
<p>I have tried it out for 3 weeks and I can say it is the software that’s closest to my ideal, though not perfect yet. I would stick to it before anything even better come up.</p>
]]></summary>
</entry>
<entry>
    <title>cedarwood: Efficiently-Updatable Double Array Trie in Rust</title>
    <link href="https://blog.paulme.ng/posts/2019-07-14-cedarwood%3A-efficiently-updatable-double-array-trie-in-rust.html" />
    <id>https://blog.paulme.ng/posts/2019-07-14-cedarwood%3A-efficiently-updatable-double-array-trie-in-rust.html</id>
    <published>2019-07-14T00:00:00Z</published>
    <updated>2019-07-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p><a href="https://github.com/MnO2/cedarwood">Cedarwood</a> is an effort to <a href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html">speed up jieba-rs</a>, an efficient implementation of trie is needed in order to satisfying the following needs.</p>
<ul>
<li>To be able to list the prefixes that exist in the dictionary with a given string. For example, given the dictionary to be <code>["a", "ab", "abc", "z", "xz", "xy"]</code> and the input string <code>abcdefg</code>. We should be able to efficiently list the prefixes <code>["a", "ab", "abc"]</code> that exist in the dictionary.</li>
<li>Due to the support of dictionary provided dynamically from the user, we need to support dynamic insertion into the data structure and at the same time still get us efficient common prefixes operation.</li>
<li>To be able to efficiently tell if the given string is in the dictionary or not.</li>
</ul>
<p>As mentioned <a href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html">previously</a> that aho-corasick performs quite slow from my brief testing, I didn’t spend extra time to dig out why and I just accepted it. My focus was turned to refine the double array trie implementation, where at the time was <a href="https://github.com/andelf/rust-darts">rust-darts</a>. It performs quite well on the reading operations but due to its implementation was based on the <a href="http://chasen.org/~taku/software/darts/">darts</a>, a C++ implementation of static double array trie. It failed the requirement 2 mentioned above. Either we have to drop the API and getting farther away from the original python’s implementation of Jieba, or we have to find alternatives to support dynamic insertion. I did contribute to the <code>rust-darts</code> with <a href="https://github.com/andelf/rust-darts/pull/25">dynamic insertion</a>, it resolves the feature lacking issue but the speed is still quite slow. It would make the loading time and testing time for jieba to be slow as well. Then I implemented the techniques mentioned <a href="https://linux.thai.net/~thep/datrie/#Alloc">here</a> to make the trie building time much faster (from 30+ seconds down to around 5 seconds on my machine). It is much much better but I still think if we can do any better. To be fair to the <code>darts</code> implementation, the vanilla double array trie was invented by Aoe in the 1989 paper: <a href="https://ieeexplore.ieee.org/document/17222">An efficient digital search algorithm by using a double-array structure</a> with mostly read operation in mind. It did support the update operations but it also said the update speed is quite slow. There are a few techniques to speed the update operations up and I already implemented <a href="https://linux.thai.net/~thep/datrie/#Alloc">one of them</a> for the clean build.</p>
<h2 id="look-for-better-solutions">Look for better solutions</h2>
<p>I googled around to see if there are any better solution, then I found <a href="https://en.wikipedia.org/wiki/HAT-trie">HAT Trie</a>, which seems to be the state of the art implementation of the trie, with the cache in consideration. It is based on the paper published by Askitis Nikolas and Sinha Ranjan: <code>HAT-trie: A Cache-conscious Trie-based Data Structure for Strings.</code>. And it is good that <a href="https://github.com/Tessil">Tessil</a> already had <a href="https://github.com/Tessil/hat-trie">C++ implementation</a> on github with <a href="https://github.com/Tessil/hat-trie/blob/master/README.md">very detailed benchmark comparisons</a>. However, when I looked at the interface provided it seems like it only supports finding the longest prefix, and iterating through the string predictions in the dictionary matching a given prefix. It doesn’t provide the interface for the requirement 1. I looked into the <a href="https://github.com/Tessil/hat-trie/blob/master/include/tsl/htrie_hash.h#L1574">code</a> and it seems that it could potentially exposed from the implementation to support that, just that I am not familiar enough of the algorithm to do that. The concept of HAT-trie is also based on hash table with the concept of “burst” is unfamiliar to me, and since it is stored in unordered, my hunch stopped me from investing my time further to read the not-so-simple algorithm. Though the hindsight now is that the operation I need is not based on the requirement of the order in keys but the traversal of the trie, which should be supported by the longest prefix operation, just need to change the implementation. I would do that if I have more time.</p>
<h2 id="cedar-and-cedarwood">Cedar and Cedarwood</h2>
<p>The main character in this post is actually one of the candidates listed in the benchmark provided by Tessil. An double array trie implementation named <a href="http://www.tkl.iis.u-tokyo.ac.jp/~ynaga/cedar/">cedar</a> caught my eyes, since its update seems to be very efficient, it is comparable with the other fast implementations in the list. And it’s read access speed is also on the same ballpark with HAT-trie in the Dr. Askitis dataset, where the median key length is shorter than what is in the wikipedia title dataset. It definitely worths a closer look to me. The read access speed seems to be slower than HAT-trie in the long key length cases but for the Chinese segmentation scenario, the shorter key seems to be the case. We are not working on a query completion use case anyway. The author of cedar is <a href="http://www.tkl.iis.u-tokyo.ac.jp/~ynaga/">Naoki Yoshinaga</a>. He pointed us to the paper of <code>A Self-adaptive Classifier for Efficient Text-stream Processing</code> for further reading on his webpage but it is mostly an NLP paper but not a data structure paper, it only has one or two paragraphs describing the working on his improvement on double array trie. In the paper he cited another one, but you couldn’t find the paper on any English website, even scihub. It turned out it is a paper in Japanese and I found the pdf on the Japanese website with the title of <code>タブル配列による動的辞書の構成と評価</code>. Even though I can understand basic to intermediate Japanese, it still didn’t address that much into the detail in that Japanese paper. Therefore I decided to read the code directly.</p>
<p>It is a header-only C++ implementation and the code is very much shortened to reduce the header size, it is generally ok to read with a C++ formatter from IDE or VSCode, but it also took me a while to get the core concept behind it on improvement techniques due to the lack of comments. And with the <code>cedarwood</code> rust porting implementation, I added much more comments so it should be much easier to understand how it is working, but it is a required reading to read the original double array trie paper so that at least you know how the <code>base</code> and <code>check</code> works in double array trie cases. The following I’ll briefly talk about the concept behind the skills.</p>
<h2 id="the-key-concepts-in-the-algorithm">The Key Concepts in the Algorithm</h2>
<p>The inefficiency in the update operation of the vanilla double array trie is caused by the free slot searching. The original paper simply implies that you could use brute-force approach to iterate through the index in <code>check</code> and see if they are marked as owned, and iterate through until the location where you have the free slots distribution exactly match what you want (suppose you are relocating an existing trie node to a new free location). The technique specified <a href="https://linux.thai.net/~thep/datrie/#Alloc">here</a> is basically leveraging on the value space on each block, and you can use negative integer to specify the free slot location and make them into an array-index based doubly linked-list, and you could make the brute forte iteration down to a fast-skipping linked list traversal. However, that technique doesn’t address the case where you still need to iterate your char set (2^21 in the unicode scalar case, or 2^8 if you are working on UTF-8) to check every slot and make sure the slots distributions match your need. The technique used in cedar is basically to address this problem, by maintain the bookkeeping of two kind of information: <code>NInfo</code> and <code>Block</code></p>
<pre><code>struct NInfo {
    sibling: u8, // the index of right sibling, it is 0 if it doesn&#39;t have a sibling.
    child: u8,   // the index of the first child
}</code></pre>
<pre><code>struct Block {
    prev: i32,   // previous block&#39;s index, 3 bytes width
    next: i32,   // next block&#39;s index, 3 bytes width
    num: i16,    // the number of slots that is free, the range is 0-256
    reject: i16, // a heuristic number to make the search for free space faster, it is the minimum number of iteration in each trie node it has to try before we can conclude that we can reject this block. If the number of kids for the block we are looking for is less than this number then this block is worthy of searching.
    trial: i32,  // the number of times this block has been probed by `find_places` for the free block.
    e_head: i32, // the index of the first empty elemenet in this block
}</code></pre>
<p>The <code>NInfo</code> is probably standing for “Trie Node Information”, it maintains the trie parent-children and siblings information, since these are the information needed when relocating the trie node around. You have to know which node has smaller size in its children, you could just traverse down and counting the number of children one by one. And you could iterate through the sibling chain when you really need to do the move and compare if the slots fit the potential free spaces.</p>
<p>As for <code>Block</code>, it maintains the book-keeping of the free space selection, simulating how you look at it of the each block (256 in size) when you squint at the data structure. It book keeps the information like how many free slots this block still have so that you could quickly prune the branch if the number of the free slots is less than the number of children the trie node you are relocating. Further more, the algorithm categorize the blocks into three kinds</p>
<ul>
<li>Full: The block where all of the slots are taken</li>
<li>Closed: The block where all of the slots are taken except for one</li>
<li>Open: The rest, but most of the time it is the free slots that just allocated at the end of the <code>base</code> and <code>check</code> when you resize the array.</li>
</ul>
<p>Each of them is put in the doubly-linked-list of their own kind. During the search process, you only need to look for the free space from <code>Closed</code> type block if you are inserting a single-child node, since it only needs one slot. You only need to look for <code>Open</code> block when you are relocating a node with more children. And since it is linked list all you need to do is just insert and remove the block from the linked-list, which is constant time. And insert / remove from the right kind of linked list after you update the node with inserted <code>label</code>.</p>
<h2 id="benchmarks">Benchmarks</h2>
<p>With the above algorithm mentioned, let’s take a look of <code>cedarwood</code>’s benchmark, the rust implementation of <code>cedar</code>. My laptop’s spec is as follows:</p>
<pre><code>MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports)
2.5 GHz Intel Core i7
16 GB 2133 MHz LPDDR3</code></pre>
<p>And here is the benchmark against C++ version and <code>rust-darts</code></p>
<h3 id="build">Build</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">impl</th>
<th style="text-align: left;">time</th>
<th style="text-align: left;">version</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">cedar</td>
<td style="text-align: left;">71 ms</td>
<td style="text-align: left;">2014-06-24</td>
</tr>
<tr class="even">
<td style="text-align: left;">cedarwood</td>
<td style="text-align: left;">64 ms</td>
<td style="text-align: left;">0.4.1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">rust-darts</td>
<td style="text-align: left;">201 ms</td>
<td style="text-align: left;">b1a6813</td>
</tr>
</tbody>
</table>
<h3 id="query">Query</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">impl</th>
<th style="text-align: left;">time</th>
<th style="text-align: left;">version</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">cedar</td>
<td style="text-align: left;">10 ms</td>
<td style="text-align: left;">2014-06-24</td>
</tr>
<tr class="even">
<td style="text-align: left;">cedarwood</td>
<td style="text-align: left;">10 ms</td>
<td style="text-align: left;">0.4.1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">rust-darts</td>
<td style="text-align: left;">18 ms</td>
<td style="text-align: left;">b1a6813</td>
</tr>
</tbody>
</table>
<p>For the <code>rust-darts</code> it is significantly slower than both of <code>cedar</code> and <code>cedarwood</code>, and it has extra requirement on the construction of the dictionary to be that the dictionary has to be lexicographically sorted. For <code>cedar</code> building contains the time for file reading (with 65536 buffer size) so it actually even slightly faster, but the querying time contains memory-only operation. I am too lazy to change the benchmarking code since it is using no-copying techniques by moving the pointer until the next new line and it is troublesome for me to change the rust implementation to be the same way so that to compare apple to apple. The <code>cedar</code> code was compiled with <code>-O3</code> and rust’s code is compiled with <code>--release</code> and only measure the time for the operations in the memory. You can see that the build time, that is <code>update</code> are roughly the same for C++ and Rust. And in the query case the Rust version is slightly slower than C++ but basically comparable. I couldn’t find where I could further optimize the Rust version since the code for the lookup case is quite simple and not much space to optimize on the source code level without going to the <code>unsafe</code> resort. Not sure where I could tune to speed it up, please let me know if you identify the part I could do further.</p>
<h2 id="lesson-learned">Lesson Learned</h2>
<p>Rust’s implementation is as comparable as C++’s implementation without much effort, I don’t need to go with <code>unsafe</code> and keep the majority of the code compiler-checked for memory safety. There is only one place where I need to go by <code>unsafe</code> block due to linked-list update but not due to the performance. There are a few handy features in C++ that I would miss, like const generics to make the data structure parameterized on the type level, but overall it is not a big issue in the cedar case. Or template specialization where you could leverage on different implementations based on the type you specify. Since <code>cedarwood</code> only save the word id in normal mode. It might require that for the support of reduced-trie feature where the value is stored in place, but right now <code>cedarwood</code> only has limited support on that so it is no big issue. Overall it is a smooth experience on porting.</p>
]]></summary>
</entry>
<entry>
    <title>最佳化 jieba-rs 中文斷詞性能測試 (快于 cppjieba 33%)</title>
    <link href="https://blog.paulme.ng/posts/2019-07-01-%E6%9C%80%E4%BD%B3%E5%8C%96jieba-rs%E4%B8%AD%E6%96%87%E6%96%B7%E8%A9%9E%E6%80%A7%E8%83%BD%E6%B8%AC%E8%A9%A6%28%E5%BF%AB%E4%BA%8Ecppjieba-33%25%29.html" />
    <id>https://blog.paulme.ng/posts/2019-07-01-%E6%9C%80%E4%BD%B3%E5%8C%96jieba-rs%E4%B8%AD%E6%96%87%E6%96%B7%E8%A9%9E%E6%80%A7%E8%83%BD%E6%B8%AC%E8%A9%A6%28%E5%BF%AB%E4%BA%8Ecppjieba-33%25%29.html</id>
    <published>2019-07-01T00:00:00Z</published>
    <updated>2019-07-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>昨晚寫了一篇關於最佳化 <a href="https://github.com/messense/jieba-rs">jieba-rs</a> 英文的<a href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html">介紹</a>，但想說 jieba 的使用者多半還是在中文圈，對於宣傳來講 hacker news 跟 reddit 可能無法觸及到真正會使用的使用者，於是為了宣傳，也是為了讓 search engine 可以搜尋到，就來把性能的部分另外寫成中文的一篇。關於過程我就不再重新用中文再寫一次了，實在太累人了。有興趣的人可以閱讀<a href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html">英文版</a></p>
<p>測試機器的機器規格如下</p>
<pre><code>MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports)
2.5 GHz Intel Core i7
16 GB 2133 MHz LPDDR3</code></pre>
<p>測試過程仿照<a href="http://yanyiwu.com/work/2015/06/14/jieba-series-performance-test.html">結巴(Jieba)中文分詞系列性能評測</a>所描述，先一行一行讀取檔案圍城到一個陣列裡，然後循環 50 次對圍城每行文字作為一個句子進行斷詞。 分詞算法都是採用精確模式，也就是包含了 HMM 的部分。</p>
<p>耗時的資料如下，從高到低排序</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">實作</th>
<th style="text-align: center;">耗時</th>
<th style="text-align: center;">版本 .</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">cppjieba</td>
<td style="text-align: center;">6.219s</td>
<td style="text-align: center;">866d0e8</td>
</tr>
<tr class="even">
<td style="text-align: left;">jieba-rs (master)</td>
<td style="text-align: center;">4.330s</td>
<td style="text-align: center;">a198e44</td>
</tr>
<tr class="odd">
<td style="text-align: left;">jieba-rs (darts)</td>
<td style="text-align: center;">4.138s</td>
<td style="text-align: center;">ab2fbfe</td>
</tr>
</tbody>
</table>
<p>以上耗時都是計算斷詞過程的耗時，不包括字典載入的耗時。</p>
<p>這篇會著重於評測只是為了宣傳，並不想陷入語言之爭，這也是我英文版有寫主要是分享關於用 Rust 最佳化的經驗，也是為了我自己衡量可以在工作中多認真使用 Rust 為目的。</p>
]]></summary>
</entry>
<entry>
    <title>优化 jieba-rs 中文分词性能评测 (快于 cppjieba 33%)</title>
    <link href="https://blog.paulme.ng/posts/2019-07-01-%E4%BC%98%E5%8C%96-jieba-rs-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D-%E6%80%A7%E8%83%BD%E8%AF%84%E6%B5%8B%EF%BC%88%E5%BF%AB%E4%BA%8E-cppjieba-33percent%29.html" />
    <id>https://blog.paulme.ng/posts/2019-07-01-%E4%BC%98%E5%8C%96-jieba-rs-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D-%E6%80%A7%E8%83%BD%E8%AF%84%E6%B5%8B%EF%BC%88%E5%BF%AB%E4%BA%8E-cppjieba-33percent%29.html</id>
    <published>2019-07-01T00:00:00Z</published>
    <updated>2019-07-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>昨晚写了一篇关于优化 <a href="https://github.com/messense/jieba-rs">jieba-rs</a> 英文的<a href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html">介绍</a>，但想说 jieba 的使用者多半还是在中文圈，对于宣传来讲 hacker news 跟 reddit 可能无法触及到真正会使用的用户群，于是为了宣传，也是为了让 search engine 可以搜索到，就来把性能的部分另外写成中文的一篇。关于过程我就不再重新用中文再写一次了，实在太累人。有兴趣的人可以阅读<a href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html">英文版</a></p>
<p>测试机器的机器规格如下</p>
<pre><code>MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports)
2.5 GHz Intel Core i7
16 GB 2133 MHz LPDDR3</code></pre>
<p>测试过程仿照<a href="http://yanyiwu.com/work/2015/06/14/jieba-series-performance-test.html">结巴(Jieba)中文分词系列性能评测</a>所描述，先按行读取文本围城到一个数组里，然后循环 50 次对围城每行文字作为一个句子进行分词。 分词算法都是采用精确模式，也就是包含了 HMM 的部分。</p>
<p>耗时数据如下，从高到低排序</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">实作</th>
<th style="text-align: left;">耗时</th>
<th style="text-align: left;">版本 .</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">cppjieba</td>
<td style="text-align: left;">6.219s</td>
<td style="text-align: left;">866d0e8</td>
</tr>
<tr class="even">
<td style="text-align: left;">jieba-rs (master)</td>
<td style="text-align: left;">4.330s</td>
<td style="text-align: left;">a198e44</td>
</tr>
<tr class="odd">
<td style="text-align: left;">jieba-rs (darts)</td>
<td style="text-align: left;">4.138s</td>
<td style="text-align: left;">ab2fbfe</td>
</tr>
</tbody>
</table>
<p>以上耗时都是计算分词过程的耗时，不包括词典载入的耗时。</p>
<p>这篇会着重于评测只是为了宣传，并不想陷入语言之争，这也是我英文版有写主要是分享关于用 Rust 优化的经验，也是为了我自己衡量可以在工作中多认真使用 Rust 为目的。</p>
]]></summary>
</entry>
<entry>
    <title>Optimizing jieba-rs to be 33% faster than cppjieba</title>
    <link href="https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html" />
    <id>https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html</id>
    <published>2019-06-30T00:00:00Z</published>
    <updated>2019-06-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>To start this blog post with a disclaimer: I admit that it is a clickbait title since every benchmark is a lie in its own, and I have no intention to start another programming language flame war. This blog post is mainly to share my experience on taking an emerging programming language’s ecosystem seriously and evaluating it by working on a serious project, and see how far we can go in terms of performance and development experience.</p>
<p>The project I chose as mentioned in the title is <a href="https://github.com/messense/jieba-rs">jieba-rs</a>, the rust implementation of a popular Chinese word segmentation library: <a href="https://github.com/fxsjy/jieba">Jieba</a>. A quick background introduction about <a href="https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation">word segmentation</a> for the readers who don’t speak Chinese. Chinese and Japanese in their written scripts are only delimited up to the sentence but words are not delimited. For example, the following sentence could be literally translated to “Today’s weather very good” in the same order, but you have no idea where the word boundaries are.</p>
<pre><code>今天的天气真好</code></pre>
<p>After the word segmentation, the sentence could be broken up as follows (space delimited):</p>
<pre><code>今天(Today)  的(&#39;s) 天气(weather) 真(very) 好(good)</code></pre>
<p>It’s unlike English where spaces are added between words. This makes the natural language processing harder since most of the methodologies requires the granularity of words. Therefore the word segmentation is a core part when it comes to the tasks like search engine and language understanding.</p>
<p><a href="https://github.com/fxsjy/jieba">Jieba</a> is the original implementation and it is implemented in python. It became so popular in the wild not only due to it strikes a good trade-off between the segmentation result and the performance. It may not be the most state-of-the-art or the most accurate word segmentation library but it’s a very popular engine and got ported into different programming languages like <a href="https://github.com/wangbin/jiebago">go</a> and the bindings to the <a href="https://github.com/yanyiwu/cppjieba">cppjieba</a>. Rust as a language focus on zero-cost abstraction, naturally I would like to benchmark the rust version against cpp version.</p>
<p>About a couple of weeks back I found that <a href="https://github.com/messense/">messense</a> already had a good version of the rust implementation with the maximum probability approach and the viterbi decoding finished. The code is well-written and self explainable therefore I decided to contribute and collaborate with messense. Even though I started following rust’s development occasionally ever since it was in 0.4, I haven’t catched up the latest 2018 edition, I started the contribution by implementing the left-out keyword extraction features using TextRank and TFIDF to get myself familiar with the language. In the meantime, I was curious how performant the rust implementation is, then I run the weicheng executable in the example folder, which is the same benchmark test used in cppjieba, by running the word segmentation on the weicheng script for 50 times and times the running time. I wasn’t not satisfied with the result since the cppjieba runs about 5-6s on the following spec of Macbook.</p>
<pre><code>MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports)
2.5 GHz Intel Core i7
16 GB 2133 MHz LPDDR3</code></pre>
<p>However, jieba-rs at the time was running like 10s-ish. It was almost the double of the running time of cppjieba.</p>
<p>With a couple of weeks of effort, now as the time of this blog post on June 30th, 2019. The running time of the jieba-rs is now around 33% faster than cppjieba as shown in the following.</p>
<p>Branch <code>darts</code> with the commit: ab2fbfe</p>
<pre><code>➜  jieba-rs git:(darts) ./target/release/weicheng
4138ms</code></pre>
<p>Branch <code>master</code> with the commit: a198e44</p>
<pre><code>➜  jieba-rs git:(master) ./target/release/weicheng
4330ms</code></pre>
<p>Branch <code>master</code> of cppjieba with the commit: 866d0e8</p>
<pre><code>➜  build git:(master) ✗ ./load_test
process [100 %]
Cut: [6.219 seconds]time consumed.
process [100 %]
Extract: [0.679 seconds]time consumed.</code></pre>
<p>You can see that 4138ms / 6219ms is roughly 66.5% of the running time, with the double array trie implementation adopted (not yet merged into the master).</p>
<p>I would also like to mention the <a href="https://github.com/andelf/rust-darts">rust-darts</a> implementation finished by <a href="https://github.com/andelf/">andelf</a>. He helped a lot on updating the three-year-old library so that I could import the library into jieba-rs to try it out. It’s on my roadmap to use DARTS as the underlying data structure once the required <a href="https://github.com/andelf/rust-darts/issues/24">features</a> are implemented.</p>
<p>Now, I’d like move on to the key steps I did to reduce the running time as much as possible.</p>
<h3 id="identifying-the-bottleneck-with-instrument">Identifying the bottleneck with Instrument</h3>
<p>The first step of performance tuning is to measure it and identifying the critical sections so you could put your efforts at the slowest part. The write-up from <a href="https://github.com/siddontang">siddontang</a> about <a href="https://www.jianshu.com/p/a80010878def">optimizing TiKV on Mac</a> is very helpful. By recoding the stack traces of the weicheng binary, it is easy to tell that the program spends its time on two parts: Regex and Viterbi Decoding.</p>
<h3 id="regex">Regex</h3>
<p><a href="https://github.com/BurntSushi">BurntSushi</a>’s regex library is very performance in general since it’s executed in DFA. The <a href="https://github.com/rust-lang/regex/blob/master/PERFORMANCE.md">documentation</a> also marked it clear where you should be looking at when the performance is the consideration you have.</p>
<pre><code>Advice: Prefer in this order: is_match, find, captures.</code></pre>
<p>A few lines of <a href="https://github.com/messense/jieba-rs/commit/3d013211f3d76d00680f6f670c4f96b808f43571">changes</a> from using Captures to Matches reduce the running time by 2s.</p>
<h3 id="using-dynamic-programming-to-trace-the-best-path-in-viterbi-decoding">Using dynamic programming to trace the best path in Viterbi decoding</h3>
<p>I have speech processing and natural language processing background so I am quite familiar with hidden markov model training and viterbi decoding. It used to be just a midterm course assignment when I was the TA of the course. I am pretty sure the viterbi decoding and path reconstruction could be done in two two-dimension matrices. In the original python implementation it has done extra memory allocation by copying the best path so far, where actually you only need to track it by a prev matrix to remember the last state. <a href="https://github.com/messense/jieba-rs/commit/2d1418092f595a3799d2d90f7a314b6855898261">changing this</a> earned us another 2s of speedup.</p>
<h3 id="move-memory-allocation-out-of-the-loop">Move memory allocation out of the loop</h3>
<p>With the convenient Vec and BTreeMap provided in the standard library, we often forget there are actually expensive memory allocation hidden behind the abstraction. The word segmentation is basically a task where you are running a main loop to loop through the character stream to see if it could be broken up here. By allocating the memory outside the loop would obviously boost the performance, and with the handy resize you only need to reallocate when you need more memory.</p>
<h3 id="remove-allocation-as-much-as-possible">Remove allocation as much as possible</h3>
<p>Transient Vec storage is convenient and it could make the code more readable, but when it comes to the scale of milli-second, it would be better to rethink if you really need them. Some cases you only need to track the states by one or two variables.</p>
<h3 id="swisstable-is-damn-fast">SwissTable is damn fast</h3>
<p>Right now we are using HashMap to store the dictionary and all of the prefixes of those words on the master branch. We know the HashMap in the standard library now is the hashbrown implementation, where it is based on Google’s Swiss Table. You can watch the introduction talk Google Engineer gave at <a href="https://www.youtube.com/watch?v=ncHmEUmJZf4">CppCon 2017</a>, it’s very inspiring on the approaches. SIMD instructions like SSE was used and therefore it’s damn fast when you are using Intel’s CPU. Even with the naive iteration through the prefixes of the strings to construct the DAG, it’s still performant since the scalar is so small. A theoretical better algorithm but bigger scalar absorbed into the big-O could perform far worse than naive approaches with SwissTable.</p>
<h3 id="adopting-double-array-trie.">Adopting Double Array Trie.</h3>
<p>There has been <a href="http://www.hankcs.com/program/algorithm/double-array-trie-vs-aho-corasick-double-array-trie.html">researches</a> comparing aho-corasik and double array trie ’s speed with respective the maximum forward-matching word segmentation algorithm, and the conclusion was double array trie performs better in the case of Chinese word segmentation due to the data access pattern. I tried it out both and the conclusion matches, at least for the implementation we have as the time of June 2019.</p>
<p>For aho-corasick I was using the library implemented by BurntSushi, so I assumed it is the best in class. It performs much worse than DARTS, therefore I briefly looked into the code base, it seems that there are indirection on whether to use NFA or DFA when <a href="https://github.com/BurntSushi/aho-corasick/blob/master/src/ahocorasick.rs#L1001">the methods are called</a>, I am not sure this is the main reason resulting into the significant slow down though.</p>
<p>For DARTS implementation I am using the one ported by <a href="https://github.com/andelf/">andelf</a> from <a href="http://chasen.org/~taku/software/darts/">darts</a> in C++. Its code is much easier to read than libdatrie though lacking the features of dynamic insertion and deletion and tail compression. I am intended to add those features into the <a href="https://github.com/andelf/rust-darts">rust-darts</a>.</p>
<h2 id="lesson-learned-and-afterword">Lesson Learned and Afterword</h2>
<p>Over the course of performance tuning and development I am convinced that Rust ecosystem has its potential and it seems to get a lot momentum in 2019, with so many Rustcons host around the world. Its spectrum of libraries is still far from the richness and matureness where you can find in C++ and Python world, but the standard library and a few core libraries contributed by star developers are very high quality and you could learn a lot by merely reading their code base. And its affine type system makes you feel safe to do a major refactoring for the performance tuning etc since you know compiler would call you dumb and catch those errors. If you are careless about a few of the details, you could still easily spit out poorly performing code. However, it’s also easy for you to improve it once you gain some experience on performance tuning and know where you should look at.</p>
<p>Specially thanks again the <a href="https://github.com/messense/">messense</a> and <a href="https://github.com/andelf/">andelf</a> on the quick response to the pull requests and reviews. It’s a pleasant experience to work on these projects in Rust with good developers.</p>
]]></summary>
</entry>
<entry>
    <title>CJK Unified Ideograph developers should know</title>
    <link href="https://blog.paulme.ng/posts/2019-06-16-cjk-unified-ideograph-developers-should-know.html" />
    <id>https://blog.paulme.ng/posts/2019-06-16-cjk-unified-ideograph-developers-should-know.html</id>
    <published>2019-06-16T00:00:00Z</published>
    <updated>2019-06-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I have been working on <a href="https://github.com/messense/jieba-rs">jieba-rs</a> these few weeks and traced the code from the original python <a href="https://github.com/fxsjy/jieba">implementation</a>. I found a wrong assumption in the code that is by assuming the unicode scalar only ranges from U+4E00 to U+9FD5</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1">re_han_default <span class="op">=</span> re.<span class="bu">compile</span>(<span class="st">&quot;([</span><span class="ch">\u4E00</span><span class="st">-</span><span class="ch">\u9FD5</span><span class="st">a-zA-Z0-9+#&amp;\._%\-]+)&quot;</span>, re.U)</a></code></pre></div>
<p>The same kind of mistakes have been made on Chinese websites, where developers don’t even bother to put minimum effort to understand what the unicode standard is, and therefore only assumes that the unicode range only from the BMP. They don’t know as of Unicode 12.0, it has defined a total of 87887 CJK Unifided Ideograph. And there are charaters defined in Extension A to Extension F, and the upcoming planned Extension G. What falls in the BMP was mainly the result of <a href="https://en.wikipedia.org/wiki/Han_unification">Han Unification</a></p>
<p>To correctly defined the range of the unicode for CJK as the time of Unicode 12, you have to define at least the following ranges.</p>
<ul>
<li>U+3400…U+4DBF (Extesnion A)</li>
<li>U+4E00…U+9FFF (BMP)</li>
<li>U+F900…U+FAFF (Compatibilty Ideograph)</li>
<li>U+20000…U+2A6DF (Extension B)</li>
<li>U+2A700…U+2B73F (Extension C)</li>
<li>U+2B740…U+2B81F (Extension D)</li>
<li>U+2B820…U+2CEAF (Extension E)</li>
<li>U+2CEB0…U+2EBEF (Extension F)</li>
<li>U+2F800…U+2FA1F (Compatibility Supplement)</li>
</ul>
<p>It would cover the test cases like so that you would have your logic built on the correct foundation.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode rust"><code class="sourceCode rust"><a class="sourceLine" id="cb2-1" title="1">   <span class="kw">let</span> <span class="kw">mut</span> jieba = <span class="pp">Jieba::</span>new();</a>
<a class="sourceLine" id="cb2-2" title="2"></a>
<a class="sourceLine" id="cb2-3" title="3">    <span class="co">//add fake word into dictionary</span></a>
<a class="sourceLine" id="cb2-4" title="4">    jieba.add_word(<span class="st">&quot;䶴䶵𦡦&quot;</span>, <span class="cn">Some</span>(<span class="dv">1000</span>), <span class="cn">None</span>);</a>
<a class="sourceLine" id="cb2-5" title="5">    jieba.add_word(<span class="st">&quot;讥䶯䶰䶱䶲䶳&quot;</span>, <span class="cn">Some</span>(<span class="dv">1000</span>), <span class="cn">None</span>);</a>
<a class="sourceLine" id="cb2-6" title="6"></a>
<a class="sourceLine" id="cb2-7" title="7">    <span class="kw">let</span> words = jieba.cut(<span class="st">&quot;讥䶯䶰䶱䶲䶳䶴䶵𦡦&quot;</span>, <span class="cn">false</span>);</a>
<a class="sourceLine" id="cb2-8" title="8">    <span class="pp">assert_eq!</span>(words, <span class="pp">vec!</span><span class="op">[</span><span class="st">&quot;讥䶯䶰䶱䶲䶳&quot;</span>, <span class="st">&quot;䶴䶵𦡦&quot;</span><span class="op">]</span>);</a></code></pre></div>
]]></summary>
</entry>
<entry>
    <title>Switch from Chrome to Mozilla Firefox</title>
    <link href="https://blog.paulme.ng/posts/2019-06-05-switch-from-chrome-to-mozilla-firefox.html" />
    <id>https://blog.paulme.ng/posts/2019-06-05-switch-from-chrome-to-mozilla-firefox.html</id>
    <published>2019-06-05T00:00:00Z</published>
    <updated>2019-06-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Not only I am leaving Medium, I am also switching my default browser from Chrome to Firefox, due to the concerns that Google is creeping in and intrude the my privacy and trust after reading the news <a href="https://news.ycombinator.com/item?id=18052923">1</a> and <a href="https://news.ycombinator.com/item?id=20044430">2</a>.</p>
<p>It’s not the first time I tried to switch my main browser to Firefox, the last time I tried was when the first version of <a href="https://wiki.mozilla.org/Quantum">Quantum</a>’s release. I heard it was shipped with its CSS engine from servo, I couldn’t help to download it and try it out. However, it wasn’t a successful attempt. There was bugs and issues I run into, apart from that, all of the extensions need to be migrated since the APIs and architectures are totally different. At the time when it was released, there wasn’t many good extensions migrated yet. But now most of my commonly used extensions are supported.</p>
<h2 id="extensions">Extensions</h2>
<p>Here are the list of the extensions I’ve installed so far.</p>
<ul>
<li>Facebook Containers</li>
<li>Multi-Account Containers</li>
<li>Firefox Lockwise</li>
<li>OneTab</li>
<li>Tampermonkey</li>
<li>IG Helper</li>
<li>Neat URL</li>
</ul>
<p>I found that Facebook Containers is extremely useful, it works pretty much like sandbox account in Android. Neat URL is also convenient in that I don’t have to manually remove those tracking parameters when I copy &amp; paste the url to others.</p>
<h2 id="issues">Issues</h2>
<p>It’s not working perfectly without any issue though. I’ve run into the following situations.</p>
<ul>
<li>FB Messenger not able to load, not sure it is because I set the content blocking rules too strict.</li>
<li>Google Drive often hits to high cpu usage when the folders have many items.</li>
<li>Random high cpu spike when open on certain web pages.</li>
</ul>
<p>It seems that when I open web pages that heavily use javascript or css, my laptop’s fan would inevitably start to work. Unlike Chrome it would also eat up battery but not to the extent of triggering the cpu’s fan running. Not sure it is due to both of browsers’ javascript engines difference.</p>
<h2 id="summary">Summary</h2>
<p>It is not without issue but it is good enough that I am happy to set it as my main browser, and get rid of the sneaky Google.</p>
]]></summary>
</entry>
<entry>
    <title>Moving away from Medium</title>
    <link href="https://blog.paulme.ng/posts/2019-05-26-moving-away-from-medium.html" />
    <id>https://blog.paulme.ng/posts/2019-05-26-moving-away-from-medium.html</id>
    <published>2019-05-26T00:00:00Z</published>
    <updated>2019-05-26T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I was a happy user of medium until they started to raise the paywall due to they need to manage to turn it into a business not losing a lot of money. I am quite OK with paying or accept that the website needs to inject ads or something else to generate revenue. However, the way the medium has been doing is annoying. They put up the warning on the top of your write-ups saying that it is not behind the paywall, and you could not hide it by clicking it away, every time you visit the page it would show, and only the author would see it.</p>
<p><img src="/images/paywall.png" /></p>
<p>One of the benefit of Medium is that it comes with an app that I could simply just edit a write-up while I am traveling (though it could not be used without internet). I also found the replacement which is <a href="https://www.mweb.im/">MWeb</a> markdown editor. I could edit the writeup offline and easily publish it to the platforms supported. The Medium’s app is no longer exclusive benefit to me.</p>
<p>I decided to move back to the static site generator. I updated the Hakyll’s theme and make it as clean as possible. I was using the old Caspor theme but this time I used purecss to make one by myself. It is responsive and look very clean with all the white color around it. Without those large size javascript and css files need to be loaded, my static website loaded much faster as well, and I have full access to edit it as much as I like. This static website is also run on the digitalocean’s managerd kubernetes.</p>
<p>My journey so far has been Octopress, Hakyll, Medium, and then move back to Hakyll.</p>
]]></summary>
</entry>
<entry>
    <title>The fallacies and numbers developers should know</title>
    <link href="https://blog.paulme.ng/posts/2019-05-20-the-fallacies-and-numbers-developers-should-know.html" />
    <id>https://blog.paulme.ng/posts/2019-05-20-the-fallacies-and-numbers-developers-should-know.html</id>
    <published>2019-05-20T00:00:00Z</published>
    <updated>2019-05-20T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Oftentimes, you could tell a developer is good or not by talking with him if he/she is aware of the common fallacies appear in Unicode and Time, or distributed systems. After all, the growth of a developer is to understand the complexity of the world and the limitation of the abstraction, so the mental model fit better to how the real world is working over time. This blog post is try to curate a list where I find it is good, especially for junior developers. There is already <a href="https://github.com/kdeldycke/awesome-fAalsehood">awesome falsehood</a> on github, but I don’t like every link provided in the wiki. And it missed some parts where they are not fallacies, but the numbers you’d better to know.</p>
<h2 id="latency">Latency</h2>
<p>Let’s start first by knowing the <a href="https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html">latency charts</a> you should be aware of. This would give a sense of relativity when you analyze a program and to know which part it should be optimized for, given different kind of situation. In a distributed system, network latency dominates the share of the total time it spends. However, in a compute-intense application, you should be aware of how to optimize for cache-line hit, and to avoid the contention for cache slots.</p>
<h2 id="distributed-system">Distributed System</h2>
<p>The second one is the famous <a href="https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing">fallacies of distributed system</a>. They are * The network is reliable * Latency is zero * Bandwidth is infinite * The network is secure * Topology doesn’t change * There is one administrator * Transport cost is zero * The network is homogeneous</p>
<p>This is especially useful when you migrate your monolith application to be several components, or even further, to be micro services working together. Some of the naive function calls need to be extended with further assumptions (or the type would be changed if there is any). The call would need a retry and it would fail if it doesn’t finish within a budget time, and you need to cancel it etc.</p>
<h2 id="unicode">Unicode</h2>
<p>Unicode has grown to a beast where most of the junior developers misunderstand. There are numerous gotchas, like <a href="https://blog.codinghorror.com/whats-wrong-with-turkey/">the common string comparison problem in Turkey when you set the wrong locale</a> to <a href="https://nukep.github.io/progblog/2015/02/26/some-gotchas-about-unicode-that-every-programmer-should-know.html">gotchas that developers should know</a>. I think Apple has done a good job on documenting and design the swift programming language in terms of the unicode support, and by explaining the concepts in the <a href="https://docs.swift.org/swift-book/LanguageGuide/StringsAndCharacters.html">document</a>.</p>
<h2 id="time">Time</h2>
<p>One thing the awesome-falsehood did well is by collecting a list of blog posts that explain the time and calendar in programming well. It ranges from <a href="https://codeblog.jonskeet.uk/2019/03/27/storing-utc-is-not-a-silver-bullet/">storing UTC is far from enough for future date.</a>, <a href="https://alexwlchan.net/2019/05/falsehoods-programmers-believe-about-unix-time/">to the falsehood that you think it holds for unix time</a>. These are the blog posts that I personally read from the list that I think it is good, but not each of them</p>
<ul>
<li><a href="https://zachholman.com/talk/utc-is-enough-for-everyone-right">UTC is enough for everyone, right?</a></li>
<li><a href="https://gist.github.com/thanatos/eee17100476a336a711e">Explanation of the critics on falsehood to Time</a></li>
</ul>
<p>Also, Apple’s document on NSCalendar also has some golds in it. * <a href="https://developer.apple.com/documentation/foundation/nscalendar">NSCalendar document</a></p>
]]></summary>
</entry>
<entry>
    <title>My Gym Training</title>
    <link href="https://blog.paulme.ng/posts/2019-05-19-gym-training.html" />
    <id>https://blog.paulme.ng/posts/2019-05-19-gym-training.html</id>
    <published>2019-05-19T00:00:00Z</published>
    <updated>2019-05-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I have been doing gym training for some time in order to keep myself in shape, and release the stress from work and life. I think it is good time to take some notes on the set of moves I have been doing as a milestone. These moves help me build the muscle on my arm, buttock and abs. Youtube is a good source of information like this, it just shows you what exactly you should do and you just need to follow the moves carefully in case you hurt yourself. And you could also find the intermittent training video with the right tempo to burn your fat and build your muscle in a scientific way. You could just play the youtube video and follow exactly and not necessarily have to download the training app. You could just play the youtube video and follow exactly and not necessarily have to download the training app.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/XhqIKAA6Bzk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/AdXo7tEfJe4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/H2ZVMCeiDpc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Z90xpWvuUPs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
]]></summary>
</entry>

</feed>
